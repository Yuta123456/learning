{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch.nn as nn\n",
    "from torchvision import models\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")  # GPUデバイスを取得\n",
    "else:\n",
    "    device = torch.device(\"cpu\")  # CPUデバイスを取得"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "画像処理のモデル\n",
    "\"\"\"\n",
    "\n",
    "class ImageEncoder(nn.Module):\n",
    "    def __init__(self, embedding_size):\n",
    "        super(ImageEncoder, self).__init__()\n",
    "        self.resnet50 = models.resnet50(pretrained=True)\n",
    "        self.fc = nn.Linear(self.resnet50.fc.out_features, embedding_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.resnet50(x)\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "テキスト処理のモデル\n",
    "\"\"\"\n",
    "class CaptionEncoder(nn.Module):\n",
    "  def __init__(self):\n",
    "    super().__init__()\n",
    "    self.bert = AutoModel.from_pretrained(\"cl-tohoku/bert-base-japanese-v2\")\n",
    "  def forward(self, x):\n",
    "    x = self.bert(x)\n",
    "    x = torch.max(x.last_hidden_state, dim=1)[0]  # max pooling\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(635192, 3)\n"
     ]
    }
   ],
   "source": [
    "from util_class.CustomDataset import EmbeddingDataset\n",
    "\n",
    "\n",
    "dataset = EmbeddingDataset('./data/anotation_new.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "learning_rate = 1e-5\n",
    "batch_size = 16\n",
    "epochs = 4\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "\n",
    "train_dataset, test_dataset = torch.utils.data.random_split(\n",
    "    dataset, [train_size, val_size]\n",
    ")\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, pin_memory=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at cl-tohoku/bert-base-japanese-v2 were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.bias', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import torch.nn.functional as F\n",
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.abspath(\"../\"))\n",
    "from models.ContrastiveLoss import ContrastiveLoss\n",
    "\n",
    "\n",
    "image_model = ImageEncoder(768).to(device)\n",
    "caption_model = CaptionEncoder().to(device)\n",
    "image_model.load_state_dict(torch.load('model/model_image_2023-07-02.pth'))\n",
    "caption_model.load_state_dict(torch.load('model/model_caption_2023-07-02.pth'))\n",
    "img_optimizer = torch.optim.Adam(image_model.parameters(), lr=learning_rate)\n",
    "cpt_optimizer = torch.optim.Adam(caption_model.parameters(), lr=learning_rate)\n",
    "\n",
    "loss_fn = ContrastiveLoss()\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"cl-tohoku/bert-base-japanese-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(dataloader, img_model, cpt_model,  loss_fn, img_opt, cpt_opt):\n",
    "    size = len(dataloader.dataset)\n",
    "    for batch, (img, cap, label, _) in enumerate(dataloader):        \n",
    "        # 予測と損失の計算\n",
    "        img = img.to(device)\n",
    "        label = label.to(device)\n",
    "        pred = img_model(img)\n",
    "        ids = tokenizer.batch_encode_plus(cap, return_tensors='pt', padding='max_length', truncation=True, max_length=256).input_ids\n",
    "        ids = ids.to(device)\n",
    "        target = cpt_model(ids)\n",
    "        # ここ不安\n",
    "        loss = loss_fn(pred, target, label)\n",
    "\n",
    "        # バックプロパゲーション\n",
    "        img_opt.zero_grad()\n",
    "        cpt_opt.zero_grad()\n",
    "        loss.backward()\n",
    "        img_opt.step()\n",
    "        cpt_opt.step()\n",
    "\n",
    "        if batch % 30 == 0:\n",
    "            loss, current = loss.item() / len(img), batch * len(img)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "\n",
    "\n",
    "def test_loop(dataloader, img_model, cpt_model,  loss_fn):\n",
    "    size = len(dataloader.dataset)\n",
    "    test_loss = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for (img, cap, label, _) in dataloader:\n",
    "            img = img.to(device)\n",
    "            label = label.to(device)\n",
    "            pred = img_model(img)\n",
    "            ids = tokenizer.encode(cap, return_tensors='pt')\n",
    "            ids = ids.to(device)\n",
    "            target = cpt_model(ids)\n",
    "            # print(pred.shape, target.shape, len(X), len(y))\n",
    "            # ここ不安\n",
    "            loss = loss_fn(pred, target, label).mean()\n",
    "            test_loss += loss.item()\n",
    "            \n",
    "    test_loss /= size\n",
    "    print(f\"Avg loss: {test_loss:>8f} \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start\n",
      "Epoch 1\\-------------------------------\n",
      "loss: 0.040809  [    0/508153]\n",
      "loss: 0.031671  [  480/508153]\n",
      "loss: 0.030572  [  960/508153]\n",
      "loss: 0.032185  [ 1440/508153]\n",
      "loss: 0.031886  [ 1920/508153]\n",
      "loss: 0.038456  [ 2400/508153]\n",
      "loss: 0.046150  [ 2880/508153]\n",
      "loss: 0.036169  [ 3360/508153]\n",
      "loss: 0.047579  [ 3840/508153]\n",
      "loss: 0.035831  [ 4320/508153]\n",
      "loss: 0.039821  [ 4800/508153]\n",
      "loss: 0.033201  [ 5280/508153]\n",
      "loss: 0.033310  [ 5760/508153]\n",
      "loss: 0.030289  [ 6240/508153]\n",
      "loss: 0.030229  [ 6720/508153]\n",
      "loss: 0.037267  [ 7200/508153]\n",
      "loss: 0.037608  [ 7680/508153]\n",
      "loss: 0.029941  [ 8160/508153]\n",
      "loss: 0.025767  [ 8640/508153]\n",
      "loss: 0.073675  [ 9120/508153]\n",
      "loss: 0.032276  [ 9600/508153]\n",
      "loss: 0.042545  [10080/508153]\n",
      "loss: 0.035856  [10560/508153]\n",
      "loss: 0.033655  [11040/508153]\n",
      "loss: 0.028770  [11520/508153]\n",
      "loss: 0.030893  [12000/508153]\n",
      "loss: 0.034909  [12480/508153]\n",
      "loss: 0.081878  [12960/508153]\n",
      "loss: 0.059162  [13440/508153]\n",
      "loss: 0.032088  [13920/508153]\n",
      "loss: 0.034830  [14400/508153]\n",
      "loss: 0.030474  [14880/508153]\n",
      "loss: 0.032549  [15360/508153]\n",
      "loss: 0.036494  [15840/508153]\n",
      "loss: 0.032600  [16320/508153]\n",
      "loss: 0.029946  [16800/508153]\n",
      "loss: 0.027388  [17280/508153]\n",
      "loss: 0.041459  [17760/508153]\n",
      "loss: 0.031330  [18240/508153]\n",
      "loss: 0.030297  [18720/508153]\n",
      "loss: 0.041643  [19200/508153]\n",
      "loss: 0.067851  [19680/508153]\n",
      "loss: 0.025274  [20160/508153]\n",
      "loss: 0.043104  [20640/508153]\n",
      "loss: 0.029421  [21120/508153]\n",
      "loss: 0.063507  [21600/508153]\n",
      "loss: 0.031073  [22080/508153]\n",
      "loss: 0.037809  [22560/508153]\n",
      "loss: 0.029621  [23040/508153]\n",
      "loss: 0.037403  [23520/508153]\n",
      "loss: 0.038568  [24000/508153]\n",
      "loss: 0.038445  [24480/508153]\n",
      "loss: 0.031810  [24960/508153]\n",
      "loss: 0.025994  [25440/508153]\n",
      "loss: 0.041483  [25920/508153]\n",
      "loss: 0.040166  [26400/508153]\n",
      "loss: 0.030812  [26880/508153]\n",
      "loss: 0.032698  [27360/508153]\n",
      "loss: 0.034875  [27840/508153]\n",
      "loss: 0.034005  [28320/508153]\n",
      "loss: 0.035851  [28800/508153]\n",
      "loss: 0.028072  [29280/508153]\n",
      "loss: 0.043733  [29760/508153]\n",
      "loss: 0.035399  [30240/508153]\n",
      "loss: 0.032763  [30720/508153]\n",
      "loss: 0.036159  [31200/508153]\n",
      "loss: 0.032111  [31680/508153]\n",
      "loss: 0.033912  [32160/508153]\n",
      "loss: 0.040090  [32640/508153]\n",
      "loss: 0.039233  [33120/508153]\n",
      "loss: 0.030458  [33600/508153]\n",
      "loss: 0.027020  [34080/508153]\n",
      "loss: 0.034476  [34560/508153]\n",
      "loss: 0.030021  [35040/508153]\n",
      "loss: 0.031897  [35520/508153]\n",
      "loss: 0.027770  [36000/508153]\n",
      "loss: 0.032962  [36480/508153]\n",
      "loss: 0.030463  [36960/508153]\n",
      "loss: 0.028702  [37440/508153]\n",
      "loss: 0.049127  [37920/508153]\n",
      "loss: 0.022207  [38400/508153]\n",
      "loss: 0.035236  [38880/508153]\n",
      "loss: 0.041743  [39360/508153]\n",
      "loss: 0.065443  [39840/508153]\n",
      "loss: 0.035660  [40320/508153]\n",
      "loss: 0.046370  [40800/508153]\n",
      "loss: 0.031348  [41280/508153]\n",
      "loss: 0.035697  [41760/508153]\n",
      "loss: 0.036688  [42240/508153]\n",
      "loss: 0.034184  [42720/508153]\n",
      "loss: 0.028188  [43200/508153]\n",
      "loss: 0.043569  [43680/508153]\n",
      "loss: 0.027791  [44160/508153]\n",
      "loss: 0.032622  [44640/508153]\n",
      "loss: 0.032534  [45120/508153]\n",
      "loss: 0.039748  [45600/508153]\n",
      "loss: 0.038800  [46080/508153]\n",
      "loss: 0.033825  [46560/508153]\n",
      "loss: 0.030356  [47040/508153]\n",
      "loss: 0.036358  [47520/508153]\n",
      "loss: 0.035950  [48000/508153]\n",
      "loss: 0.031338  [48480/508153]\n",
      "loss: 0.034243  [48960/508153]\n",
      "loss: 0.077955  [49440/508153]\n",
      "loss: 0.029908  [49920/508153]\n",
      "loss: 0.031442  [50400/508153]\n",
      "loss: 0.035354  [50880/508153]\n",
      "loss: 0.038769  [51360/508153]\n",
      "loss: 0.038099  [51840/508153]\n",
      "loss: 0.035903  [52320/508153]\n",
      "loss: 0.029131  [52800/508153]\n",
      "loss: 0.031879  [53280/508153]\n",
      "loss: 0.027523  [53760/508153]\n",
      "loss: 0.042516  [54240/508153]\n",
      "loss: 0.027687  [54720/508153]\n",
      "loss: 0.030691  [55200/508153]\n",
      "loss: 0.035460  [55680/508153]\n",
      "loss: 0.037962  [56160/508153]\n",
      "loss: 0.036587  [56640/508153]\n",
      "loss: 0.031875  [57120/508153]\n",
      "loss: 0.027221  [57600/508153]\n",
      "loss: 0.028396  [58080/508153]\n",
      "loss: 0.030606  [58560/508153]\n",
      "loss: 0.031190  [59040/508153]\n",
      "loss: 0.030940  [59520/508153]\n",
      "loss: 0.034416  [60000/508153]\n",
      "loss: 0.027437  [60480/508153]\n",
      "loss: 0.051259  [60960/508153]\n",
      "loss: 0.024857  [61440/508153]\n",
      "loss: 0.032632  [61920/508153]\n",
      "loss: 0.038808  [62400/508153]\n",
      "loss: 0.034855  [62880/508153]\n",
      "loss: 0.047630  [63360/508153]\n",
      "loss: 0.036462  [63840/508153]\n",
      "loss: 0.060660  [64320/508153]\n",
      "loss: 0.048892  [64800/508153]\n",
      "loss: 0.033662  [65280/508153]\n",
      "loss: 0.034718  [65760/508153]\n",
      "loss: 0.034765  [66240/508153]\n",
      "loss: 0.034132  [66720/508153]\n",
      "loss: 0.034698  [67200/508153]\n",
      "loss: 0.032255  [67680/508153]\n",
      "loss: 0.026750  [68160/508153]\n",
      "loss: 0.031454  [68640/508153]\n",
      "loss: 0.027540  [69120/508153]\n",
      "loss: 0.026633  [69600/508153]\n",
      "loss: 0.040177  [70080/508153]\n",
      "loss: 0.031872  [70560/508153]\n",
      "loss: 0.064130  [71040/508153]\n",
      "loss: 0.031666  [71520/508153]\n",
      "loss: 0.034189  [72000/508153]\n",
      "loss: 0.034316  [72480/508153]\n",
      "loss: 0.033852  [72960/508153]\n",
      "loss: 0.030974  [73440/508153]\n",
      "loss: 0.032235  [73920/508153]\n",
      "loss: 0.035807  [74400/508153]\n",
      "loss: 0.034772  [74880/508153]\n",
      "loss: 0.035754  [75360/508153]\n",
      "loss: 0.033827  [75840/508153]\n",
      "loss: 0.037057  [76320/508153]\n",
      "loss: 0.029799  [76800/508153]\n",
      "loss: 0.025799  [77280/508153]\n",
      "loss: 0.035873  [77760/508153]\n",
      "loss: 0.033830  [78240/508153]\n",
      "loss: 0.063560  [78720/508153]\n",
      "loss: 0.034079  [79200/508153]\n",
      "loss: 0.038475  [79680/508153]\n",
      "loss: 0.031039  [80160/508153]\n",
      "loss: 0.028684  [80640/508153]\n",
      "loss: 0.060172  [81120/508153]\n",
      "loss: 0.070987  [81600/508153]\n",
      "loss: 0.041980  [82080/508153]\n",
      "loss: 0.035787  [82560/508153]\n",
      "loss: 0.036978  [83040/508153]\n",
      "loss: 0.037722  [83520/508153]\n",
      "loss: 0.029521  [84000/508153]\n",
      "loss: 0.039604  [84480/508153]\n",
      "loss: 0.031666  [84960/508153]\n",
      "loss: 0.030175  [85440/508153]\n",
      "loss: 0.030557  [85920/508153]\n",
      "loss: 0.028081  [86400/508153]\n",
      "loss: 0.028537  [86880/508153]\n",
      "loss: 0.033646  [87360/508153]\n",
      "loss: 0.038059  [87840/508153]\n",
      "loss: 0.036637  [88320/508153]\n",
      "loss: 0.032900  [88800/508153]\n",
      "loss: 0.037668  [89280/508153]\n",
      "loss: 0.035932  [89760/508153]\n",
      "loss: 0.028662  [90240/508153]\n",
      "loss: 0.035572  [90720/508153]\n",
      "loss: 0.028940  [91200/508153]\n",
      "loss: 0.038778  [91680/508153]\n",
      "loss: 0.035218  [92160/508153]\n",
      "loss: 0.027436  [92640/508153]\n",
      "loss: 0.033991  [93120/508153]\n",
      "loss: 0.061605  [93600/508153]\n",
      "loss: 0.029878  [94080/508153]\n",
      "loss: 0.041349  [94560/508153]\n",
      "loss: 0.032567  [95040/508153]\n",
      "loss: 0.030620  [95520/508153]\n",
      "loss: 0.032419  [96000/508153]\n",
      "loss: 0.031154  [96480/508153]\n",
      "loss: 0.030897  [96960/508153]\n",
      "loss: 0.033752  [97440/508153]\n",
      "loss: 0.032949  [97920/508153]\n",
      "loss: 0.028292  [98400/508153]\n",
      "loss: 0.044546  [98880/508153]\n",
      "loss: 0.029785  [99360/508153]\n",
      "loss: 0.027753  [99840/508153]\n",
      "loss: 0.033514  [100320/508153]\n",
      "loss: 0.040037  [100800/508153]\n",
      "loss: 0.031517  [101280/508153]\n",
      "loss: 0.041199  [101760/508153]\n",
      "loss: 0.037487  [102240/508153]\n",
      "loss: 0.058756  [102720/508153]\n",
      "loss: 0.039757  [103200/508153]\n",
      "loss: 0.032829  [103680/508153]\n",
      "loss: 0.032033  [104160/508153]\n",
      "loss: 0.032157  [104640/508153]\n",
      "loss: 0.034844  [105120/508153]\n",
      "loss: 0.035001  [105600/508153]\n",
      "loss: 0.034476  [106080/508153]\n",
      "loss: 0.058783  [106560/508153]\n",
      "loss: 0.029004  [107040/508153]\n",
      "loss: 0.032773  [107520/508153]\n",
      "loss: 0.034840  [108000/508153]\n",
      "loss: 0.033326  [108480/508153]\n",
      "loss: 0.045253  [108960/508153]\n",
      "loss: 0.029284  [109440/508153]\n",
      "loss: 0.031459  [109920/508153]\n",
      "loss: 0.029798  [110400/508153]\n",
      "loss: 0.036063  [110880/508153]\n",
      "loss: 0.034055  [111360/508153]\n",
      "loss: 0.031309  [111840/508153]\n",
      "loss: 0.036234  [112320/508153]\n",
      "loss: 0.033526  [112800/508153]\n",
      "loss: 0.035319  [113280/508153]\n",
      "loss: 0.032784  [113760/508153]\n",
      "loss: 0.031173  [114240/508153]\n",
      "loss: 0.031563  [114720/508153]\n",
      "loss: 0.034143  [115200/508153]\n",
      "loss: 0.037687  [115680/508153]\n",
      "loss: 0.036529  [116160/508153]\n",
      "loss: 0.034324  [116640/508153]\n",
      "loss: 0.030632  [117120/508153]\n",
      "loss: 0.036613  [117600/508153]\n",
      "loss: 0.029950  [118080/508153]\n",
      "loss: 0.030340  [118560/508153]\n",
      "loss: 0.026728  [119040/508153]\n",
      "loss: 0.033669  [119520/508153]\n",
      "loss: 0.034928  [120000/508153]\n",
      "loss: 0.029479  [120480/508153]\n",
      "loss: 0.034530  [120960/508153]\n",
      "loss: 0.032388  [121440/508153]\n",
      "loss: 0.031239  [121920/508153]\n",
      "loss: 0.038813  [122400/508153]\n",
      "loss: 0.035922  [122880/508153]\n",
      "loss: 0.032679  [123360/508153]\n",
      "loss: 0.031393  [123840/508153]\n",
      "loss: 0.036103  [124320/508153]\n",
      "loss: 0.033135  [124800/508153]\n",
      "loss: 0.033309  [125280/508153]\n",
      "loss: 0.031317  [125760/508153]\n",
      "loss: 0.027854  [126240/508153]\n",
      "loss: 0.036078  [126720/508153]\n",
      "loss: 0.033017  [127200/508153]\n",
      "loss: 0.039203  [127680/508153]\n",
      "loss: 0.038343  [128160/508153]\n",
      "loss: 0.071328  [128640/508153]\n",
      "loss: 0.027760  [129120/508153]\n",
      "loss: 0.031186  [129600/508153]\n",
      "loss: 0.034171  [130080/508153]\n",
      "loss: 0.034841  [130560/508153]\n",
      "loss: 0.031782  [131040/508153]\n",
      "loss: 0.038632  [131520/508153]\n",
      "loss: 0.026696  [132000/508153]\n",
      "loss: 0.033501  [132480/508153]\n",
      "loss: 0.032929  [132960/508153]\n",
      "loss: 0.036910  [133440/508153]\n",
      "loss: 0.026102  [133920/508153]\n",
      "loss: 0.033474  [134400/508153]\n",
      "loss: 0.035298  [134880/508153]\n",
      "loss: 0.036580  [135360/508153]\n",
      "loss: 0.028268  [135840/508153]\n",
      "loss: 0.029510  [136320/508153]\n",
      "loss: 0.036433  [136800/508153]\n",
      "loss: 0.034011  [137280/508153]\n",
      "loss: 0.029571  [137760/508153]\n",
      "loss: 0.067837  [138240/508153]\n",
      "loss: 0.033167  [138720/508153]\n",
      "loss: 0.028608  [139200/508153]\n",
      "loss: 0.040153  [139680/508153]\n",
      "loss: 0.038578  [140160/508153]\n",
      "loss: 0.030123  [140640/508153]\n",
      "loss: 0.028606  [141120/508153]\n",
      "loss: 0.027748  [141600/508153]\n",
      "loss: 0.033193  [142080/508153]\n",
      "loss: 0.030697  [142560/508153]\n",
      "loss: 0.028107  [143040/508153]\n",
      "loss: 0.036007  [143520/508153]\n",
      "loss: 0.034478  [144000/508153]\n",
      "loss: 0.030186  [144480/508153]\n",
      "loss: 0.027395  [144960/508153]\n",
      "loss: 0.036909  [145440/508153]\n",
      "loss: 0.030875  [145920/508153]\n",
      "loss: 0.036745  [146400/508153]\n",
      "loss: 0.026840  [146880/508153]\n",
      "loss: 0.027721  [147360/508153]\n",
      "loss: 0.028308  [147840/508153]\n",
      "loss: 0.029742  [148320/508153]\n",
      "loss: 0.094364  [148800/508153]\n",
      "loss: 0.035740  [149280/508153]\n",
      "loss: 0.027891  [149760/508153]\n",
      "loss: 0.029507  [150240/508153]\n",
      "loss: 0.027481  [150720/508153]\n",
      "loss: 0.036345  [151200/508153]\n",
      "loss: 0.029157  [151680/508153]\n",
      "loss: 0.026734  [152160/508153]\n",
      "loss: 0.032017  [152640/508153]\n",
      "loss: 0.030239  [153120/508153]\n",
      "loss: 0.032819  [153600/508153]\n",
      "loss: 0.061849  [154080/508153]\n",
      "loss: 0.034784  [154560/508153]\n",
      "loss: 0.032351  [155040/508153]\n",
      "loss: 0.031164  [155520/508153]\n",
      "loss: 0.031641  [156000/508153]\n",
      "loss: 0.032865  [156480/508153]\n",
      "loss: 0.033362  [156960/508153]\n",
      "loss: 0.030832  [157440/508153]\n",
      "loss: 0.027876  [157920/508153]\n",
      "loss: 0.035714  [158400/508153]\n",
      "loss: 0.029110  [158880/508153]\n",
      "loss: 0.037323  [159360/508153]\n",
      "loss: 0.031879  [159840/508153]\n",
      "loss: 0.036225  [160320/508153]\n",
      "loss: 0.035280  [160800/508153]\n",
      "loss: 0.029763  [161280/508153]\n",
      "loss: 0.028802  [161760/508153]\n",
      "loss: 0.029466  [162240/508153]\n",
      "loss: 0.031688  [162720/508153]\n",
      "loss: 0.034621  [163200/508153]\n",
      "loss: 0.035569  [163680/508153]\n",
      "loss: 0.036801  [164160/508153]\n",
      "loss: 0.030106  [164640/508153]\n",
      "loss: 0.029941  [165120/508153]\n",
      "loss: 0.033438  [165600/508153]\n",
      "loss: 0.032720  [166080/508153]\n",
      "loss: 0.034771  [166560/508153]\n",
      "loss: 0.035044  [167040/508153]\n",
      "loss: 0.029403  [167520/508153]\n",
      "loss: 0.055048  [168000/508153]\n",
      "loss: 0.031567  [168480/508153]\n",
      "loss: 0.033524  [168960/508153]\n",
      "loss: 0.031505  [169440/508153]\n",
      "loss: 0.032579  [169920/508153]\n",
      "loss: 0.032460  [170400/508153]\n",
      "loss: 0.030427  [170880/508153]\n",
      "loss: 0.033795  [171360/508153]\n",
      "loss: 0.030081  [171840/508153]\n",
      "loss: 0.031729  [172320/508153]\n",
      "loss: 0.036450  [172800/508153]\n",
      "loss: 0.029520  [173280/508153]\n",
      "loss: 0.035011  [173760/508153]\n",
      "loss: 0.033226  [174240/508153]\n",
      "loss: 0.030898  [174720/508153]\n",
      "loss: 0.031280  [175200/508153]\n",
      "loss: 0.035641  [175680/508153]\n",
      "loss: 0.034700  [176160/508153]\n",
      "loss: 0.030371  [176640/508153]\n",
      "loss: 0.033998  [177120/508153]\n",
      "loss: 0.029890  [177600/508153]\n",
      "loss: 0.030980  [178080/508153]\n",
      "loss: 0.029920  [178560/508153]\n",
      "loss: 0.031547  [179040/508153]\n",
      "loss: 0.033096  [179520/508153]\n",
      "loss: 0.036504  [180000/508153]\n",
      "loss: 0.031455  [180480/508153]\n",
      "loss: 0.031551  [180960/508153]\n",
      "loss: 0.033032  [181440/508153]\n",
      "loss: 0.033709  [181920/508153]\n",
      "loss: 0.032584  [182400/508153]\n",
      "loss: 0.030560  [182880/508153]\n",
      "loss: 0.035739  [183360/508153]\n",
      "loss: 0.040733  [183840/508153]\n",
      "loss: 0.041908  [184320/508153]\n",
      "loss: 0.032082  [184800/508153]\n",
      "loss: 0.048280  [185280/508153]\n",
      "loss: 0.033630  [185760/508153]\n",
      "loss: 0.051246  [186240/508153]\n",
      "loss: 0.042888  [186720/508153]\n",
      "loss: 0.031813  [187200/508153]\n",
      "loss: 0.031824  [187680/508153]\n",
      "loss: 0.060505  [188160/508153]\n",
      "loss: 0.039384  [188640/508153]\n",
      "loss: 0.040418  [189120/508153]\n",
      "loss: 0.036252  [189600/508153]\n",
      "loss: 0.033508  [190080/508153]\n",
      "loss: 0.033566  [190560/508153]\n",
      "loss: 0.029186  [191040/508153]\n",
      "loss: 0.035755  [191520/508153]\n",
      "loss: 0.032784  [192000/508153]\n",
      "loss: 0.023718  [192480/508153]\n",
      "loss: 0.031245  [192960/508153]\n",
      "loss: 0.029175  [193440/508153]\n",
      "loss: 0.035006  [193920/508153]\n",
      "loss: 0.029856  [194400/508153]\n",
      "loss: 0.033846  [194880/508153]\n",
      "loss: 0.032628  [195360/508153]\n",
      "loss: 0.030387  [195840/508153]\n",
      "loss: 0.028353  [196320/508153]\n",
      "loss: 0.030778  [196800/508153]\n",
      "loss: 0.031316  [197280/508153]\n",
      "loss: 0.033010  [197760/508153]\n",
      "loss: 0.027591  [198240/508153]\n",
      "loss: 0.035971  [198720/508153]\n",
      "loss: 0.033717  [199200/508153]\n",
      "loss: 0.024219  [199680/508153]\n",
      "loss: 0.059560  [200160/508153]\n",
      "loss: 0.034146  [200640/508153]\n",
      "loss: 0.028491  [201120/508153]\n",
      "loss: 0.032253  [201600/508153]\n",
      "loss: 0.034168  [202080/508153]\n",
      "loss: 0.034493  [202560/508153]\n",
      "loss: 0.057706  [203040/508153]\n",
      "loss: 0.031918  [203520/508153]\n",
      "loss: 0.031076  [204000/508153]\n",
      "loss: 0.030607  [204480/508153]\n",
      "loss: 0.037743  [204960/508153]\n",
      "loss: 0.031345  [205440/508153]\n",
      "loss: 0.030675  [205920/508153]\n",
      "loss: 0.029275  [206400/508153]\n",
      "loss: 0.030816  [206880/508153]\n",
      "loss: 0.034859  [207360/508153]\n",
      "loss: 0.035721  [207840/508153]\n",
      "loss: 0.031251  [208320/508153]\n",
      "loss: 0.034148  [208800/508153]\n",
      "loss: 0.037437  [209280/508153]\n",
      "loss: 0.031326  [209760/508153]\n",
      "loss: 0.068351  [210240/508153]\n",
      "loss: 0.031836  [210720/508153]\n",
      "loss: 0.031262  [211200/508153]\n",
      "loss: 0.031246  [211680/508153]\n",
      "loss: 0.026407  [212160/508153]\n",
      "loss: 0.038371  [212640/508153]\n",
      "loss: 0.030440  [213120/508153]\n",
      "loss: 0.030324  [213600/508153]\n",
      "loss: 0.030860  [214080/508153]\n",
      "loss: 0.028802  [214560/508153]\n",
      "loss: 0.067757  [215040/508153]\n",
      "loss: 0.032525  [215520/508153]\n",
      "loss: 0.035778  [216000/508153]\n",
      "loss: 0.033615  [216480/508153]\n",
      "loss: 0.031681  [216960/508153]\n",
      "loss: 0.027657  [217440/508153]\n",
      "loss: 0.032745  [217920/508153]\n",
      "loss: 0.035714  [218400/508153]\n",
      "loss: 0.029539  [218880/508153]\n",
      "loss: 0.049972  [219360/508153]\n",
      "loss: 0.037514  [219840/508153]\n",
      "loss: 0.027495  [220320/508153]\n",
      "loss: 0.029849  [220800/508153]\n",
      "loss: 0.032702  [221280/508153]\n",
      "loss: 0.034068  [221760/508153]\n",
      "loss: 0.033920  [222240/508153]\n",
      "loss: 0.028623  [222720/508153]\n",
      "loss: 0.028180  [223200/508153]\n",
      "loss: 0.033676  [223680/508153]\n",
      "loss: 0.027552  [224160/508153]\n",
      "loss: 0.057241  [224640/508153]\n",
      "loss: 0.030577  [225120/508153]\n",
      "loss: 0.035440  [225600/508153]\n",
      "loss: 0.031609  [226080/508153]\n",
      "loss: 0.032271  [226560/508153]\n",
      "loss: 0.029928  [227040/508153]\n",
      "loss: 0.033052  [227520/508153]\n",
      "loss: 0.029260  [228000/508153]\n",
      "loss: 0.031722  [228480/508153]\n",
      "loss: 0.032353  [228960/508153]\n",
      "loss: 0.031695  [229440/508153]\n",
      "loss: 0.031613  [229920/508153]\n",
      "loss: 0.034473  [230400/508153]\n",
      "loss: 0.033335  [230880/508153]\n",
      "loss: 0.032073  [231360/508153]\n",
      "loss: 0.026674  [231840/508153]\n",
      "loss: 0.028670  [232320/508153]\n",
      "loss: 0.031533  [232800/508153]\n",
      "loss: 0.036262  [233280/508153]\n",
      "loss: 0.032615  [233760/508153]\n",
      "loss: 0.029627  [234240/508153]\n",
      "loss: 0.034386  [234720/508153]\n",
      "loss: 0.032383  [235200/508153]\n",
      "loss: 0.031972  [235680/508153]\n",
      "loss: 0.033392  [236160/508153]\n",
      "loss: 0.028410  [236640/508153]\n",
      "loss: 0.034497  [237120/508153]\n",
      "loss: 0.036276  [237600/508153]\n",
      "loss: 0.034544  [238080/508153]\n",
      "loss: 0.029268  [238560/508153]\n",
      "loss: 0.027971  [239040/508153]\n",
      "loss: 0.032435  [239520/508153]\n",
      "loss: 0.033100  [240000/508153]\n",
      "loss: 0.037309  [240480/508153]\n",
      "loss: 0.033339  [240960/508153]\n",
      "loss: 0.034697  [241440/508153]\n",
      "loss: 0.038087  [241920/508153]\n",
      "loss: 0.029339  [242400/508153]\n",
      "loss: 0.028563  [242880/508153]\n",
      "loss: 0.029989  [243360/508153]\n",
      "loss: 0.027416  [243840/508153]\n",
      "loss: 0.026073  [244320/508153]\n",
      "loss: 0.028754  [244800/508153]\n",
      "loss: 0.035479  [245280/508153]\n",
      "loss: 0.030290  [245760/508153]\n",
      "loss: 0.030714  [246240/508153]\n",
      "loss: 0.034926  [246720/508153]\n",
      "loss: 0.033676  [247200/508153]\n",
      "loss: 0.035524  [247680/508153]\n",
      "loss: 0.033295  [248160/508153]\n",
      "loss: 0.031987  [248640/508153]\n",
      "loss: 0.033392  [249120/508153]\n",
      "loss: 0.031988  [249600/508153]\n",
      "loss: 0.030327  [250080/508153]\n",
      "loss: 0.032005  [250560/508153]\n",
      "loss: 0.035313  [251040/508153]\n",
      "loss: 0.036350  [251520/508153]\n",
      "loss: 0.030678  [252000/508153]\n",
      "loss: 0.056350  [252480/508153]\n",
      "loss: 0.030946  [252960/508153]\n",
      "loss: 0.033209  [253440/508153]\n",
      "loss: 0.037141  [253920/508153]\n",
      "loss: 0.030838  [254400/508153]\n",
      "loss: 0.030817  [254880/508153]\n",
      "loss: 0.032828  [255360/508153]\n",
      "loss: 0.051425  [255840/508153]\n",
      "loss: 0.034109  [256320/508153]\n",
      "loss: 0.033410  [256800/508153]\n",
      "loss: 0.032376  [257280/508153]\n",
      "loss: 0.032335  [257760/508153]\n",
      "loss: 0.037468  [258240/508153]\n",
      "loss: 0.032041  [258720/508153]\n",
      "loss: 0.030449  [259200/508153]\n",
      "loss: 0.029504  [259680/508153]\n",
      "loss: 0.036582  [260160/508153]\n",
      "loss: 0.031157  [260640/508153]\n",
      "loss: 0.034719  [261120/508153]\n",
      "loss: 0.029674  [261600/508153]\n",
      "loss: 0.031826  [262080/508153]\n",
      "loss: 0.032293  [262560/508153]\n",
      "loss: 0.032527  [263040/508153]\n",
      "loss: 0.030099  [263520/508153]\n",
      "loss: 0.035413  [264000/508153]\n",
      "loss: 0.051103  [264480/508153]\n",
      "loss: 0.037848  [264960/508153]\n",
      "loss: 0.033668  [265440/508153]\n",
      "loss: 0.026593  [265920/508153]\n",
      "loss: 0.036442  [266400/508153]\n",
      "loss: 0.025599  [266880/508153]\n",
      "loss: 0.026326  [267360/508153]\n",
      "loss: 0.032586  [267840/508153]\n",
      "loss: 0.029484  [268320/508153]\n",
      "loss: 0.031737  [268800/508153]\n",
      "loss: 0.032927  [269280/508153]\n",
      "loss: 0.030527  [269760/508153]\n",
      "loss: 0.029834  [270240/508153]\n",
      "loss: 0.036661  [270720/508153]\n",
      "loss: 0.030302  [271200/508153]\n",
      "loss: 0.029627  [271680/508153]\n",
      "loss: 0.032937  [272160/508153]\n",
      "loss: 0.029789  [272640/508153]\n",
      "loss: 0.036534  [273120/508153]\n",
      "loss: 0.028561  [273600/508153]\n",
      "loss: 0.032734  [274080/508153]\n",
      "loss: 0.032953  [274560/508153]\n",
      "loss: 0.034940  [275040/508153]\n",
      "loss: 0.030665  [275520/508153]\n",
      "loss: 0.032127  [276000/508153]\n",
      "loss: 0.030452  [276480/508153]\n",
      "loss: 0.034279  [276960/508153]\n",
      "loss: 0.028236  [277440/508153]\n",
      "loss: 0.031340  [277920/508153]\n",
      "loss: 0.030705  [278400/508153]\n",
      "loss: 0.032542  [278880/508153]\n",
      "loss: 0.032354  [279360/508153]\n",
      "loss: 0.029712  [279840/508153]\n",
      "loss: 0.029979  [280320/508153]\n",
      "loss: 0.031094  [280800/508153]\n",
      "loss: 0.032612  [281280/508153]\n",
      "loss: 0.030661  [281760/508153]\n",
      "loss: 0.034663  [282240/508153]\n",
      "loss: 0.033945  [282720/508153]\n",
      "loss: 0.029625  [283200/508153]\n",
      "loss: 0.033323  [283680/508153]\n",
      "loss: 0.033896  [284160/508153]\n",
      "loss: 0.032310  [284640/508153]\n",
      "loss: 0.030223  [285120/508153]\n",
      "loss: 0.033213  [285600/508153]\n",
      "loss: 0.030488  [286080/508153]\n",
      "loss: 0.034602  [286560/508153]\n",
      "loss: 0.028941  [287040/508153]\n",
      "loss: 0.028756  [287520/508153]\n",
      "loss: 0.033347  [288000/508153]\n",
      "loss: 0.052363  [288480/508153]\n",
      "loss: 0.028915  [288960/508153]\n",
      "loss: 0.034065  [289440/508153]\n",
      "loss: 0.035653  [289920/508153]\n",
      "loss: 0.029686  [290400/508153]\n",
      "loss: 0.032415  [290880/508153]\n",
      "loss: 0.038069  [291360/508153]\n",
      "loss: 0.034247  [291840/508153]\n",
      "loss: 0.056971  [292320/508153]\n",
      "loss: 0.027140  [292800/508153]\n",
      "loss: 0.032117  [293280/508153]\n",
      "loss: 0.029119  [293760/508153]\n",
      "loss: 0.031804  [294240/508153]\n",
      "loss: 0.032311  [294720/508153]\n",
      "loss: 0.032234  [295200/508153]\n",
      "loss: 0.029111  [295680/508153]\n",
      "loss: 0.031248  [296160/508153]\n",
      "loss: 0.047366  [296640/508153]\n",
      "loss: 0.030826  [297120/508153]\n",
      "loss: 0.032855  [297600/508153]\n",
      "loss: 0.030219  [298080/508153]\n",
      "loss: 0.030524  [298560/508153]\n",
      "loss: 0.035366  [299040/508153]\n",
      "loss: 0.034108  [299520/508153]\n",
      "loss: 0.036840  [300000/508153]\n",
      "loss: 0.033365  [300480/508153]\n",
      "loss: 0.030323  [300960/508153]\n",
      "loss: 0.027735  [301440/508153]\n",
      "loss: 0.033340  [301920/508153]\n",
      "loss: 0.031634  [302400/508153]\n",
      "loss: 0.033162  [302880/508153]\n",
      "loss: 0.033900  [303360/508153]\n",
      "loss: 0.031570  [303840/508153]\n",
      "loss: 0.034753  [304320/508153]\n",
      "loss: 0.031542  [304800/508153]\n",
      "loss: 0.032802  [305280/508153]\n",
      "loss: 0.036575  [305760/508153]\n",
      "loss: 0.031704  [306240/508153]\n",
      "loss: 0.030393  [306720/508153]\n",
      "loss: 0.031280  [307200/508153]\n",
      "loss: 0.038042  [307680/508153]\n",
      "loss: 0.034962  [308160/508153]\n",
      "loss: 0.029952  [308640/508153]\n",
      "loss: 0.031357  [309120/508153]\n",
      "loss: 0.036027  [309600/508153]\n",
      "loss: 0.033982  [310080/508153]\n",
      "loss: 0.030360  [310560/508153]\n",
      "loss: 0.033527  [311040/508153]\n",
      "loss: 0.029720  [311520/508153]\n",
      "loss: 0.030924  [312000/508153]\n",
      "loss: 0.038200  [312480/508153]\n",
      "loss: 0.030036  [312960/508153]\n",
      "loss: 0.035644  [313440/508153]\n",
      "loss: 0.032512  [313920/508153]\n",
      "loss: 0.029705  [314400/508153]\n",
      "loss: 0.032341  [314880/508153]\n",
      "loss: 0.029617  [315360/508153]\n",
      "loss: 0.029617  [315840/508153]\n",
      "loss: 0.031765  [316320/508153]\n",
      "loss: 0.036003  [316800/508153]\n",
      "loss: 0.035875  [317280/508153]\n",
      "loss: 0.032999  [317760/508153]\n",
      "loss: 0.029891  [318240/508153]\n",
      "loss: 0.029711  [318720/508153]\n",
      "loss: 0.024747  [319200/508153]\n",
      "loss: 0.029192  [319680/508153]\n",
      "loss: 0.031326  [320160/508153]\n",
      "loss: 0.036460  [320640/508153]\n",
      "loss: 0.030578  [321120/508153]\n",
      "loss: 0.034488  [321600/508153]\n",
      "loss: 0.037115  [322080/508153]\n",
      "loss: 0.031284  [322560/508153]\n",
      "loss: 0.033939  [323040/508153]\n",
      "loss: 0.029294  [323520/508153]\n",
      "loss: 0.029511  [324000/508153]\n",
      "loss: 0.028012  [324480/508153]\n",
      "loss: 0.032009  [324960/508153]\n",
      "loss: 0.034677  [325440/508153]\n",
      "loss: 0.034100  [325920/508153]\n",
      "loss: 0.027809  [326400/508153]\n",
      "loss: 0.031831  [326880/508153]\n",
      "loss: 0.032924  [327360/508153]\n",
      "loss: 0.033158  [327840/508153]\n",
      "loss: 0.030901  [328320/508153]\n",
      "loss: 0.030716  [328800/508153]\n",
      "loss: 0.031358  [329280/508153]\n",
      "loss: 0.029022  [329760/508153]\n",
      "loss: 0.031797  [330240/508153]\n",
      "loss: 0.032381  [330720/508153]\n",
      "loss: 0.030664  [331200/508153]\n",
      "loss: 0.029626  [331680/508153]\n",
      "loss: 0.048972  [332160/508153]\n",
      "loss: 0.032910  [332640/508153]\n",
      "loss: 0.031611  [333120/508153]\n",
      "loss: 0.033518  [333600/508153]\n",
      "loss: 0.029272  [334080/508153]\n",
      "loss: 0.028935  [334560/508153]\n",
      "loss: 0.036488  [335040/508153]\n",
      "loss: 0.028979  [335520/508153]\n",
      "loss: 0.024390  [336000/508153]\n",
      "loss: 0.029382  [336480/508153]\n",
      "loss: 0.028150  [336960/508153]\n",
      "loss: 0.032876  [337440/508153]\n",
      "loss: 0.034003  [337920/508153]\n",
      "loss: 0.032849  [338400/508153]\n",
      "loss: 0.032561  [338880/508153]\n",
      "loss: 0.027072  [339360/508153]\n",
      "loss: 0.039296  [339840/508153]\n",
      "loss: 0.030487  [340320/508153]\n",
      "loss: 0.038726  [340800/508153]\n",
      "loss: 0.036922  [341280/508153]\n",
      "loss: 0.027143  [341760/508153]\n",
      "loss: 0.066676  [342240/508153]\n",
      "loss: 0.038459  [342720/508153]\n",
      "loss: 0.034506  [343200/508153]\n",
      "loss: 0.033952  [343680/508153]\n",
      "loss: 0.033935  [344160/508153]\n",
      "loss: 0.030663  [344640/508153]\n",
      "loss: 0.030325  [345120/508153]\n",
      "loss: 0.048369  [345600/508153]\n",
      "loss: 0.027799  [346080/508153]\n",
      "loss: 0.034586  [346560/508153]\n",
      "loss: 0.035537  [347040/508153]\n",
      "loss: 0.030372  [347520/508153]\n",
      "loss: 0.030191  [348000/508153]\n",
      "loss: 0.030037  [348480/508153]\n",
      "loss: 0.031939  [348960/508153]\n",
      "loss: 0.031240  [349440/508153]\n",
      "loss: 0.036731  [349920/508153]\n",
      "loss: 0.033380  [350400/508153]\n",
      "loss: 0.032091  [350880/508153]\n",
      "loss: 0.027868  [351360/508153]\n",
      "loss: 0.031398  [351840/508153]\n",
      "loss: 0.034635  [352320/508153]\n",
      "loss: 0.032279  [352800/508153]\n",
      "loss: 0.032288  [353280/508153]\n",
      "loss: 0.028383  [353760/508153]\n",
      "loss: 0.030454  [354240/508153]\n",
      "loss: 0.055378  [354720/508153]\n",
      "loss: 0.035834  [355200/508153]\n",
      "loss: 0.032185  [355680/508153]\n",
      "loss: 0.028966  [356160/508153]\n",
      "loss: 0.034058  [356640/508153]\n",
      "loss: 0.032425  [357120/508153]\n",
      "loss: 0.033382  [357600/508153]\n",
      "loss: 0.032263  [358080/508153]\n",
      "loss: 0.036350  [358560/508153]\n",
      "loss: 0.031848  [359040/508153]\n",
      "loss: 0.032168  [359520/508153]\n",
      "loss: 0.032766  [360000/508153]\n",
      "loss: 0.030277  [360480/508153]\n",
      "loss: 0.031664  [360960/508153]\n",
      "loss: 0.035228  [361440/508153]\n",
      "loss: 0.034974  [361920/508153]\n",
      "loss: 0.029623  [362400/508153]\n",
      "loss: 0.037115  [362880/508153]\n",
      "loss: 0.033057  [363360/508153]\n",
      "loss: 0.033358  [363840/508153]\n",
      "loss: 0.027826  [364320/508153]\n",
      "loss: 0.032572  [364800/508153]\n",
      "loss: 0.031024  [365280/508153]\n",
      "loss: 0.040318  [365760/508153]\n",
      "loss: 0.032917  [366240/508153]\n",
      "loss: 0.034140  [366720/508153]\n",
      "loss: 0.030062  [367200/508153]\n",
      "loss: 0.030706  [367680/508153]\n",
      "loss: 0.042186  [368160/508153]\n",
      "loss: 0.029745  [368640/508153]\n",
      "loss: 0.029441  [369120/508153]\n",
      "loss: 0.028934  [369600/508153]\n",
      "loss: 0.031751  [370080/508153]\n",
      "loss: 0.031662  [370560/508153]\n",
      "loss: 0.033323  [371040/508153]\n",
      "loss: 0.033940  [371520/508153]\n",
      "loss: 0.032342  [372000/508153]\n",
      "loss: 0.029861  [372480/508153]\n",
      "loss: 0.030365  [372960/508153]\n",
      "loss: 0.032514  [373440/508153]\n",
      "loss: 0.031751  [373920/508153]\n",
      "loss: 0.032772  [374400/508153]\n",
      "loss: 0.032001  [374880/508153]\n",
      "loss: 0.032357  [375360/508153]\n",
      "loss: 0.035767  [375840/508153]\n",
      "loss: 0.031262  [376320/508153]\n",
      "loss: 0.030156  [376800/508153]\n",
      "loss: 0.031993  [377280/508153]\n",
      "loss: 0.031937  [377760/508153]\n",
      "loss: 0.031159  [378240/508153]\n",
      "loss: 0.033475  [378720/508153]\n",
      "loss: 0.034805  [379200/508153]\n",
      "loss: 0.036423  [379680/508153]\n",
      "loss: 0.030774  [380160/508153]\n",
      "loss: 0.035340  [380640/508153]\n",
      "loss: 0.037155  [381120/508153]\n",
      "loss: 0.034423  [381600/508153]\n",
      "loss: 0.032102  [382080/508153]\n",
      "loss: 0.030533  [382560/508153]\n",
      "loss: 0.032015  [383040/508153]\n",
      "loss: 0.031672  [383520/508153]\n",
      "loss: 0.032608  [384000/508153]\n",
      "loss: 0.032070  [384480/508153]\n",
      "loss: 0.037113  [384960/508153]\n",
      "loss: 0.034712  [385440/508153]\n",
      "loss: 0.028386  [385920/508153]\n",
      "loss: 0.029260  [386400/508153]\n",
      "loss: 0.032520  [386880/508153]\n",
      "loss: 0.032435  [387360/508153]\n",
      "loss: 0.030285  [387840/508153]\n",
      "loss: 0.030192  [388320/508153]\n",
      "loss: 0.034550  [388800/508153]\n",
      "loss: 0.035271  [389280/508153]\n",
      "loss: 0.032857  [389760/508153]\n",
      "loss: 0.029065  [390240/508153]\n",
      "loss: 0.033466  [390720/508153]\n",
      "loss: 0.029850  [391200/508153]\n",
      "loss: 0.025119  [391680/508153]\n",
      "loss: 0.036772  [392160/508153]\n",
      "loss: 0.031265  [392640/508153]\n",
      "loss: 0.029411  [393120/508153]\n",
      "loss: 0.034991  [393600/508153]\n",
      "loss: 0.033015  [394080/508153]\n",
      "loss: 0.031433  [394560/508153]\n",
      "loss: 0.032041  [395040/508153]\n",
      "loss: 0.031314  [395520/508153]\n",
      "loss: 0.034615  [396000/508153]\n",
      "loss: 0.029585  [396480/508153]\n",
      "loss: 0.030914  [396960/508153]\n",
      "loss: 0.030079  [397440/508153]\n",
      "loss: 0.029653  [397920/508153]\n",
      "loss: 0.031148  [398400/508153]\n",
      "loss: 0.031087  [398880/508153]\n",
      "loss: 0.026529  [399360/508153]\n",
      "loss: 0.035878  [399840/508153]\n",
      "loss: 0.039007  [400320/508153]\n",
      "loss: 0.032610  [400800/508153]\n",
      "loss: 0.035721  [401280/508153]\n",
      "loss: 0.031274  [401760/508153]\n",
      "loss: 0.036579  [402240/508153]\n",
      "loss: 0.031703  [402720/508153]\n",
      "loss: 0.030849  [403200/508153]\n",
      "loss: 0.031802  [403680/508153]\n",
      "loss: 0.031359  [404160/508153]\n",
      "loss: 0.033924  [404640/508153]\n",
      "loss: 0.029836  [405120/508153]\n",
      "loss: 0.031929  [405600/508153]\n",
      "loss: 0.031189  [406080/508153]\n",
      "loss: 0.029604  [406560/508153]\n",
      "loss: 0.029908  [407040/508153]\n",
      "loss: 0.031599  [407520/508153]\n",
      "loss: 0.037648  [408000/508153]\n",
      "loss: 0.029693  [408480/508153]\n",
      "loss: 0.031492  [408960/508153]\n",
      "loss: 0.030830  [409440/508153]\n",
      "loss: 0.033383  [409920/508153]\n",
      "loss: 0.030332  [410400/508153]\n",
      "loss: 0.029960  [410880/508153]\n",
      "loss: 0.029394  [411360/508153]\n",
      "loss: 0.032710  [411840/508153]\n",
      "loss: 0.041068  [412320/508153]\n",
      "loss: 0.032661  [412800/508153]\n",
      "loss: 0.037565  [413280/508153]\n",
      "loss: 0.030466  [413760/508153]\n",
      "loss: 0.032485  [414240/508153]\n",
      "loss: 0.032018  [414720/508153]\n",
      "loss: 0.030771  [415200/508153]\n",
      "loss: 0.032615  [415680/508153]\n",
      "loss: 0.029371  [416160/508153]\n",
      "loss: 0.031561  [416640/508153]\n",
      "loss: 0.033030  [417120/508153]\n",
      "loss: 0.034745  [417600/508153]\n",
      "loss: 0.030534  [418080/508153]\n",
      "loss: 0.031048  [418560/508153]\n",
      "loss: 0.032276  [419040/508153]\n",
      "loss: 0.031007  [419520/508153]\n",
      "loss: 0.032154  [420000/508153]\n",
      "loss: 0.032665  [420480/508153]\n",
      "loss: 0.027263  [420960/508153]\n",
      "loss: 0.031752  [421440/508153]\n",
      "loss: 0.043132  [421920/508153]\n",
      "loss: 0.030905  [422400/508153]\n",
      "loss: 0.026082  [422880/508153]\n",
      "loss: 0.038432  [423360/508153]\n",
      "loss: 0.031075  [423840/508153]\n",
      "loss: 0.031784  [424320/508153]\n",
      "loss: 0.030947  [424800/508153]\n",
      "loss: 0.032239  [425280/508153]\n",
      "loss: 0.029811  [425760/508153]\n",
      "loss: 0.030988  [426240/508153]\n",
      "loss: 0.027125  [426720/508153]\n",
      "loss: 0.033394  [427200/508153]\n",
      "loss: 0.028528  [427680/508153]\n",
      "loss: 0.033618  [428160/508153]\n",
      "loss: 0.031507  [428640/508153]\n",
      "loss: 0.029873  [429120/508153]\n",
      "loss: 0.033521  [429600/508153]\n",
      "loss: 0.031908  [430080/508153]\n",
      "loss: 0.032650  [430560/508153]\n",
      "loss: 0.028296  [431040/508153]\n",
      "loss: 0.030537  [431520/508153]\n",
      "loss: 0.033729  [432000/508153]\n",
      "loss: 0.031354  [432480/508153]\n",
      "loss: 0.027601  [432960/508153]\n",
      "loss: 0.037121  [433440/508153]\n",
      "loss: 0.030066  [433920/508153]\n",
      "loss: 0.030478  [434400/508153]\n",
      "loss: 0.030809  [434880/508153]\n",
      "loss: 0.032350  [435360/508153]\n",
      "loss: 0.034085  [435840/508153]\n",
      "loss: 0.052329  [436320/508153]\n",
      "loss: 0.032565  [436800/508153]\n",
      "loss: 0.031005  [437280/508153]\n",
      "loss: 0.034824  [437760/508153]\n",
      "loss: 0.031481  [438240/508153]\n",
      "loss: 0.030338  [438720/508153]\n",
      "loss: 0.030521  [439200/508153]\n",
      "loss: 0.032244  [439680/508153]\n",
      "loss: 0.034260  [440160/508153]\n",
      "loss: 0.030292  [440640/508153]\n",
      "loss: 0.033589  [441120/508153]\n",
      "loss: 0.035125  [441600/508153]\n",
      "loss: 0.034583  [442080/508153]\n",
      "loss: 0.048410  [442560/508153]\n",
      "loss: 0.030321  [443040/508153]\n",
      "loss: 0.031052  [443520/508153]\n",
      "loss: 0.029788  [444000/508153]\n",
      "loss: 0.030408  [444480/508153]\n",
      "loss: 0.030674  [444960/508153]\n",
      "loss: 0.031498  [445440/508153]\n",
      "loss: 0.034318  [445920/508153]\n",
      "loss: 0.031417  [446400/508153]\n",
      "loss: 0.031092  [446880/508153]\n",
      "loss: 0.029723  [447360/508153]\n",
      "loss: 0.032261  [447840/508153]\n",
      "loss: 0.032309  [448320/508153]\n",
      "loss: 0.030746  [448800/508153]\n",
      "loss: 0.033072  [449280/508153]\n",
      "loss: 0.033101  [449760/508153]\n",
      "loss: 0.032650  [450240/508153]\n",
      "loss: 0.032634  [450720/508153]\n",
      "loss: 0.026542  [451200/508153]\n",
      "loss: 0.033470  [451680/508153]\n",
      "loss: 0.030213  [452160/508153]\n",
      "loss: 0.031260  [452640/508153]\n",
      "loss: 0.031133  [453120/508153]\n",
      "loss: 0.032550  [453600/508153]\n",
      "loss: 0.031239  [454080/508153]\n",
      "loss: 0.033128  [454560/508153]\n",
      "loss: 0.033328  [455040/508153]\n",
      "loss: 0.032546  [455520/508153]\n",
      "loss: 0.031973  [456000/508153]\n",
      "loss: 0.043661  [456480/508153]\n",
      "loss: 0.031495  [456960/508153]\n",
      "loss: 0.030809  [457440/508153]\n",
      "loss: 0.034008  [457920/508153]\n",
      "loss: 0.030515  [458400/508153]\n",
      "loss: 0.031608  [458880/508153]\n",
      "loss: 0.029881  [459360/508153]\n",
      "loss: 0.031654  [459840/508153]\n",
      "loss: 0.030023  [460320/508153]\n",
      "loss: 0.027534  [460800/508153]\n",
      "loss: 0.031291  [461280/508153]\n",
      "loss: 0.033673  [461760/508153]\n",
      "loss: 0.032869  [462240/508153]\n",
      "loss: 0.032833  [462720/508153]\n",
      "loss: 0.031666  [463200/508153]\n",
      "loss: 0.031435  [463680/508153]\n",
      "loss: 0.049538  [464160/508153]\n",
      "loss: 0.032234  [464640/508153]\n",
      "loss: 0.033250  [465120/508153]\n",
      "loss: 0.032811  [465600/508153]\n",
      "loss: 0.035737  [466080/508153]\n",
      "loss: 0.034290  [466560/508153]\n",
      "loss: 0.034680  [467040/508153]\n",
      "loss: 0.038653  [467520/508153]\n",
      "loss: 0.031187  [468000/508153]\n",
      "loss: 0.030108  [468480/508153]\n",
      "loss: 0.033642  [468960/508153]\n",
      "loss: 0.034736  [469440/508153]\n",
      "loss: 0.033202  [469920/508153]\n",
      "loss: 0.032600  [470400/508153]\n",
      "loss: 0.031859  [470880/508153]\n",
      "loss: 0.032015  [471360/508153]\n",
      "loss: 0.032541  [471840/508153]\n",
      "loss: 0.031535  [472320/508153]\n",
      "loss: 0.034463  [472800/508153]\n",
      "loss: 0.031676  [473280/508153]\n",
      "loss: 0.030637  [473760/508153]\n",
      "loss: 0.031989  [474240/508153]\n",
      "loss: 0.028100  [474720/508153]\n",
      "loss: 0.032005  [475200/508153]\n",
      "loss: 0.030437  [475680/508153]\n",
      "loss: 0.029255  [476160/508153]\n",
      "loss: 0.032305  [476640/508153]\n",
      "loss: 0.032371  [477120/508153]\n",
      "loss: 0.034259  [477600/508153]\n",
      "loss: 0.032872  [478080/508153]\n",
      "loss: 0.034921  [478560/508153]\n",
      "loss: 0.033589  [479040/508153]\n",
      "loss: 0.032296  [479520/508153]\n",
      "loss: 0.030513  [480000/508153]\n",
      "loss: 0.032156  [480480/508153]\n",
      "loss: 0.032940  [480960/508153]\n",
      "loss: 0.028226  [481440/508153]\n",
      "loss: 0.034337  [481920/508153]\n",
      "loss: 0.031819  [482400/508153]\n",
      "loss: 0.028049  [482880/508153]\n",
      "loss: 0.031403  [483360/508153]\n",
      "loss: 0.032565  [483840/508153]\n",
      "loss: 0.030877  [484320/508153]\n",
      "loss: 0.027377  [484800/508153]\n",
      "loss: 0.026902  [485280/508153]\n",
      "loss: 0.029733  [485760/508153]\n",
      "loss: 0.030026  [486240/508153]\n",
      "loss: 0.031140  [486720/508153]\n",
      "loss: 0.030547  [487200/508153]\n",
      "loss: 0.030724  [487680/508153]\n",
      "loss: 0.037252  [488160/508153]\n",
      "loss: 0.031971  [488640/508153]\n",
      "loss: 0.030282  [489120/508153]\n",
      "loss: 0.034954  [489600/508153]\n",
      "loss: 0.033616  [490080/508153]\n",
      "loss: 0.031950  [490560/508153]\n",
      "loss: 0.031458  [491040/508153]\n",
      "loss: 0.027939  [491520/508153]\n",
      "loss: 0.030230  [492000/508153]\n",
      "loss: 0.028807  [492480/508153]\n",
      "loss: 0.032179  [492960/508153]\n",
      "loss: 0.030947  [493440/508153]\n",
      "loss: 0.035751  [493920/508153]\n",
      "loss: 0.034654  [494400/508153]\n",
      "loss: 0.031811  [494880/508153]\n",
      "loss: 0.037516  [495360/508153]\n",
      "loss: 0.029134  [495840/508153]\n",
      "loss: 0.032610  [496320/508153]\n",
      "loss: 0.033128  [496800/508153]\n",
      "loss: 0.044305  [497280/508153]\n",
      "loss: 0.035112  [497760/508153]\n",
      "loss: 0.033032  [498240/508153]\n",
      "loss: 0.029986  [498720/508153]\n",
      "loss: 0.029655  [499200/508153]\n",
      "loss: 0.034244  [499680/508153]\n",
      "loss: 0.030566  [500160/508153]\n",
      "loss: 0.032033  [500640/508153]\n",
      "loss: 0.032953  [501120/508153]\n",
      "loss: 0.027819  [501600/508153]\n",
      "loss: 0.036935  [502080/508153]\n",
      "loss: 0.030473  [502560/508153]\n",
      "loss: 0.033432  [503040/508153]\n",
      "loss: 0.031848  [503520/508153]\n",
      "loss: 0.032871  [504000/508153]\n",
      "loss: 0.031663  [504480/508153]\n",
      "loss: 0.030886  [504960/508153]\n",
      "loss: 0.029585  [505440/508153]\n",
      "loss: 0.029951  [505920/508153]\n",
      "loss: 0.027629  [506400/508153]\n",
      "loss: 0.035188  [506880/508153]\n",
      "loss: 0.030947  [507360/508153]\n",
      "loss: 0.029706  [507840/508153]\n",
      "Avg loss: 1.457842 \n",
      "\n",
      "Epoch 2\\-------------------------------\n",
      "loss: 0.032271  [    0/508153]\n",
      "loss: 0.031667  [  480/508153]\n",
      "loss: 0.031036  [  960/508153]\n",
      "loss: 0.032527  [ 1440/508153]\n",
      "loss: 0.033079  [ 1920/508153]\n",
      "loss: 0.031541  [ 2400/508153]\n",
      "loss: 0.030923  [ 2880/508153]\n",
      "loss: 0.035744  [ 3360/508153]\n",
      "loss: 0.032352  [ 3840/508153]\n",
      "loss: 0.032664  [ 4320/508153]\n",
      "loss: 0.033932  [ 4800/508153]\n",
      "loss: 0.032939  [ 5280/508153]\n",
      "loss: 0.035341  [ 5760/508153]\n",
      "loss: 0.032079  [ 6240/508153]\n",
      "loss: 0.029122  [ 6720/508153]\n",
      "loss: 0.031287  [ 7200/508153]\n",
      "loss: 0.031744  [ 7680/508153]\n",
      "loss: 0.031738  [ 8160/508153]\n",
      "loss: 0.035729  [ 8640/508153]\n",
      "loss: 0.031223  [ 9120/508153]\n",
      "loss: 0.029344  [ 9600/508153]\n",
      "loss: 0.030197  [10080/508153]\n",
      "loss: 0.031409  [10560/508153]\n",
      "loss: 0.033780  [11040/508153]\n",
      "loss: 0.025266  [11520/508153]\n",
      "loss: 0.030287  [12000/508153]\n",
      "loss: 0.028091  [12480/508153]\n",
      "loss: 0.033864  [12960/508153]\n",
      "loss: 0.031418  [13440/508153]\n",
      "loss: 0.040941  [13920/508153]\n",
      "loss: 0.033128  [14400/508153]\n",
      "loss: 0.033079  [14880/508153]\n",
      "loss: 0.032322  [15360/508153]\n",
      "loss: 0.032050  [15840/508153]\n",
      "loss: 0.028654  [16320/508153]\n",
      "loss: 0.032895  [16800/508153]\n",
      "loss: 0.031400  [17280/508153]\n",
      "loss: 0.032058  [17760/508153]\n",
      "loss: 0.031259  [18240/508153]\n",
      "loss: 0.030827  [18720/508153]\n",
      "loss: 0.028843  [19200/508153]\n",
      "loss: 0.030295  [19680/508153]\n",
      "loss: 0.031678  [20160/508153]\n",
      "loss: 0.033740  [20640/508153]\n",
      "loss: 0.030320  [21120/508153]\n",
      "loss: 0.034771  [21600/508153]\n",
      "loss: 0.031794  [22080/508153]\n",
      "loss: 0.032782  [22560/508153]\n",
      "loss: 0.033264  [23040/508153]\n",
      "loss: 0.031388  [23520/508153]\n",
      "loss: 0.032746  [24000/508153]\n",
      "loss: 0.035776  [24480/508153]\n",
      "loss: 0.032678  [24960/508153]\n",
      "loss: 0.033190  [25440/508153]\n",
      "loss: 0.031649  [25920/508153]\n",
      "loss: 0.029238  [26400/508153]\n",
      "loss: 0.031347  [26880/508153]\n",
      "loss: 0.029680  [27360/508153]\n",
      "loss: 0.030400  [27840/508153]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_10980/651807150.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"Epoch {t+1}\\-------------------------------\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m     \u001b[0mtrain_loop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_dataloader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mimage_model\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcaption_model\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mimg_optimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcpt_optimizer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m     \u001b[0mtest_loop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_dataloader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mimage_model\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcaption_model\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Done!\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_10980/57400724.py\u001b[0m in \u001b[0;36mtrain_loop\u001b[1;34m(dataloader, img_model, cpt_model, loss_fn, img_opt, cpt_opt)\u001b[0m\n\u001b[0;32m     15\u001b[0m         \u001b[0mimg_opt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m         \u001b[0mcpt_opt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m         \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m         \u001b[0mimg_opt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m         \u001b[0mcpt_opt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\Anaconda\\lib\\site-packages\\torch\\_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    361\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    362\u001b[0m                 inputs=inputs)\n\u001b[1;32m--> 363\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    364\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    365\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\Anaconda\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    171\u001b[0m     \u001b[1;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    172\u001b[0m     \u001b[1;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 173\u001b[1;33m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[0;32m    174\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    175\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "print(\"start\")\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\-------------------------------\")\n",
    "    train_loop(train_dataloader, image_model, caption_model, loss_fn, img_optimizer, cpt_optimizer)\n",
    "    test_loop(test_dataloader, image_model, caption_model, loss_fn)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "203"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "# 現在の日付を取得します\n",
    "now = datetime.now()\n",
    "\n",
    "# YYYY-MM-DD形式で日付を出力します\n",
    "formatted_date = now.strftime(\"%Y-%m-%d\")\n",
    "\n",
    "torch.save(caption_model.state_dict(), f'model_caption_{formatted_date}.pth')\n",
    "torch.save(image_model.state_dict(), f'model_image_{formatted_date}.pth')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
