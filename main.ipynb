{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch.nn as nn\n",
    "from torchvision import models\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")  # GPUデバイスを取得\n",
    "else:\n",
    "    device = torch.device(\"cpu\")  # CPUデバイスを取得"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "画像処理のモデル\n",
    "\"\"\"\n",
    "\n",
    "class ImageEncoder(nn.Module):\n",
    "    def __init__(self, embedding_size):\n",
    "        super(ImageEncoder, self).__init__()\n",
    "        self.resnet50 = models.resnet50(pretrained=True)\n",
    "        self.fc = nn.Linear(self.resnet50.fc.out_features, embedding_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.resnet50(x)\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "テキスト処理のモデル\n",
    "\"\"\"\n",
    "class CaptionEncoder(nn.Module):\n",
    "  def __init__(self):\n",
    "    super().__init__()\n",
    "    self.bert = AutoModel.from_pretrained(\"cl-tohoku/bert-base-japanese-v2\")\n",
    "  def forward(self, x):\n",
    "    x = self.bert(x)\n",
    "    x = torch.max(x.last_hidden_state, dim=1)[0]  # max pooling\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(635192, 3)\n"
     ]
    }
   ],
   "source": [
    "from CustomDataset import EmbeddingDataset\n",
    "\n",
    "\n",
    "dataset = EmbeddingDataset('./data/anotation_new.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "learning_rate = 1e-5\n",
    "batch_size = 32\n",
    "epochs = 10\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "\n",
    "train_dataset, test_dataset = torch.utils.data.random_split(\n",
    "    dataset, [train_size, val_size]\n",
    ")\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, pin_memory=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at cl-tohoku/bert-base-japanese-v2 were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import torch.nn.functional as F\n",
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.abspath(\"../\"))\n",
    "from models.ContrastiveLoss import ContrastiveLoss\n",
    "\n",
    "\n",
    "\n",
    "image_model = ImageEncoder(768).to(device)\n",
    "caption_model = CaptionEncoder().to(device)\n",
    "img_optimizer = torch.optim.SGD(image_model.parameters(), lr=learning_rate)\n",
    "cpt_optimizer = torch.optim.SGD(caption_model.parameters(), lr=learning_rate)\n",
    "\n",
    "loss_fn = ContrastiveLoss()\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"cl-tohoku/bert-base-japanese-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(dataloader, img_model, cpt_model,  loss_fn, img_opt, cpt_opt):\n",
    "    size = len(dataloader.dataset)\n",
    "    for batch, (img, cap, label, _) in enumerate(dataloader):        \n",
    "        # 予測と損失の計算\n",
    "        img = img.to(device)\n",
    "        label = label.to(device)\n",
    "        pred = img_model(img)\n",
    "        ids = tokenizer.encode(cap, return_tensors='pt')\n",
    "        ids = ids.to(device)\n",
    "        target = cpt_model(ids)\n",
    "        # print(pred.shape, target.shape, len(X), len(y))\n",
    "        # ここ不安\n",
    "        loss = loss_fn(pred, target, label)\n",
    "\n",
    "        # バックプロパゲーション\n",
    "        img_opt.zero_grad()\n",
    "        cpt_opt.zero_grad()\n",
    "        loss.backward()\n",
    "        img_opt.step()\n",
    "        cpt_opt.step()\n",
    "\n",
    "        if batch % 30 == 0:\n",
    "            loss, current = loss.item() / len(img), batch * len(img)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "\n",
    "\n",
    "def test_loop(dataloader, img_model, cpt_model,  loss_fn):\n",
    "    size = len(dataloader.dataset)\n",
    "    test_loss = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for (img, cap, label, _) in dataloader:\n",
    "            img = img.to(device)\n",
    "            label = label.to(device)\n",
    "            pred = img_model(img)\n",
    "            ids = tokenizer.encode(cap, return_tensors='pt')\n",
    "            ids = ids.to(device)\n",
    "            target = cpt_model(ids)\n",
    "            # print(pred.shape, target.shape, len(X), len(y))\n",
    "            # ここ不安\n",
    "            loss = loss_fn(pred, target, label).mean()\n",
    "            test_loss += loss.item()\n",
    "            \n",
    "    test_loss /= size\n",
    "    print(f\"Avg loss: {test_loss:>8f} \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start\n",
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 9.359360  [    0/508153]\n",
      "loss: 4.711126  [  960/508153]\n",
      "loss: 4.329562  [ 1920/508153]\n",
      "loss: 2.594324  [ 2880/508153]\n",
      "loss: 1.986495  [ 3840/508153]\n",
      "loss: 1.738975  [ 4800/508153]\n",
      "loss: 1.120986  [ 5760/508153]\n",
      "loss: 1.226524  [ 6720/508153]\n",
      "loss: 0.736406  [ 7680/508153]\n",
      "loss: 0.855570  [ 8640/508153]\n",
      "loss: 0.960222  [ 9600/508153]\n",
      "loss: 0.921505  [10560/508153]\n",
      "loss: 0.737187  [11520/508153]\n",
      "loss: 0.756590  [12480/508153]\n",
      "loss: 0.652004  [13440/508153]\n",
      "loss: 0.698407  [14400/508153]\n",
      "loss: 0.527368  [15360/508153]\n",
      "loss: 0.310774  [16320/508153]\n",
      "loss: 0.363598  [17280/508153]\n",
      "loss: 0.467646  [18240/508153]\n",
      "loss: 0.299791  [19200/508153]\n",
      "loss: 0.331140  [20160/508153]\n",
      "loss: 0.210381  [21120/508153]\n",
      "loss: 0.336266  [22080/508153]\n",
      "loss: 0.225302  [23040/508153]\n",
      "loss: 0.361531  [24000/508153]\n",
      "loss: 0.341535  [24960/508153]\n",
      "loss: 0.291959  [25920/508153]\n",
      "loss: 0.337865  [26880/508153]\n",
      "loss: 0.294940  [27840/508153]\n",
      "loss: 0.249909  [28800/508153]\n",
      "loss: 0.190371  [29760/508153]\n",
      "loss: 0.152221  [30720/508153]\n",
      "loss: 0.193577  [31680/508153]\n",
      "loss: 0.129354  [32640/508153]\n",
      "loss: 0.251428  [33600/508153]\n",
      "loss: 0.169394  [34560/508153]\n",
      "loss: 0.163300  [35520/508153]\n",
      "loss: 0.124705  [36480/508153]\n",
      "loss: 0.124038  [37440/508153]\n",
      "loss: 0.117121  [38400/508153]\n",
      "loss: 0.216920  [39360/508153]\n",
      "loss: 0.156822  [40320/508153]\n",
      "loss: 0.160596  [41280/508153]\n",
      "loss: 0.166735  [42240/508153]\n",
      "loss: 0.113679  [43200/508153]\n",
      "loss: 0.139796  [44160/508153]\n",
      "loss: 0.096524  [45120/508153]\n",
      "loss: 0.110666  [46080/508153]\n",
      "loss: 0.104783  [47040/508153]\n",
      "loss: 0.095704  [48000/508153]\n",
      "loss: 0.126049  [48960/508153]\n",
      "loss: 0.065714  [49920/508153]\n",
      "loss: 0.160229  [50880/508153]\n",
      "loss: 0.096145  [51840/508153]\n",
      "loss: 0.090909  [52800/508153]\n",
      "loss: 0.117270  [53760/508153]\n",
      "loss: 0.080529  [54720/508153]\n",
      "loss: 0.070948  [55680/508153]\n",
      "loss: 0.067708  [56640/508153]\n",
      "loss: 0.067580  [57600/508153]\n",
      "loss: 0.111132  [58560/508153]\n",
      "loss: 0.096676  [59520/508153]\n",
      "loss: 0.066778  [60480/508153]\n",
      "loss: 0.092264  [61440/508153]\n",
      "loss: 0.091791  [62400/508153]\n",
      "loss: 0.066824  [63360/508153]\n",
      "loss: 0.092241  [64320/508153]\n",
      "loss: 0.057425  [65280/508153]\n",
      "loss: 0.093538  [66240/508153]\n",
      "loss: 0.070197  [67200/508153]\n",
      "loss: 0.076972  [68160/508153]\n",
      "loss: 0.052619  [69120/508153]\n",
      "loss: 0.045646  [70080/508153]\n",
      "loss: 0.097968  [71040/508153]\n",
      "loss: 0.056697  [72000/508153]\n",
      "loss: 0.050899  [72960/508153]\n",
      "loss: 0.057957  [73920/508153]\n",
      "loss: 0.066049  [74880/508153]\n",
      "loss: 0.060198  [75840/508153]\n",
      "loss: 0.056756  [76800/508153]\n",
      "loss: 0.055539  [77760/508153]\n",
      "loss: 0.031869  [78720/508153]\n",
      "loss: 0.067573  [79680/508153]\n",
      "loss: 0.078216  [80640/508153]\n",
      "loss: 0.041886  [81600/508153]\n",
      "loss: 0.052073  [82560/508153]\n",
      "loss: 0.040243  [83520/508153]\n",
      "loss: 0.037016  [84480/508153]\n",
      "loss: 0.047804  [85440/508153]\n",
      "loss: 0.038089  [86400/508153]\n",
      "loss: 0.053380  [87360/508153]\n",
      "loss: 0.033339  [88320/508153]\n",
      "loss: 0.036678  [89280/508153]\n",
      "loss: 0.033066  [90240/508153]\n",
      "loss: 0.028058  [91200/508153]\n",
      "loss: 0.032647  [92160/508153]\n",
      "loss: 0.045340  [93120/508153]\n",
      "loss: 0.041926  [94080/508153]\n",
      "loss: 0.047861  [95040/508153]\n",
      "loss: 0.052886  [96000/508153]\n",
      "loss: 0.037000  [96960/508153]\n",
      "loss: 0.053152  [97920/508153]\n",
      "loss: 0.029106  [98880/508153]\n",
      "loss: 0.026620  [99840/508153]\n",
      "loss: 0.052340  [100800/508153]\n",
      "loss: 0.051355  [101760/508153]\n",
      "loss: 0.050341  [102720/508153]\n",
      "loss: 0.029613  [103680/508153]\n",
      "loss: 0.044967  [104640/508153]\n",
      "loss: 0.056739  [105600/508153]\n",
      "loss: 0.031656  [106560/508153]\n",
      "loss: 0.068655  [107520/508153]\n",
      "loss: 0.047831  [108480/508153]\n",
      "loss: 0.039358  [109440/508153]\n",
      "loss: 0.047758  [110400/508153]\n",
      "loss: 0.024718  [111360/508153]\n",
      "loss: 0.036916  [112320/508153]\n",
      "loss: 0.034850  [113280/508153]\n",
      "loss: 0.057049  [114240/508153]\n",
      "loss: 0.033081  [115200/508153]\n",
      "loss: 0.036378  [116160/508153]\n",
      "loss: 0.037400  [117120/508153]\n",
      "loss: 0.061200  [118080/508153]\n",
      "loss: 0.039242  [119040/508153]\n",
      "loss: 0.027422  [120000/508153]\n",
      "loss: 0.034369  [120960/508153]\n",
      "loss: 0.023808  [121920/508153]\n",
      "loss: 0.022157  [122880/508153]\n",
      "loss: 0.030864  [123840/508153]\n",
      "loss: 0.043137  [124800/508153]\n",
      "loss: 0.034834  [125760/508153]\n",
      "loss: 0.028972  [126720/508153]\n",
      "loss: 0.031963  [127680/508153]\n",
      "loss: 0.019800  [128640/508153]\n",
      "loss: 0.028495  [129600/508153]\n",
      "loss: 0.031516  [130560/508153]\n",
      "loss: 0.035078  [131520/508153]\n",
      "loss: 0.048265  [132480/508153]\n",
      "loss: 0.030961  [133440/508153]\n",
      "loss: 0.028512  [134400/508153]\n",
      "loss: 0.038455  [135360/508153]\n",
      "loss: 0.023429  [136320/508153]\n",
      "loss: 0.034159  [137280/508153]\n",
      "loss: 0.034902  [138240/508153]\n",
      "loss: 0.036431  [139200/508153]\n",
      "loss: 0.020840  [140160/508153]\n",
      "loss: 0.042766  [141120/508153]\n",
      "loss: 0.036735  [142080/508153]\n",
      "loss: 0.026306  [143040/508153]\n",
      "loss: 0.022265  [144000/508153]\n",
      "loss: 0.034040  [144960/508153]\n",
      "loss: 0.023232  [145920/508153]\n",
      "loss: 0.031107  [146880/508153]\n",
      "loss: 0.019226  [147840/508153]\n",
      "loss: 0.022494  [148800/508153]\n",
      "loss: 0.025087  [149760/508153]\n",
      "loss: 0.023763  [150720/508153]\n",
      "loss: 0.019806  [151680/508153]\n",
      "loss: 0.017879  [152640/508153]\n",
      "loss: 0.037900  [153600/508153]\n",
      "loss: 0.015948  [154560/508153]\n",
      "loss: 0.021641  [155520/508153]\n",
      "loss: 0.028802  [156480/508153]\n",
      "loss: 0.030961  [157440/508153]\n",
      "loss: 0.029892  [158400/508153]\n",
      "loss: 0.019860  [159360/508153]\n",
      "loss: 0.033291  [160320/508153]\n",
      "loss: 0.029793  [161280/508153]\n",
      "loss: 0.014563  [162240/508153]\n",
      "loss: 0.026945  [163200/508153]\n",
      "loss: 0.042321  [164160/508153]\n",
      "loss: 0.021241  [165120/508153]\n",
      "loss: 0.024456  [166080/508153]\n",
      "loss: 0.028157  [167040/508153]\n",
      "loss: 0.031039  [168000/508153]\n",
      "loss: 0.021690  [168960/508153]\n",
      "loss: 0.037499  [169920/508153]\n",
      "loss: 0.028454  [170880/508153]\n",
      "loss: 0.029148  [171840/508153]\n",
      "loss: 0.021526  [172800/508153]\n",
      "loss: 0.023577  [173760/508153]\n",
      "loss: 0.021926  [174720/508153]\n",
      "loss: 0.028338  [175680/508153]\n",
      "loss: 0.037813  [176640/508153]\n",
      "loss: 0.031031  [177600/508153]\n",
      "loss: 0.021271  [178560/508153]\n",
      "loss: 0.030506  [179520/508153]\n",
      "loss: 0.022224  [180480/508153]\n",
      "loss: 0.020047  [181440/508153]\n",
      "loss: 0.022942  [182400/508153]\n",
      "loss: 0.028203  [183360/508153]\n",
      "loss: 0.022607  [184320/508153]\n",
      "loss: 0.014550  [185280/508153]\n",
      "loss: 0.028852  [186240/508153]\n",
      "loss: 0.033987  [187200/508153]\n",
      "loss: 0.012777  [188160/508153]\n",
      "loss: 0.023710  [189120/508153]\n",
      "loss: 0.023481  [190080/508153]\n",
      "loss: 0.019202  [191040/508153]\n",
      "loss: 0.023700  [192000/508153]\n",
      "loss: 0.016536  [192960/508153]\n",
      "loss: 0.035334  [193920/508153]\n",
      "loss: 0.023593  [194880/508153]\n",
      "loss: 0.014723  [195840/508153]\n",
      "loss: 0.025143  [196800/508153]\n",
      "loss: 0.034995  [197760/508153]\n",
      "loss: 0.019827  [198720/508153]\n",
      "loss: 0.020459  [199680/508153]\n",
      "loss: 0.023917  [200640/508153]\n",
      "loss: 0.019838  [201600/508153]\n",
      "loss: 0.020514  [202560/508153]\n",
      "loss: 0.016461  [203520/508153]\n",
      "loss: 0.044557  [204480/508153]\n",
      "loss: 0.021186  [205440/508153]\n",
      "loss: 0.019279  [206400/508153]\n",
      "loss: 0.023668  [207360/508153]\n",
      "loss: 0.028380  [208320/508153]\n",
      "loss: 0.022710  [209280/508153]\n",
      "loss: 0.021620  [210240/508153]\n",
      "loss: 0.030271  [211200/508153]\n",
      "loss: 0.025372  [212160/508153]\n",
      "loss: 0.014402  [213120/508153]\n",
      "loss: 0.019336  [214080/508153]\n",
      "loss: 0.019553  [215040/508153]\n",
      "loss: 0.018688  [216000/508153]\n",
      "loss: 0.019548  [216960/508153]\n",
      "loss: 0.012229  [217920/508153]\n",
      "loss: 0.018359  [218880/508153]\n",
      "loss: 0.022561  [219840/508153]\n",
      "loss: 0.019550  [220800/508153]\n",
      "loss: 0.021242  [221760/508153]\n",
      "loss: 0.029281  [222720/508153]\n",
      "loss: 0.023385  [223680/508153]\n",
      "loss: 0.040925  [224640/508153]\n",
      "loss: 0.021839  [225600/508153]\n",
      "loss: 0.019169  [226560/508153]\n",
      "loss: 0.017774  [227520/508153]\n",
      "loss: 0.027317  [228480/508153]\n",
      "loss: 0.021751  [229440/508153]\n",
      "loss: 0.025147  [230400/508153]\n",
      "loss: 0.019710  [231360/508153]\n",
      "loss: 0.022084  [232320/508153]\n",
      "loss: 0.017417  [233280/508153]\n",
      "loss: 0.023970  [234240/508153]\n",
      "loss: 0.018544  [235200/508153]\n",
      "loss: 0.024309  [236160/508153]\n",
      "loss: 0.020437  [237120/508153]\n",
      "loss: 0.020421  [238080/508153]\n",
      "loss: 0.023673  [239040/508153]\n",
      "loss: 0.018937  [240000/508153]\n",
      "loss: 0.020240  [240960/508153]\n",
      "loss: 0.020493  [241920/508153]\n",
      "loss: 0.016459  [242880/508153]\n",
      "loss: 0.015303  [243840/508153]\n",
      "loss: 0.017102  [244800/508153]\n",
      "loss: 0.018418  [245760/508153]\n",
      "loss: 0.033098  [246720/508153]\n",
      "loss: 0.016823  [247680/508153]\n",
      "loss: 0.018651  [248640/508153]\n",
      "loss: 0.017201  [249600/508153]\n",
      "loss: 0.019729  [250560/508153]\n",
      "loss: 0.033920  [251520/508153]\n",
      "loss: 0.033130  [252480/508153]\n",
      "loss: 0.020419  [253440/508153]\n",
      "loss: 0.022861  [254400/508153]\n",
      "loss: 0.015778  [255360/508153]\n",
      "loss: 0.017320  [256320/508153]\n",
      "loss: 0.019734  [257280/508153]\n",
      "loss: 0.022917  [258240/508153]\n",
      "loss: 0.020240  [259200/508153]\n",
      "loss: 0.020452  [260160/508153]\n",
      "loss: 0.019174  [261120/508153]\n",
      "loss: 0.020178  [262080/508153]\n",
      "loss: 0.025918  [263040/508153]\n",
      "loss: 0.020966  [264000/508153]\n",
      "loss: 0.023637  [264960/508153]\n",
      "loss: 0.017563  [265920/508153]\n",
      "loss: 0.019895  [266880/508153]\n",
      "loss: 0.016168  [267840/508153]\n",
      "loss: 0.032124  [268800/508153]\n",
      "loss: 0.012488  [269760/508153]\n",
      "loss: 0.022090  [270720/508153]\n",
      "loss: 0.018734  [271680/508153]\n",
      "loss: 0.016059  [272640/508153]\n",
      "loss: 0.019799  [273600/508153]\n",
      "loss: 0.016921  [274560/508153]\n",
      "loss: 0.016225  [275520/508153]\n",
      "loss: 0.021984  [276480/508153]\n",
      "loss: 0.020793  [277440/508153]\n",
      "loss: 0.014198  [278400/508153]\n",
      "loss: 0.020575  [279360/508153]\n",
      "loss: 0.016427  [280320/508153]\n",
      "loss: 0.019216  [281280/508153]\n",
      "loss: 0.014627  [282240/508153]\n",
      "loss: 0.018168  [283200/508153]\n",
      "loss: 0.021281  [284160/508153]\n",
      "loss: 0.011729  [285120/508153]\n",
      "loss: 0.019303  [286080/508153]\n",
      "loss: 0.027107  [287040/508153]\n",
      "loss: 0.021335  [288000/508153]\n",
      "loss: 0.019028  [288960/508153]\n",
      "loss: 0.020133  [289920/508153]\n",
      "loss: 0.018791  [290880/508153]\n",
      "loss: 0.014184  [291840/508153]\n",
      "loss: 0.014298  [292800/508153]\n",
      "loss: 0.017613  [293760/508153]\n",
      "loss: 0.015406  [294720/508153]\n",
      "loss: 0.027212  [295680/508153]\n",
      "loss: 0.023889  [296640/508153]\n",
      "loss: 0.013012  [297600/508153]\n",
      "loss: 0.018188  [298560/508153]\n",
      "loss: 0.020822  [299520/508153]\n",
      "loss: 0.024586  [300480/508153]\n",
      "loss: 0.024600  [301440/508153]\n",
      "loss: 0.011797  [302400/508153]\n",
      "loss: 0.020471  [303360/508153]\n",
      "loss: 0.022324  [304320/508153]\n",
      "loss: 0.019500  [305280/508153]\n",
      "loss: 0.016177  [306240/508153]\n",
      "loss: 0.018465  [307200/508153]\n",
      "loss: 0.020535  [308160/508153]\n",
      "loss: 0.015391  [309120/508153]\n",
      "loss: 0.020103  [310080/508153]\n",
      "loss: 0.015300  [311040/508153]\n",
      "loss: 0.021921  [312000/508153]\n",
      "loss: 0.017395  [312960/508153]\n",
      "loss: 0.016147  [313920/508153]\n",
      "loss: 0.020736  [314880/508153]\n",
      "loss: 0.018300  [315840/508153]\n",
      "loss: 0.021243  [316800/508153]\n",
      "loss: 0.016102  [317760/508153]\n",
      "loss: 0.018919  [318720/508153]\n",
      "loss: 0.015680  [319680/508153]\n",
      "loss: 0.016456  [320640/508153]\n",
      "loss: 0.023496  [321600/508153]\n",
      "loss: 0.016548  [322560/508153]\n",
      "loss: 0.016465  [323520/508153]\n",
      "loss: 0.020514  [324480/508153]\n",
      "loss: 0.017326  [325440/508153]\n",
      "loss: 0.020733  [326400/508153]\n",
      "loss: 0.017456  [327360/508153]\n",
      "loss: 0.023383  [328320/508153]\n",
      "loss: 0.020217  [329280/508153]\n",
      "loss: 0.017146  [330240/508153]\n",
      "loss: 0.017297  [331200/508153]\n",
      "loss: 0.016099  [332160/508153]\n",
      "loss: 0.015854  [333120/508153]\n",
      "loss: 0.020295  [334080/508153]\n",
      "loss: 0.016886  [335040/508153]\n",
      "loss: 0.018448  [336000/508153]\n",
      "loss: 0.017504  [336960/508153]\n",
      "loss: 0.017131  [337920/508153]\n",
      "loss: 0.031558  [338880/508153]\n",
      "loss: 0.015205  [339840/508153]\n",
      "loss: 0.017788  [340800/508153]\n",
      "loss: 0.026354  [341760/508153]\n",
      "loss: 0.015462  [342720/508153]\n",
      "loss: 0.019561  [343680/508153]\n",
      "loss: 0.018280  [344640/508153]\n",
      "loss: 0.031395  [345600/508153]\n",
      "loss: 0.019639  [346560/508153]\n",
      "loss: 0.015444  [347520/508153]\n",
      "loss: 0.015540  [348480/508153]\n",
      "loss: 0.018388  [349440/508153]\n",
      "loss: 0.016974  [350400/508153]\n",
      "loss: 0.016220  [351360/508153]\n",
      "loss: 0.018382  [352320/508153]\n",
      "loss: 0.020073  [353280/508153]\n",
      "loss: 0.017352  [354240/508153]\n",
      "loss: 0.017987  [355200/508153]\n",
      "loss: 0.019563  [356160/508153]\n",
      "loss: 0.019099  [357120/508153]\n",
      "loss: 0.017582  [358080/508153]\n",
      "loss: 0.028716  [359040/508153]\n",
      "loss: 0.019723  [360000/508153]\n",
      "loss: 0.018506  [360960/508153]\n",
      "loss: 0.018975  [361920/508153]\n",
      "loss: 0.025301  [362880/508153]\n",
      "loss: 0.016192  [363840/508153]\n",
      "loss: 0.013564  [364800/508153]\n",
      "loss: 0.016622  [365760/508153]\n",
      "loss: 0.015354  [366720/508153]\n",
      "loss: 0.012749  [367680/508153]\n",
      "loss: 0.026845  [368640/508153]\n",
      "loss: 0.017610  [369600/508153]\n",
      "loss: 0.016146  [370560/508153]\n",
      "loss: 0.015165  [371520/508153]\n",
      "loss: 0.015681  [372480/508153]\n",
      "loss: 0.018494  [373440/508153]\n",
      "loss: 0.017100  [374400/508153]\n",
      "loss: 0.021118  [375360/508153]\n",
      "loss: 0.017254  [376320/508153]\n",
      "loss: 0.016327  [377280/508153]\n",
      "loss: 0.016095  [378240/508153]\n",
      "loss: 0.015983  [379200/508153]\n",
      "loss: 0.024420  [380160/508153]\n",
      "loss: 0.017931  [381120/508153]\n",
      "loss: 0.016347  [382080/508153]\n",
      "loss: 0.018181  [383040/508153]\n",
      "loss: 0.015111  [384000/508153]\n",
      "loss: 0.019495  [384960/508153]\n",
      "loss: 0.020452  [385920/508153]\n",
      "loss: 0.018285  [386880/508153]\n",
      "loss: 0.019431  [387840/508153]\n",
      "loss: 0.015398  [388800/508153]\n",
      "loss: 0.025613  [389760/508153]\n",
      "loss: 0.018097  [390720/508153]\n",
      "loss: 0.019314  [391680/508153]\n",
      "loss: 0.019771  [392640/508153]\n",
      "loss: 0.012889  [393600/508153]\n",
      "loss: 0.025554  [394560/508153]\n",
      "loss: 0.015315  [395520/508153]\n",
      "loss: 0.015584  [396480/508153]\n",
      "loss: 0.016847  [397440/508153]\n",
      "loss: 0.016617  [398400/508153]\n",
      "loss: 0.027962  [399360/508153]\n",
      "loss: 0.019843  [400320/508153]\n",
      "loss: 0.016372  [401280/508153]\n",
      "loss: 0.016868  [402240/508153]\n",
      "loss: 0.028427  [403200/508153]\n",
      "loss: 0.015685  [404160/508153]\n",
      "loss: 0.015358  [405120/508153]\n",
      "loss: 0.017344  [406080/508153]\n",
      "loss: 0.015644  [407040/508153]\n",
      "loss: 0.018732  [408000/508153]\n",
      "loss: 0.018386  [408960/508153]\n",
      "loss: 0.022489  [409920/508153]\n",
      "loss: 0.020268  [410880/508153]\n",
      "loss: 0.020165  [411840/508153]\n",
      "loss: 0.018581  [412800/508153]\n",
      "loss: 0.017190  [413760/508153]\n",
      "loss: 0.014496  [414720/508153]\n",
      "loss: 0.018503  [415680/508153]\n",
      "loss: 0.016299  [416640/508153]\n",
      "loss: 0.012998  [417600/508153]\n",
      "loss: 0.015683  [418560/508153]\n",
      "loss: 0.018232  [419520/508153]\n",
      "loss: 0.020898  [420480/508153]\n",
      "loss: 0.016387  [421440/508153]\n",
      "loss: 0.013507  [422400/508153]\n",
      "loss: 0.015123  [423360/508153]\n",
      "loss: 0.025522  [424320/508153]\n",
      "loss: 0.018198  [425280/508153]\n",
      "loss: 0.016220  [426240/508153]\n",
      "loss: 0.029140  [427200/508153]\n",
      "loss: 0.018732  [428160/508153]\n",
      "loss: 0.017166  [429120/508153]\n",
      "loss: 0.026234  [430080/508153]\n",
      "loss: 0.018113  [431040/508153]\n",
      "loss: 0.015059  [432000/508153]\n",
      "loss: 0.015362  [432960/508153]\n",
      "loss: 0.014108  [433920/508153]\n",
      "loss: 0.017206  [434880/508153]\n",
      "loss: 0.014298  [435840/508153]\n",
      "loss: 0.017438  [436800/508153]\n",
      "loss: 0.018606  [437760/508153]\n",
      "loss: 0.017195  [438720/508153]\n",
      "loss: 0.015102  [439680/508153]\n",
      "loss: 0.017231  [440640/508153]\n",
      "loss: 0.017285  [441600/508153]\n",
      "loss: 0.016873  [442560/508153]\n",
      "loss: 0.016718  [443520/508153]\n",
      "loss: 0.015446  [444480/508153]\n",
      "loss: 0.016748  [445440/508153]\n",
      "loss: 0.015048  [446400/508153]\n",
      "loss: 0.014152  [447360/508153]\n",
      "loss: 0.018282  [448320/508153]\n",
      "loss: 0.016243  [449280/508153]\n",
      "loss: 0.019250  [450240/508153]\n",
      "loss: 0.017612  [451200/508153]\n",
      "loss: 0.015609  [452160/508153]\n",
      "loss: 0.018906  [453120/508153]\n",
      "loss: 0.017875  [454080/508153]\n",
      "loss: 0.013701  [455040/508153]\n",
      "loss: 0.014740  [456000/508153]\n",
      "loss: 0.017222  [456960/508153]\n",
      "loss: 0.014983  [457920/508153]\n",
      "loss: 0.016787  [458880/508153]\n",
      "loss: 0.016503  [459840/508153]\n",
      "loss: 0.016818  [460800/508153]\n",
      "loss: 0.016838  [461760/508153]\n",
      "loss: 0.015370  [462720/508153]\n",
      "loss: 0.016462  [463680/508153]\n",
      "loss: 0.017940  [464640/508153]\n",
      "loss: 0.013752  [465600/508153]\n",
      "loss: 0.017990  [466560/508153]\n",
      "loss: 0.015421  [467520/508153]\n",
      "loss: 0.020831  [468480/508153]\n",
      "loss: 0.021309  [469440/508153]\n",
      "loss: 0.016688  [470400/508153]\n",
      "loss: 0.016399  [471360/508153]\n",
      "loss: 0.016937  [472320/508153]\n",
      "loss: 0.021791  [473280/508153]\n",
      "loss: 0.018524  [474240/508153]\n",
      "loss: 0.015211  [475200/508153]\n",
      "loss: 0.015437  [476160/508153]\n",
      "loss: 0.015784  [477120/508153]\n",
      "loss: 0.021290  [478080/508153]\n",
      "loss: 0.012784  [479040/508153]\n",
      "loss: 0.017723  [480000/508153]\n",
      "loss: 0.018588  [480960/508153]\n",
      "loss: 0.016474  [481920/508153]\n",
      "loss: 0.017142  [482880/508153]\n",
      "loss: 0.024926  [483840/508153]\n",
      "loss: 0.015888  [484800/508153]\n",
      "loss: 0.016257  [485760/508153]\n",
      "loss: 0.016338  [486720/508153]\n",
      "loss: 0.015041  [487680/508153]\n",
      "loss: 0.019489  [488640/508153]\n",
      "loss: 0.017867  [489600/508153]\n",
      "loss: 0.017208  [490560/508153]\n",
      "loss: 0.016460  [491520/508153]\n",
      "loss: 0.016433  [492480/508153]\n",
      "loss: 0.016234  [493440/508153]\n",
      "loss: 0.015493  [494400/508153]\n",
      "loss: 0.014417  [495360/508153]\n",
      "loss: 0.016121  [496320/508153]\n",
      "loss: 0.018056  [497280/508153]\n",
      "loss: 0.022696  [498240/508153]\n",
      "loss: 0.016439  [499200/508153]\n",
      "loss: 0.016931  [500160/508153]\n",
      "loss: 0.014363  [501120/508153]\n",
      "loss: 0.014875  [502080/508153]\n",
      "loss: 0.018031  [503040/508153]\n",
      "loss: 0.015102  [504000/508153]\n",
      "loss: 0.015893  [504960/508153]\n",
      "loss: 0.017151  [505920/508153]\n",
      "loss: 0.018268  [506880/508153]\n",
      "loss: 0.018238  [507840/508153]\n"
     ]
    }
   ],
   "source": [
    "print(\"start\")\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train_loop(train_dataloader, image_model, caption_model, loss_fn, img_optimizer, cpt_optimizer)\n",
    "    test_loop(test_dataloader, image_model, caption_model, loss_fn)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "# 現在の日付を取得します\n",
    "now = datetime.now()\n",
    "\n",
    "# YYYY-MM-DD形式で日付を出力します\n",
    "formatted_date = now.strftime(\"%Y-%m-%d\")\n",
    "\n",
    "torch.save(caption_model.state_dict(), f'model_caption_{formatted_date}.pth')\n",
    "torch.save(image_model.state_dict(), f'model_image_{formatted_date}.pth')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
