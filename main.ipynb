{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch.nn as nn\n",
    "from torchvision import models\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")  # GPUデバイスを取得\n",
    "else:\n",
    "    device = torch.device(\"cpu\")  # CPUデバイスを取得"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "画像処理のモデル\n",
    "\"\"\"\n",
    "\n",
    "class ImageEncoder(nn.Module):\n",
    "    def __init__(self, embedding_size):\n",
    "        super(ImageEncoder, self).__init__()\n",
    "        self.resnet50 = models.resnet50(pretrained=True)\n",
    "        self.fc = nn.Linear(self.resnet50.fc.out_features, embedding_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.resnet50(x)\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "テキスト処理のモデル\n",
    "\"\"\"\n",
    "class CaptionEncoder(nn.Module):\n",
    "  def __init__(self):\n",
    "    super().__init__()\n",
    "    self.bert = AutoModel.from_pretrained(\"cl-tohoku/bert-base-japanese-v2\")\n",
    "  def forward(self, x):\n",
    "    x = self.bert(x)\n",
    "    x = torch.max(x.last_hidden_state, dim=1)[0]  # max pooling\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from CustomDataset import EmbeddingDataset\n",
    "\n",
    "\n",
    "dataset = EmbeddingDataset('./data/anotation_new.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "\n",
    "train_dataset, test_dataset = torch.utils.data.random_split(\n",
    "    dataset, [train_size, val_size]\n",
    ")\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=64, shuffle=True, pin_memory=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=64, shuffle=True, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at cl-tohoku/bert-base-japanese-v2 were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.decoder.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from torch import cosine_embedding_loss\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# from models.ContrastiveLoss import ContrastiveLoss\n",
    "\n",
    "learning_rate = 1e-3\n",
    "batch_size = 32\n",
    "epochs = 1\n",
    "\n",
    "image_model = ImageEncoder(768).to(device)\n",
    "caption_model = CaptionEncoder().to(device)\n",
    "img_optimizer = torch.optim.SGD(image_model.parameters(), lr=learning_rate)\n",
    "cpt_optimizer = torch.optim.SGD(caption_model.parameters(), lr=learning_rate)\n",
    "\n",
    "# loss_fn = ContrastiveLoss()\n",
    "loss_fn = torch.nn.CosineSimilarity(dim=1)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"cl-tohoku/bert-base-japanese-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(dataloader, img_model, cpt_model,  loss_fn, img_opt, cpt_opt):\n",
    "    size = len(dataloader.dataset)\n",
    "    for batch, (X, y) in enumerate(dataloader):        \n",
    "        # 予測と損失の計算\n",
    "        X = X.to(device)\n",
    "        pred = img_model(X)\n",
    "        ids = tokenizer.encode(y, return_tensors='pt')\n",
    "        ids = ids.to(device)\n",
    "        target = cpt_model(ids)\n",
    "        # print(pred.shape, target.shape, len(X), len(y))\n",
    "        # ここ不安\n",
    "        loss = loss_fn(pred, target).mean()\n",
    "\n",
    "        # バックプロパゲーション\n",
    "        img_opt.zero_grad()\n",
    "        cpt_opt.zero_grad()\n",
    "        loss.backward()\n",
    "        img_opt.step()\n",
    "        cpt_opt.step()\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), batch * len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "\n",
    "\n",
    "def test_loop(dataloader, img_model, cpt_model,  loss_fn):\n",
    "    size = len(dataloader.dataset)\n",
    "    test_loss = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            X = X.to(device)\n",
    "            pred = img_model(X)\n",
    "            ids = tokenizer.encode(y, return_tensors='pt')\n",
    "            ids = ids.to(device)\n",
    "            target = cpt_model(ids)\n",
    "            loss = loss_fn(pred, target).mean()\n",
    "            test_loss += loss.item()\n",
    "            \n",
    "    test_loss /= size\n",
    "    print(f\"Avg loss: {test_loss:>8f} \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 0.016977  [    0/1016309]\n",
      "loss: -0.201239  [ 6400/1016309]\n",
      "loss: -0.346563  [12800/1016309]\n",
      "loss: -0.466060  [19200/1016309]\n",
      "loss: -0.582007  [25600/1016309]\n",
      "loss: -0.670583  [32000/1016309]\n",
      "loss: -0.751494  [38400/1016309]\n",
      "loss: -0.816779  [44800/1016309]\n",
      "loss: -0.845950  [51200/1016309]\n",
      "loss: -0.873586  [57600/1016309]\n",
      "loss: -0.892018  [64000/1016309]\n",
      "loss: -0.915056  [70400/1016309]\n",
      "loss: -0.921607  [76800/1016309]\n",
      "loss: -0.926777  [83200/1016309]\n",
      "loss: -0.938782  [89600/1016309]\n",
      "loss: -0.946150  [96000/1016309]\n",
      "loss: -0.950328  [102400/1016309]\n",
      "loss: -0.952487  [108800/1016309]\n",
      "loss: -0.956814  [115200/1016309]\n",
      "loss: -0.960172  [121600/1016309]\n",
      "loss: -0.961146  [128000/1016309]\n",
      "loss: -0.967722  [134400/1016309]\n",
      "loss: -0.967676  [140800/1016309]\n",
      "loss: -0.966824  [147200/1016309]\n",
      "loss: -0.974732  [153600/1016309]\n",
      "loss: -0.971047  [160000/1016309]\n",
      "loss: -0.970789  [166400/1016309]\n",
      "loss: -0.971973  [172800/1016309]\n",
      "loss: -0.974743  [179200/1016309]\n",
      "loss: -0.977387  [185600/1016309]\n",
      "loss: -0.976763  [192000/1016309]\n",
      "loss: -0.979685  [198400/1016309]\n",
      "loss: -0.979650  [204800/1016309]\n",
      "loss: -0.978877  [211200/1016309]\n",
      "loss: -0.979262  [217600/1016309]\n",
      "loss: -0.980758  [224000/1016309]\n",
      "loss: -0.982808  [230400/1016309]\n",
      "loss: -0.984356  [236800/1016309]\n",
      "loss: -0.984017  [243200/1016309]\n",
      "loss: -0.983215  [249600/1016309]\n",
      "loss: -0.983381  [256000/1016309]\n",
      "loss: -0.984490  [262400/1016309]\n",
      "loss: -0.985362  [268800/1016309]\n",
      "loss: -0.987514  [275200/1016309]\n",
      "loss: -0.987023  [281600/1016309]\n",
      "loss: -0.986804  [288000/1016309]\n",
      "loss: -0.987371  [294400/1016309]\n",
      "loss: -0.985604  [300800/1016309]\n",
      "loss: -0.987916  [307200/1016309]\n",
      "loss: -0.987679  [313600/1016309]\n",
      "loss: -0.988657  [320000/1016309]\n",
      "loss: -0.987940  [326400/1016309]\n",
      "loss: -0.989120  [332800/1016309]\n",
      "loss: -0.988675  [339200/1016309]\n",
      "loss: -0.989160  [345600/1016309]\n",
      "loss: -0.988981  [352000/1016309]\n",
      "loss: -0.989104  [358400/1016309]\n",
      "loss: -0.989731  [364800/1016309]\n",
      "loss: -0.990862  [371200/1016309]\n",
      "loss: -0.989375  [377600/1016309]\n",
      "loss: -0.990082  [384000/1016309]\n",
      "loss: -0.991671  [390400/1016309]\n",
      "loss: -0.990143  [396800/1016309]\n",
      "loss: -0.991292  [403200/1016309]\n",
      "loss: -0.990449  [409600/1016309]\n",
      "loss: -0.991136  [416000/1016309]\n",
      "loss: -0.989962  [422400/1016309]\n",
      "loss: -0.991211  [428800/1016309]\n",
      "loss: -0.991335  [435200/1016309]\n",
      "loss: -0.992455  [441600/1016309]\n",
      "loss: -0.991517  [448000/1016309]\n",
      "loss: -0.991294  [454400/1016309]\n",
      "loss: -0.991642  [460800/1016309]\n",
      "loss: -0.992359  [467200/1016309]\n",
      "loss: -0.993140  [473600/1016309]\n",
      "loss: -0.992901  [480000/1016309]\n",
      "loss: -0.992179  [486400/1016309]\n",
      "loss: -0.992926  [492800/1016309]\n",
      "loss: -0.993234  [499200/1016309]\n",
      "loss: -0.991828  [505600/1016309]\n",
      "loss: -0.993907  [512000/1016309]\n",
      "loss: -0.993823  [518400/1016309]\n",
      "loss: -0.994001  [524800/1016309]\n",
      "loss: -0.993124  [531200/1016309]\n",
      "loss: -0.992830  [537600/1016309]\n",
      "loss: -0.994202  [544000/1016309]\n",
      "loss: -0.993428  [550400/1016309]\n",
      "loss: -0.992955  [556800/1016309]\n",
      "loss: -0.993923  [563200/1016309]\n",
      "loss: -0.995034  [569600/1016309]\n",
      "loss: -0.994381  [576000/1016309]\n",
      "loss: -0.994332  [582400/1016309]\n",
      "loss: -0.994627  [588800/1016309]\n",
      "loss: -0.994214  [595200/1016309]\n",
      "loss: -0.993539  [601600/1016309]\n",
      "loss: -0.994207  [608000/1016309]\n",
      "loss: -0.994388  [614400/1016309]\n",
      "loss: -0.995057  [620800/1016309]\n",
      "loss: -0.994853  [627200/1016309]\n",
      "loss: -0.994251  [633600/1016309]\n",
      "loss: -0.995119  [640000/1016309]\n",
      "loss: -0.994321  [646400/1016309]\n",
      "loss: -0.994593  [652800/1016309]\n",
      "loss: -0.995268  [659200/1016309]\n",
      "loss: -0.995525  [665600/1016309]\n",
      "loss: -0.995279  [672000/1016309]\n",
      "loss: -0.994528  [678400/1016309]\n",
      "loss: -0.995339  [684800/1016309]\n",
      "loss: -0.995713  [691200/1016309]\n",
      "loss: -0.995374  [697600/1016309]\n",
      "loss: -0.994083  [704000/1016309]\n",
      "loss: -0.994880  [710400/1016309]\n",
      "loss: -0.996081  [716800/1016309]\n",
      "loss: -0.995676  [723200/1016309]\n",
      "loss: -0.995004  [729600/1016309]\n",
      "loss: -0.995680  [736000/1016309]\n",
      "loss: -0.995706  [742400/1016309]\n",
      "loss: -0.995587  [748800/1016309]\n",
      "loss: -0.996071  [755200/1016309]\n",
      "loss: -0.996003  [761600/1016309]\n",
      "loss: -0.995632  [768000/1016309]\n",
      "loss: -0.996593  [774400/1016309]\n",
      "loss: -0.995987  [780800/1016309]\n",
      "loss: -0.996299  [787200/1016309]\n",
      "loss: -0.996064  [793600/1016309]\n",
      "loss: -0.995740  [800000/1016309]\n",
      "loss: -0.995914  [806400/1016309]\n",
      "loss: -0.996415  [812800/1016309]\n",
      "loss: -0.996248  [819200/1016309]\n",
      "loss: -0.995371  [825600/1016309]\n",
      "loss: -0.996150  [832000/1016309]\n",
      "loss: -0.995835  [838400/1016309]\n",
      "loss: -0.996082  [844800/1016309]\n",
      "loss: -0.996144  [851200/1016309]\n",
      "loss: -0.996412  [857600/1016309]\n",
      "loss: -0.996174  [864000/1016309]\n",
      "loss: -0.996485  [870400/1016309]\n",
      "loss: -0.996459  [876800/1016309]\n",
      "loss: -0.996043  [883200/1016309]\n",
      "loss: -0.996439  [889600/1016309]\n",
      "loss: -0.996731  [896000/1016309]\n",
      "loss: -0.995366  [902400/1016309]\n",
      "loss: -0.996295  [908800/1016309]\n",
      "loss: -0.996537  [915200/1016309]\n",
      "loss: -0.996477  [921600/1016309]\n",
      "loss: -0.996395  [928000/1016309]\n",
      "loss: -0.996759  [934400/1016309]\n",
      "loss: -0.997007  [940800/1016309]\n",
      "loss: -0.996362  [947200/1016309]\n",
      "loss: -0.996820  [953600/1016309]\n",
      "loss: -0.997195  [960000/1016309]\n",
      "loss: -0.996783  [966400/1016309]\n",
      "loss: -0.997193  [972800/1016309]\n",
      "loss: -0.996482  [979200/1016309]\n",
      "loss: -0.997102  [985600/1016309]\n",
      "loss: -0.997223  [992000/1016309]\n",
      "loss: -0.995987  [998400/1016309]\n",
      "loss: -0.997191  [1004800/1016309]\n",
      "loss: -0.997319  [1011200/1016309]\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCanceled future for execute_request message before replies were done"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train_loop(train_dataloader, image_model, caption_model, loss_fn, img_optimizer, cpt_optimizer)\n",
    "    test_loop(train_dataloader, image_model, caption_model, loss_fn)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'torch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_1144/3510128675.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0mformatted_date\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstrftime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"%Y-%m-%d\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcaption_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34mf'model_caption_{formatted_date}.pth'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimage_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34mf'model_caption_{formatted_date}.pth'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'torch' is not defined"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "# 現在の日付を取得します\n",
    "now = datetime.now()\n",
    "\n",
    "# YYYY-MM-DD形式で日付を出力します\n",
    "formatted_date = now.strftime(\"%Y-%m-%d\")\n",
    "\n",
    "torch.save(caption_model.state_dict(), f'model_caption_{formatted_date}.pth')\n",
    "torch.save(image_model.state_dict(), f'model_caption_{formatted_date}.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-06-05\n"
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
