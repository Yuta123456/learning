{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch.nn as nn\n",
    "from torchvision import models\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")  # GPUデバイスを取得\n",
    "else:\n",
    "    device = torch.device(\"cpu\")  # CPUデバイスを取得"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "画像処理のモデル\n",
    "\"\"\"\n",
    "\n",
    "class ImageEncoder(nn.Module):\n",
    "    def __init__(self, embedding_size):\n",
    "        super(ImageEncoder, self).__init__()\n",
    "        self.resnet50 = models.resnet50(pretrained=True)\n",
    "        self.fc = nn.Linear(self.resnet50.fc.out_features, embedding_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.resnet50(x)\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "テキスト処理のモデル\n",
    "\"\"\"\n",
    "class CaptionEncoder(nn.Module):\n",
    "  def __init__(self):\n",
    "    super().__init__()\n",
    "    self.bert = AutoModel.from_pretrained(\"cl-tohoku/bert-base-japanese-v2\")\n",
    "  def forward(self, x):\n",
    "    x = self.bert(x)\n",
    "    x = torch.max(x.last_hidden_state, dim=1)[0]  # max pooling\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(635192, 3)\n"
     ]
    }
   ],
   "source": [
    "from CustomDataset import EmbeddingDataset\n",
    "\n",
    "\n",
    "dataset = EmbeddingDataset('./data/anotation_new.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "learning_rate = 1e-5\n",
    "batch_size = 32\n",
    "epochs = 10\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "\n",
    "train_dataset, test_dataset = torch.utils.data.random_split(\n",
    "    dataset, [train_size, val_size]\n",
    ")\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, pin_memory=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at cl-tohoku/bert-base-japanese-v2 were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import torch.nn.functional as F\n",
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.abspath(\"../\"))\n",
    "from models.ContrastiveLoss import ContrastiveLoss\n",
    "\n",
    "\n",
    "\n",
    "image_model = ImageEncoder(768).to(device)\n",
    "caption_model = CaptionEncoder().to(device)\n",
    "img_optimizer = torch.optim.SGD(image_model.parameters(), lr=learning_rate)\n",
    "cpt_optimizer = torch.optim.SGD(caption_model.parameters(), lr=learning_rate)\n",
    "\n",
    "loss_fn = ContrastiveLoss()\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"cl-tohoku/bert-base-japanese-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(dataloader, img_model, cpt_model,  loss_fn, img_opt, cpt_opt):\n",
    "    size = len(dataloader.dataset)\n",
    "    for batch, (img, cap, label) in enumerate(dataloader):        \n",
    "        # 予測と損失の計算\n",
    "        img = img.to(device)\n",
    "        label = label.to(device)\n",
    "        pred = img_model(img)\n",
    "        ids = tokenizer.encode(cap, return_tensors='pt')\n",
    "        ids = ids.to(device)\n",
    "        target = cpt_model(ids)\n",
    "        # print(pred.shape, target.shape, len(X), len(y))\n",
    "        # ここ不安\n",
    "        loss = loss_fn(pred, target, label).mean()\n",
    "\n",
    "        # バックプロパゲーション\n",
    "        img_opt.zero_grad()\n",
    "        cpt_opt.zero_grad()\n",
    "        loss.backward()\n",
    "        img_opt.step()\n",
    "        cpt_opt.step()\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), batch * len(img)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "\n",
    "\n",
    "def test_loop(dataloader, img_model, cpt_model,  loss_fn):\n",
    "    size = len(dataloader.dataset)\n",
    "    test_loss = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for (img, cap, label) in dataloader:\n",
    "            img = img.to(device)\n",
    "            label = label.to(device)\n",
    "            pred = img_model(img)\n",
    "            ids = tokenizer.encode(cap, return_tensors='pt')\n",
    "            ids = ids.to(device)\n",
    "            target = cpt_model(ids)\n",
    "            # print(pred.shape, target.shape, len(X), len(y))\n",
    "            # ここ不安\n",
    "            loss = loss_fn(pred, target, label).mean()\n",
    "            test_loss += loss.item()\n",
    "            \n",
    "    test_loss /= size\n",
    "    print(f\"Avg loss: {test_loss:>8f} \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start\n",
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 342.870422  [    0/508153]\n",
      "loss: 70.764656  [ 3200/508153]\n",
      "loss: 49.755508  [ 6400/508153]\n",
      "loss: 16.763018  [ 9600/508153]\n",
      "loss: 19.896872  [12800/508153]\n",
      "loss: 18.127861  [16000/508153]\n",
      "loss: 14.961330  [19200/508153]\n",
      "loss: 12.657497  [22400/508153]\n",
      "loss: 7.990142  [25600/508153]\n",
      "loss: 6.734111  [28800/508153]\n",
      "loss: 8.504412  [32000/508153]\n",
      "loss: 5.309472  [35200/508153]\n",
      "loss: 2.148442  [38400/508153]\n",
      "loss: 6.180494  [41600/508153]\n",
      "loss: 2.344090  [44800/508153]\n",
      "loss: 3.789439  [48000/508153]\n",
      "loss: 4.484220  [51200/508153]\n",
      "loss: 2.415667  [54400/508153]\n",
      "loss: 3.380153  [57600/508153]\n",
      "loss: 2.984679  [60800/508153]\n",
      "loss: 3.439856  [64000/508153]\n",
      "loss: 1.225802  [67200/508153]\n",
      "loss: 2.702730  [70400/508153]\n",
      "loss: 2.728698  [73600/508153]\n",
      "loss: 2.633343  [76800/508153]\n",
      "loss: 2.268293  [80000/508153]\n",
      "loss: 2.046721  [83200/508153]\n",
      "loss: 1.736589  [86400/508153]\n",
      "loss: 2.433505  [89600/508153]\n",
      "loss: 1.475938  [92800/508153]\n",
      "loss: 2.635262  [96000/508153]\n",
      "loss: 2.460637  [99200/508153]\n",
      "loss: 1.657118  [102400/508153]\n",
      "loss: 2.071444  [105600/508153]\n",
      "loss: 1.960426  [108800/508153]\n",
      "loss: 1.786264  [112000/508153]\n",
      "loss: 1.414813  [115200/508153]\n",
      "loss: 1.413895  [118400/508153]\n",
      "loss: 1.549773  [121600/508153]\n",
      "loss: 1.031885  [124800/508153]\n",
      "loss: 1.010870  [128000/508153]\n",
      "loss: 1.633709  [131200/508153]\n",
      "loss: 1.427294  [134400/508153]\n",
      "loss: 0.665206  [137600/508153]\n",
      "loss: 1.181438  [140800/508153]\n",
      "loss: 1.452827  [144000/508153]\n",
      "loss: 1.099042  [147200/508153]\n",
      "loss: 1.232757  [150400/508153]\n",
      "loss: 1.112977  [153600/508153]\n",
      "loss: 1.016330  [156800/508153]\n",
      "loss: 1.026620  [160000/508153]\n",
      "loss: 1.260709  [163200/508153]\n",
      "loss: 0.768569  [166400/508153]\n",
      "loss: 0.842885  [169600/508153]\n",
      "loss: 1.195833  [172800/508153]\n",
      "loss: 0.528039  [176000/508153]\n",
      "loss: 1.095277  [179200/508153]\n",
      "loss: 0.665629  [182400/508153]\n",
      "loss: 0.680757  [185600/508153]\n",
      "loss: 1.087927  [188800/508153]\n",
      "loss: 1.002023  [192000/508153]\n",
      "loss: 1.097402  [195200/508153]\n",
      "loss: 0.959801  [198400/508153]\n",
      "loss: 0.948252  [201600/508153]\n",
      "loss: 1.074558  [204800/508153]\n",
      "loss: 0.625755  [208000/508153]\n",
      "loss: 0.693414  [211200/508153]\n",
      "loss: 0.586376  [214400/508153]\n",
      "loss: 0.639588  [217600/508153]\n",
      "loss: 0.432715  [220800/508153]\n",
      "loss: 0.588169  [224000/508153]\n",
      "loss: 1.024133  [227200/508153]\n",
      "loss: 0.975009  [230400/508153]\n",
      "loss: 0.795072  [233600/508153]\n",
      "loss: 0.618754  [236800/508153]\n",
      "loss: 0.863321  [240000/508153]\n",
      "loss: 0.536587  [243200/508153]\n",
      "loss: 0.555605  [246400/508153]\n",
      "loss: 0.795334  [249600/508153]\n",
      "loss: 0.825568  [252800/508153]\n",
      "loss: 0.543201  [256000/508153]\n",
      "loss: 0.497316  [259200/508153]\n",
      "loss: 0.548433  [262400/508153]\n",
      "loss: 0.900938  [265600/508153]\n",
      "loss: 0.727265  [268800/508153]\n",
      "loss: 0.534612  [272000/508153]\n",
      "loss: 0.495749  [275200/508153]\n",
      "loss: 0.920056  [278400/508153]\n",
      "loss: 0.579401  [281600/508153]\n",
      "loss: 0.668081  [284800/508153]\n",
      "loss: 0.581830  [288000/508153]\n",
      "loss: 0.678601  [291200/508153]\n",
      "loss: 0.770698  [294400/508153]\n",
      "loss: 0.468566  [297600/508153]\n",
      "loss: 0.578348  [300800/508153]\n",
      "loss: 0.589959  [304000/508153]\n",
      "loss: 0.789099  [307200/508153]\n",
      "loss: 0.696502  [310400/508153]\n",
      "loss: 0.623396  [313600/508153]\n",
      "loss: 0.633001  [316800/508153]\n",
      "loss: 0.682479  [320000/508153]\n",
      "loss: 0.517191  [323200/508153]\n",
      "loss: 0.715817  [326400/508153]\n",
      "loss: 0.707592  [329600/508153]\n",
      "loss: 0.827096  [332800/508153]\n",
      "loss: 0.795397  [336000/508153]\n",
      "loss: 0.583964  [339200/508153]\n",
      "loss: 0.778777  [342400/508153]\n",
      "loss: 0.755222  [345600/508153]\n",
      "loss: 0.356390  [348800/508153]\n",
      "loss: 0.738521  [352000/508153]\n",
      "loss: 0.543443  [355200/508153]\n",
      "loss: 0.633093  [358400/508153]\n",
      "loss: 0.568781  [361600/508153]\n",
      "loss: 0.843446  [364800/508153]\n",
      "loss: 0.790433  [368000/508153]\n",
      "loss: 0.662673  [371200/508153]\n",
      "loss: 0.692530  [374400/508153]\n",
      "loss: 0.739343  [377600/508153]\n",
      "loss: 0.421939  [380800/508153]\n",
      "loss: 0.632340  [384000/508153]\n",
      "loss: 0.535252  [387200/508153]\n",
      "loss: 0.695306  [390400/508153]\n",
      "loss: 0.654734  [393600/508153]\n",
      "loss: 0.483695  [396800/508153]\n",
      "loss: 0.561426  [400000/508153]\n",
      "loss: 0.572362  [403200/508153]\n",
      "loss: 0.639912  [406400/508153]\n",
      "loss: 0.639910  [409600/508153]\n",
      "loss: 0.534867  [412800/508153]\n",
      "loss: 0.540466  [416000/508153]\n",
      "loss: 0.541441  [419200/508153]\n",
      "loss: 0.684553  [422400/508153]\n",
      "loss: 0.524988  [425600/508153]\n",
      "loss: 0.588415  [428800/508153]\n",
      "loss: 0.658658  [432000/508153]\n",
      "loss: 0.591996  [435200/508153]\n",
      "loss: 0.660420  [438400/508153]\n",
      "loss: 0.520853  [441600/508153]\n",
      "loss: 0.659102  [444800/508153]\n",
      "loss: 0.611805  [448000/508153]\n",
      "loss: 0.876447  [451200/508153]\n",
      "loss: 0.500381  [454400/508153]\n",
      "loss: 0.562813  [457600/508153]\n",
      "loss: 0.581941  [460800/508153]\n",
      "loss: 0.465204  [464000/508153]\n",
      "loss: 0.634189  [467200/508153]\n",
      "loss: 0.485118  [470400/508153]\n",
      "loss: 0.538969  [473600/508153]\n",
      "loss: 0.614486  [476800/508153]\n",
      "loss: 0.542913  [480000/508153]\n",
      "loss: 0.504820  [483200/508153]\n",
      "loss: 0.500827  [486400/508153]\n",
      "loss: 0.460058  [489600/508153]\n",
      "loss: 0.490724  [492800/508153]\n",
      "loss: 0.496721  [496000/508153]\n",
      "loss: 0.585869  [499200/508153]\n",
      "loss: 0.472267  [502400/508153]\n",
      "loss: 0.815402  [505600/508153]\n",
      "Avg loss: 0.018237 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 0.492100  [    0/508153]\n",
      "loss: 0.567555  [ 3200/508153]\n",
      "loss: 0.604108  [ 6400/508153]\n",
      "loss: 0.605383  [ 9600/508153]\n",
      "loss: 0.640547  [12800/508153]\n",
      "loss: 0.622502  [16000/508153]\n",
      "loss: 0.508745  [19200/508153]\n",
      "loss: 0.427843  [22400/508153]\n",
      "loss: 0.503628  [25600/508153]\n",
      "loss: 0.507980  [28800/508153]\n",
      "loss: 0.466763  [32000/508153]\n",
      "loss: 0.550329  [35200/508153]\n",
      "loss: 0.469360  [38400/508153]\n",
      "loss: 0.556962  [41600/508153]\n",
      "loss: 0.559937  [44800/508153]\n",
      "loss: 0.574911  [48000/508153]\n",
      "loss: 0.518177  [51200/508153]\n",
      "loss: 0.467671  [54400/508153]\n",
      "loss: 0.541456  [57600/508153]\n",
      "loss: 0.567118  [60800/508153]\n",
      "loss: 0.489850  [64000/508153]\n",
      "loss: 0.644721  [67200/508153]\n",
      "loss: 0.530081  [70400/508153]\n",
      "loss: 0.448652  [73600/508153]\n",
      "loss: 0.775771  [76800/508153]\n",
      "loss: 0.517381  [80000/508153]\n",
      "loss: 0.766016  [83200/508153]\n",
      "loss: 0.417563  [86400/508153]\n",
      "loss: 0.681718  [89600/508153]\n",
      "loss: 0.544227  [92800/508153]\n",
      "loss: 0.542612  [96000/508153]\n",
      "loss: 0.552302  [99200/508153]\n",
      "loss: 0.401819  [102400/508153]\n",
      "loss: 0.457260  [105600/508153]\n",
      "loss: 0.511410  [108800/508153]\n",
      "loss: 0.585518  [112000/508153]\n",
      "loss: 0.512953  [115200/508153]\n",
      "loss: 0.567414  [118400/508153]\n",
      "loss: 0.473370  [121600/508153]\n",
      "loss: 0.521374  [124800/508153]\n",
      "loss: 0.586798  [128000/508153]\n",
      "loss: 0.493753  [131200/508153]\n",
      "loss: 0.691035  [134400/508153]\n",
      "loss: 0.495093  [137600/508153]\n",
      "loss: 0.580322  [140800/508153]\n",
      "loss: 0.580258  [144000/508153]\n",
      "loss: 0.787667  [147200/508153]\n",
      "loss: 0.572547  [150400/508153]\n",
      "loss: 0.527200  [153600/508153]\n",
      "loss: 0.726358  [156800/508153]\n",
      "loss: 0.603502  [160000/508153]\n",
      "loss: 0.466945  [163200/508153]\n",
      "loss: 0.842863  [166400/508153]\n",
      "loss: 0.800601  [169600/508153]\n",
      "loss: 0.614423  [172800/508153]\n",
      "loss: 0.825008  [176000/508153]\n",
      "loss: 0.548359  [179200/508153]\n",
      "loss: 0.516212  [182400/508153]\n",
      "loss: 0.568345  [185600/508153]\n",
      "loss: 0.529358  [188800/508153]\n",
      "loss: 0.680165  [192000/508153]\n",
      "loss: 0.513449  [195200/508153]\n",
      "loss: 0.497624  [198400/508153]\n",
      "loss: 0.504077  [201600/508153]\n",
      "loss: 0.673416  [204800/508153]\n",
      "loss: 0.522833  [208000/508153]\n",
      "loss: 0.526289  [211200/508153]\n",
      "loss: 0.459100  [214400/508153]\n",
      "loss: 0.487307  [217600/508153]\n",
      "loss: 0.487512  [220800/508153]\n",
      "loss: 0.530241  [224000/508153]\n",
      "loss: 0.702797  [227200/508153]\n",
      "loss: 0.489382  [230400/508153]\n",
      "loss: 0.496407  [233600/508153]\n",
      "loss: 0.569652  [236800/508153]\n",
      "loss: 0.507715  [240000/508153]\n",
      "loss: 0.536831  [243200/508153]\n",
      "loss: 0.522505  [246400/508153]\n",
      "loss: 0.747068  [249600/508153]\n",
      "loss: 0.544215  [252800/508153]\n",
      "loss: 0.484530  [256000/508153]\n",
      "loss: 0.466491  [259200/508153]\n",
      "loss: 0.497451  [262400/508153]\n",
      "loss: 0.505441  [265600/508153]\n",
      "loss: 0.518656  [268800/508153]\n",
      "loss: 0.531927  [272000/508153]\n",
      "loss: 0.567036  [275200/508153]\n",
      "loss: 0.529300  [278400/508153]\n",
      "loss: 0.505386  [281600/508153]\n",
      "loss: 0.569286  [284800/508153]\n",
      "loss: 0.502800  [288000/508153]\n",
      "loss: 0.634591  [291200/508153]\n",
      "loss: 0.547918  [294400/508153]\n",
      "loss: 0.542911  [297600/508153]\n",
      "loss: 0.498188  [300800/508153]\n",
      "loss: 0.508929  [304000/508153]\n",
      "loss: 0.555375  [307200/508153]\n",
      "loss: 0.499035  [310400/508153]\n",
      "loss: 0.506711  [313600/508153]\n",
      "loss: 0.595147  [316800/508153]\n",
      "loss: 0.530477  [320000/508153]\n",
      "loss: 0.555221  [323200/508153]\n",
      "loss: 0.482569  [326400/508153]\n",
      "loss: 0.492884  [329600/508153]\n",
      "loss: 0.495090  [332800/508153]\n",
      "loss: 0.481118  [336000/508153]\n",
      "loss: 0.574453  [339200/508153]\n",
      "loss: 0.508114  [342400/508153]\n",
      "loss: 0.472143  [345600/508153]\n",
      "loss: 0.675106  [348800/508153]\n",
      "loss: 0.544763  [352000/508153]\n",
      "loss: 0.538001  [355200/508153]\n",
      "loss: 0.514875  [358400/508153]\n",
      "loss: 0.519227  [361600/508153]\n",
      "loss: 0.475896  [364800/508153]\n",
      "loss: 0.584956  [368000/508153]\n",
      "loss: 0.539451  [371200/508153]\n",
      "loss: 0.515102  [374400/508153]\n",
      "loss: 0.482274  [377600/508153]\n",
      "loss: 0.523680  [380800/508153]\n",
      "loss: 0.519522  [384000/508153]\n",
      "loss: 0.484673  [387200/508153]\n",
      "loss: 0.516568  [390400/508153]\n",
      "loss: 0.479110  [393600/508153]\n",
      "loss: 0.451757  [396800/508153]\n",
      "loss: 0.580129  [400000/508153]\n",
      "loss: 0.492783  [403200/508153]\n",
      "loss: 0.537173  [406400/508153]\n",
      "loss: 0.521870  [409600/508153]\n",
      "loss: 0.481585  [412800/508153]\n",
      "loss: 0.628693  [416000/508153]\n",
      "loss: 0.672886  [419200/508153]\n",
      "loss: 0.574428  [422400/508153]\n",
      "loss: 0.515570  [425600/508153]\n",
      "loss: 0.672889  [428800/508153]\n",
      "loss: 0.549465  [432000/508153]\n",
      "loss: 0.534137  [435200/508153]\n",
      "loss: 0.472898  [438400/508153]\n",
      "loss: 0.619031  [441600/508153]\n",
      "loss: 0.570282  [444800/508153]\n",
      "loss: 0.516561  [448000/508153]\n",
      "loss: 0.517350  [451200/508153]\n",
      "loss: 0.518688  [454400/508153]\n",
      "loss: 0.619613  [457600/508153]\n",
      "loss: 0.512354  [460800/508153]\n",
      "loss: 0.513806  [464000/508153]\n",
      "loss: 0.532119  [467200/508153]\n",
      "loss: 0.495594  [470400/508153]\n",
      "loss: 0.472940  [473600/508153]\n",
      "loss: 0.638152  [476800/508153]\n",
      "loss: 0.532958  [480000/508153]\n",
      "loss: 0.545419  [483200/508153]\n",
      "loss: 0.538736  [486400/508153]\n",
      "loss: 0.505758  [489600/508153]\n",
      "loss: 0.529287  [492800/508153]\n",
      "loss: 0.531979  [496000/508153]\n",
      "loss: 0.538561  [499200/508153]\n",
      "loss: 0.449285  [502400/508153]\n",
      "loss: 0.503420  [505600/508153]\n",
      "Avg loss: 0.016692 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 0.510926  [    0/508153]\n",
      "loss: 0.522839  [ 3200/508153]\n",
      "loss: 0.521422  [ 6400/508153]\n",
      "loss: 0.636534  [ 9600/508153]\n",
      "loss: 0.501285  [12800/508153]\n",
      "loss: 0.490880  [16000/508153]\n",
      "loss: 0.493883  [19200/508153]\n",
      "loss: 0.464725  [22400/508153]\n",
      "loss: 0.530986  [25600/508153]\n",
      "loss: 0.581280  [28800/508153]\n",
      "loss: 0.596316  [32000/508153]\n",
      "loss: 0.525148  [35200/508153]\n",
      "loss: 0.582208  [38400/508153]\n",
      "loss: 0.519524  [41600/508153]\n",
      "loss: 0.619232  [44800/508153]\n",
      "loss: 0.506835  [48000/508153]\n",
      "loss: 0.527419  [51200/508153]\n",
      "loss: 0.540188  [54400/508153]\n",
      "loss: 0.513300  [57600/508153]\n",
      "loss: 0.482111  [60800/508153]\n",
      "loss: 0.467098  [64000/508153]\n",
      "loss: 0.457484  [67200/508153]\n",
      "loss: 0.703657  [70400/508153]\n",
      "loss: 0.506304  [73600/508153]\n",
      "loss: 0.514580  [76800/508153]\n",
      "loss: 0.462817  [80000/508153]\n",
      "loss: 0.488136  [83200/508153]\n",
      "loss: 0.524621  [86400/508153]\n",
      "loss: 0.585490  [89600/508153]\n",
      "loss: 0.499278  [92800/508153]\n",
      "loss: 0.527389  [96000/508153]\n",
      "loss: 0.524390  [99200/508153]\n",
      "loss: 0.496411  [102400/508153]\n",
      "loss: 0.507687  [105600/508153]\n",
      "loss: 0.551005  [108800/508153]\n",
      "loss: 0.493167  [112000/508153]\n",
      "loss: 0.634189  [115200/508153]\n",
      "loss: 0.508620  [118400/508153]\n",
      "loss: 0.505813  [121600/508153]\n",
      "loss: 0.485737  [124800/508153]\n",
      "loss: 0.481113  [128000/508153]\n",
      "loss: 0.623314  [131200/508153]\n",
      "loss: 0.567376  [134400/508153]\n",
      "loss: 0.466665  [137600/508153]\n",
      "loss: 0.508107  [140800/508153]\n",
      "loss: 0.518511  [144000/508153]\n",
      "loss: 0.508658  [147200/508153]\n",
      "loss: 0.541468  [150400/508153]\n",
      "loss: 0.535048  [153600/508153]\n",
      "loss: 0.552707  [156800/508153]\n",
      "loss: 0.507740  [160000/508153]\n",
      "loss: 0.530299  [163200/508153]\n",
      "loss: 0.500534  [166400/508153]\n",
      "loss: 0.534094  [169600/508153]\n",
      "loss: 0.514697  [172800/508153]\n",
      "loss: 0.516018  [176000/508153]\n",
      "loss: 0.527736  [179200/508153]\n",
      "loss: 0.511236  [182400/508153]\n",
      "loss: 0.504498  [185600/508153]\n",
      "loss: 0.463013  [188800/508153]\n",
      "loss: 0.505314  [192000/508153]\n",
      "loss: 0.482026  [195200/508153]\n",
      "loss: 0.498097  [198400/508153]\n",
      "loss: 0.509000  [201600/508153]\n",
      "loss: 0.545505  [204800/508153]\n",
      "loss: 0.481681  [208000/508153]\n",
      "loss: 0.564364  [211200/508153]\n",
      "loss: 0.539789  [214400/508153]\n",
      "loss: 0.484869  [217600/508153]\n",
      "loss: 0.505993  [220800/508153]\n",
      "loss: 0.497672  [224000/508153]\n",
      "loss: 0.559185  [227200/508153]\n",
      "loss: 0.514731  [230400/508153]\n",
      "loss: 0.512031  [233600/508153]\n",
      "loss: 0.476349  [236800/508153]\n",
      "loss: 0.682576  [240000/508153]\n",
      "loss: 0.515682  [243200/508153]\n",
      "loss: 0.512406  [246400/508153]\n",
      "loss: 0.536760  [249600/508153]\n",
      "loss: 0.600960  [252800/508153]\n",
      "loss: 0.506935  [256000/508153]\n",
      "loss: 0.544839  [259200/508153]\n",
      "loss: 0.520245  [262400/508153]\n",
      "loss: 0.473315  [265600/508153]\n",
      "loss: 0.484668  [268800/508153]\n",
      "loss: 0.527359  [272000/508153]\n",
      "loss: 0.641769  [275200/508153]\n",
      "loss: 0.492089  [278400/508153]\n",
      "loss: 0.705829  [281600/508153]\n",
      "loss: 0.508432  [284800/508153]\n",
      "loss: 0.543846  [288000/508153]\n",
      "loss: 0.515764  [291200/508153]\n",
      "loss: 0.506270  [294400/508153]\n",
      "loss: 0.518133  [297600/508153]\n",
      "loss: 0.458667  [300800/508153]\n",
      "loss: 0.463994  [304000/508153]\n",
      "loss: 0.525301  [307200/508153]\n",
      "loss: 0.533046  [310400/508153]\n",
      "loss: 0.580079  [313600/508153]\n",
      "loss: 0.476773  [316800/508153]\n",
      "loss: 0.500971  [320000/508153]\n",
      "loss: 0.523176  [323200/508153]\n",
      "loss: 0.503216  [326400/508153]\n",
      "loss: 0.491012  [329600/508153]\n",
      "loss: 0.521610  [332800/508153]\n",
      "loss: 0.483841  [336000/508153]\n",
      "loss: 0.495339  [339200/508153]\n",
      "loss: 0.488657  [342400/508153]\n",
      "loss: 0.474942  [345600/508153]\n",
      "loss: 0.484607  [348800/508153]\n",
      "loss: 0.551286  [352000/508153]\n",
      "loss: 0.514131  [355200/508153]\n",
      "loss: 0.730966  [358400/508153]\n",
      "loss: 0.531322  [361600/508153]\n",
      "loss: 0.525616  [364800/508153]\n",
      "loss: 0.512148  [368000/508153]\n",
      "loss: 0.510435  [371200/508153]\n",
      "loss: 0.499497  [374400/508153]\n",
      "loss: 0.479789  [377600/508153]\n",
      "loss: 0.508336  [380800/508153]\n",
      "loss: 0.520906  [384000/508153]\n",
      "loss: 0.539944  [387200/508153]\n",
      "loss: 0.531982  [390400/508153]\n",
      "loss: 0.576593  [393600/508153]\n",
      "loss: 0.476587  [396800/508153]\n",
      "loss: 0.526245  [400000/508153]\n",
      "loss: 0.524246  [403200/508153]\n",
      "loss: 0.510613  [406400/508153]\n",
      "loss: 0.492712  [409600/508153]\n",
      "loss: 0.529738  [412800/508153]\n",
      "loss: 0.510449  [416000/508153]\n",
      "loss: 0.458749  [419200/508153]\n",
      "loss: 0.535255  [422400/508153]\n",
      "loss: 0.509827  [425600/508153]\n",
      "loss: 0.489807  [428800/508153]\n",
      "loss: 0.498994  [432000/508153]\n",
      "loss: 0.502319  [435200/508153]\n",
      "loss: 0.482340  [438400/508153]\n",
      "loss: 0.454705  [441600/508153]\n",
      "loss: 0.478705  [444800/508153]\n",
      "loss: 0.522705  [448000/508153]\n",
      "loss: 0.488859  [451200/508153]\n",
      "loss: 0.511815  [454400/508153]\n",
      "loss: 0.477680  [457600/508153]\n",
      "loss: 0.515511  [460800/508153]\n",
      "loss: 0.502977  [464000/508153]\n",
      "loss: 0.513304  [467200/508153]\n",
      "loss: 0.583040  [470400/508153]\n",
      "loss: 0.553749  [473600/508153]\n",
      "loss: 0.526032  [476800/508153]\n",
      "loss: 0.515864  [480000/508153]\n",
      "loss: 0.507212  [483200/508153]\n",
      "loss: 0.531780  [486400/508153]\n",
      "loss: 0.508438  [489600/508153]\n",
      "loss: 0.542319  [492800/508153]\n",
      "loss: 0.589384  [496000/508153]\n",
      "loss: 0.500893  [499200/508153]\n",
      "loss: 0.498797  [502400/508153]\n",
      "loss: 0.663560  [505600/508153]\n",
      "Avg loss: 0.016552 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 0.477903  [    0/508153]\n",
      "loss: 0.531756  [ 3200/508153]\n",
      "loss: 0.483932  [ 6400/508153]\n",
      "loss: 0.475523  [ 9600/508153]\n",
      "loss: 0.508516  [12800/508153]\n",
      "loss: 0.482718  [16000/508153]\n",
      "loss: 0.502411  [19200/508153]\n",
      "loss: 0.485183  [22400/508153]\n",
      "loss: 0.533118  [25600/508153]\n",
      "loss: 0.502411  [28800/508153]\n",
      "loss: 0.519453  [32000/508153]\n",
      "loss: 0.550312  [35200/508153]\n",
      "loss: 0.496313  [38400/508153]\n",
      "loss: 0.499005  [41600/508153]\n",
      "loss: 0.463351  [44800/508153]\n",
      "loss: 0.528186  [48000/508153]\n",
      "loss: 0.483251  [51200/508153]\n",
      "loss: 0.535079  [54400/508153]\n",
      "loss: 0.539958  [57600/508153]\n",
      "loss: 0.504267  [60800/508153]\n",
      "loss: 0.529967  [64000/508153]\n",
      "loss: 0.498868  [67200/508153]\n",
      "loss: 0.502569  [70400/508153]\n",
      "loss: 0.499108  [73600/508153]\n",
      "loss: 0.527913  [76800/508153]\n",
      "loss: 0.512087  [80000/508153]\n",
      "loss: 0.523935  [83200/508153]\n",
      "loss: 0.515838  [86400/508153]\n",
      "loss: 0.475581  [89600/508153]\n",
      "loss: 0.503589  [92800/508153]\n",
      "loss: 0.483372  [96000/508153]\n",
      "loss: 0.478417  [99200/508153]\n",
      "loss: 0.477377  [102400/508153]\n",
      "loss: 0.506816  [105600/508153]\n",
      "loss: 0.587284  [108800/508153]\n",
      "loss: 0.543817  [112000/508153]\n",
      "loss: 0.512950  [115200/508153]\n",
      "loss: 0.630805  [118400/508153]\n",
      "loss: 0.536093  [121600/508153]\n",
      "loss: 0.506534  [124800/508153]\n",
      "loss: 0.515213  [128000/508153]\n",
      "loss: 0.501922  [131200/508153]\n",
      "loss: 0.514888  [134400/508153]\n",
      "loss: 0.491970  [137600/508153]\n",
      "loss: 0.510603  [140800/508153]\n",
      "loss: 0.499855  [144000/508153]\n",
      "loss: 0.474746  [147200/508153]\n",
      "loss: 0.516358  [150400/508153]\n",
      "loss: 0.521228  [153600/508153]\n",
      "loss: 0.510519  [156800/508153]\n",
      "loss: 0.508646  [160000/508153]\n",
      "loss: 0.494744  [163200/508153]\n",
      "loss: 0.513493  [166400/508153]\n",
      "loss: 0.535144  [169600/508153]\n",
      "loss: 0.513122  [172800/508153]\n",
      "loss: 0.512656  [176000/508153]\n",
      "loss: 0.554549  [179200/508153]\n",
      "loss: 0.508306  [182400/508153]\n",
      "loss: 0.502885  [185600/508153]\n",
      "loss: 0.507259  [188800/508153]\n",
      "loss: 0.496091  [192000/508153]\n",
      "loss: 0.507452  [195200/508153]\n",
      "loss: 0.534604  [198400/508153]\n",
      "loss: 0.511134  [201600/508153]\n",
      "loss: 0.515452  [204800/508153]\n",
      "loss: 0.505554  [208000/508153]\n",
      "loss: 0.511558  [211200/508153]\n",
      "loss: 0.528575  [214400/508153]\n",
      "loss: 0.495751  [217600/508153]\n",
      "loss: 0.485922  [220800/508153]\n",
      "loss: 0.544433  [224000/508153]\n",
      "loss: 0.523048  [227200/508153]\n",
      "loss: 0.517349  [230400/508153]\n",
      "loss: 0.485871  [233600/508153]\n",
      "loss: 0.496476  [236800/508153]\n",
      "loss: 0.509335  [240000/508153]\n",
      "loss: 0.510216  [243200/508153]\n",
      "loss: 0.499909  [246400/508153]\n",
      "loss: 0.487084  [249600/508153]\n",
      "loss: 0.488717  [252800/508153]\n",
      "loss: 0.484725  [256000/508153]\n",
      "loss: 0.572991  [259200/508153]\n",
      "loss: 0.514626  [262400/508153]\n",
      "loss: 0.504357  [265600/508153]\n",
      "loss: 0.491698  [268800/508153]\n",
      "loss: 0.586150  [272000/508153]\n",
      "loss: 0.526673  [275200/508153]\n",
      "loss: 0.505553  [278400/508153]\n",
      "loss: 0.541796  [281600/508153]\n",
      "loss: 0.518144  [284800/508153]\n",
      "loss: 0.526805  [288000/508153]\n",
      "loss: 0.487234  [291200/508153]\n",
      "loss: 0.502643  [294400/508153]\n",
      "loss: 0.503893  [297600/508153]\n",
      "loss: 0.495708  [300800/508153]\n",
      "loss: 0.493816  [304000/508153]\n",
      "loss: 0.495610  [307200/508153]\n",
      "loss: 0.503390  [310400/508153]\n",
      "loss: 0.512081  [313600/508153]\n",
      "loss: 0.472404  [316800/508153]\n",
      "loss: 0.554339  [320000/508153]\n",
      "loss: 0.514898  [323200/508153]\n",
      "loss: 0.549286  [326400/508153]\n",
      "loss: 0.504983  [329600/508153]\n",
      "loss: 0.458029  [332800/508153]\n",
      "loss: 0.529370  [336000/508153]\n",
      "loss: 0.496149  [339200/508153]\n",
      "loss: 0.474569  [342400/508153]\n",
      "loss: 0.504555  [345600/508153]\n",
      "loss: 0.499608  [348800/508153]\n",
      "loss: 0.566442  [352000/508153]\n",
      "loss: 0.481882  [355200/508153]\n",
      "loss: 0.532167  [358400/508153]\n",
      "loss: 0.525389  [361600/508153]\n",
      "loss: 0.539516  [364800/508153]\n",
      "loss: 0.505796  [368000/508153]\n",
      "loss: 0.609704  [371200/508153]\n",
      "loss: 0.495404  [374400/508153]\n",
      "loss: 0.480208  [377600/508153]\n",
      "loss: 0.494805  [380800/508153]\n",
      "loss: 0.498650  [384000/508153]\n",
      "loss: 0.601819  [387200/508153]\n",
      "loss: 0.489063  [390400/508153]\n",
      "loss: 0.511911  [393600/508153]\n",
      "loss: 0.512094  [396800/508153]\n",
      "loss: 0.473441  [400000/508153]\n",
      "loss: 0.513744  [403200/508153]\n",
      "loss: 0.481487  [406400/508153]\n",
      "loss: 0.469651  [409600/508153]\n",
      "loss: 0.522610  [412800/508153]\n",
      "loss: 0.500006  [416000/508153]\n",
      "loss: 0.526272  [419200/508153]\n",
      "loss: 0.524290  [422400/508153]\n",
      "loss: 0.518493  [425600/508153]\n",
      "loss: 0.565416  [428800/508153]\n",
      "loss: 0.510020  [432000/508153]\n",
      "loss: 0.474681  [435200/508153]\n",
      "loss: 0.531150  [438400/508153]\n",
      "loss: 0.497881  [441600/508153]\n",
      "loss: 0.517191  [444800/508153]\n",
      "loss: 0.505775  [448000/508153]\n",
      "loss: 0.556002  [451200/508153]\n",
      "loss: 0.554663  [454400/508153]\n",
      "loss: 0.499110  [457600/508153]\n",
      "loss: 0.494540  [460800/508153]\n",
      "loss: 0.497744  [464000/508153]\n",
      "loss: 0.547091  [467200/508153]\n",
      "loss: 0.486066  [470400/508153]\n",
      "loss: 0.650312  [473600/508153]\n",
      "loss: 0.522282  [476800/508153]\n",
      "loss: 0.539591  [480000/508153]\n",
      "loss: 0.493036  [483200/508153]\n",
      "loss: 0.486648  [486400/508153]\n",
      "loss: 0.490521  [489600/508153]\n",
      "loss: 0.496503  [492800/508153]\n",
      "loss: 0.541840  [496000/508153]\n",
      "loss: 0.510697  [499200/508153]\n",
      "loss: 0.527918  [502400/508153]\n",
      "loss: 0.497037  [505600/508153]\n",
      "Avg loss: 0.016088 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 0.534527  [    0/508153]\n",
      "loss: 0.636214  [ 3200/508153]\n",
      "loss: 0.508250  [ 6400/508153]\n",
      "loss: 0.510001  [ 9600/508153]\n",
      "loss: 0.491008  [12800/508153]\n",
      "loss: 0.508310  [16000/508153]\n",
      "loss: 0.520225  [19200/508153]\n",
      "loss: 0.490693  [22400/508153]\n",
      "loss: 0.528846  [25600/508153]\n",
      "loss: 0.482496  [28800/508153]\n",
      "loss: 0.507444  [32000/508153]\n",
      "loss: 0.507225  [35200/508153]\n",
      "loss: 0.506315  [38400/508153]\n",
      "loss: 0.541920  [41600/508153]\n",
      "loss: 0.495002  [44800/508153]\n",
      "loss: 0.503877  [48000/508153]\n",
      "loss: 0.497855  [51200/508153]\n",
      "loss: 0.466565  [54400/508153]\n",
      "loss: 0.497938  [57600/508153]\n",
      "loss: 0.477238  [60800/508153]\n",
      "loss: 0.524660  [64000/508153]\n",
      "loss: 0.506770  [67200/508153]\n",
      "loss: 0.502517  [70400/508153]\n",
      "loss: 0.537933  [73600/508153]\n",
      "loss: 0.507604  [76800/508153]\n",
      "loss: 0.503626  [80000/508153]\n",
      "loss: 0.518193  [83200/508153]\n",
      "loss: 0.498282  [86400/508153]\n",
      "loss: 0.493760  [89600/508153]\n",
      "loss: 0.511371  [92800/508153]\n",
      "loss: 0.507635  [96000/508153]\n",
      "loss: 0.491557  [99200/508153]\n",
      "loss: 0.646280  [102400/508153]\n",
      "loss: 0.521225  [105600/508153]\n",
      "loss: 0.634439  [108800/508153]\n",
      "loss: 0.511896  [112000/508153]\n",
      "loss: 0.528371  [115200/508153]\n",
      "loss: 0.539782  [118400/508153]\n",
      "loss: 0.525776  [121600/508153]\n",
      "loss: 0.479703  [124800/508153]\n",
      "loss: 0.494504  [128000/508153]\n",
      "loss: 0.494069  [131200/508153]\n",
      "loss: 0.478833  [134400/508153]\n",
      "loss: 0.503357  [137600/508153]\n",
      "loss: 0.498144  [140800/508153]\n",
      "loss: 0.495651  [144000/508153]\n",
      "loss: 0.616732  [147200/508153]\n",
      "loss: 0.607148  [150400/508153]\n",
      "loss: 0.465002  [153600/508153]\n",
      "loss: 0.486590  [156800/508153]\n",
      "loss: 0.504806  [160000/508153]\n",
      "loss: 0.524258  [163200/508153]\n",
      "loss: 0.498317  [166400/508153]\n",
      "loss: 0.475768  [169600/508153]\n",
      "loss: 0.541309  [172800/508153]\n",
      "loss: 0.497032  [176000/508153]\n",
      "loss: 0.553807  [179200/508153]\n",
      "loss: 0.521012  [182400/508153]\n",
      "loss: 0.504438  [185600/508153]\n",
      "loss: 0.530649  [188800/508153]\n",
      "loss: 0.505403  [192000/508153]\n",
      "loss: 0.526578  [195200/508153]\n",
      "loss: 0.492200  [198400/508153]\n",
      "loss: 0.497254  [201600/508153]\n",
      "loss: 0.487403  [204800/508153]\n",
      "loss: 0.489846  [208000/508153]\n",
      "loss: 0.519230  [211200/508153]\n",
      "loss: 0.504829  [214400/508153]\n",
      "loss: 0.495902  [217600/508153]\n",
      "loss: 0.503974  [220800/508153]\n",
      "loss: 0.494694  [224000/508153]\n",
      "loss: 0.527326  [227200/508153]\n",
      "loss: 0.496786  [230400/508153]\n",
      "loss: 0.482736  [233600/508153]\n",
      "loss: 0.502200  [236800/508153]\n",
      "loss: 0.509522  [240000/508153]\n",
      "loss: 0.537780  [243200/508153]\n",
      "loss: 0.501435  [246400/508153]\n",
      "loss: 0.507317  [249600/508153]\n",
      "loss: 0.501846  [252800/508153]\n",
      "loss: 0.479293  [256000/508153]\n",
      "loss: 0.491784  [259200/508153]\n",
      "loss: 0.508371  [262400/508153]\n",
      "loss: 0.495236  [265600/508153]\n",
      "loss: 0.498978  [268800/508153]\n",
      "loss: 0.498849  [272000/508153]\n",
      "loss: 0.515722  [275200/508153]\n",
      "loss: 0.472292  [278400/508153]\n",
      "loss: 0.483020  [281600/508153]\n",
      "loss: 0.474335  [284800/508153]\n",
      "loss: 0.500017  [288000/508153]\n",
      "loss: 0.522327  [291200/508153]\n",
      "loss: 0.479508  [294400/508153]\n",
      "loss: 0.493047  [297600/508153]\n",
      "loss: 0.528028  [300800/508153]\n",
      "loss: 0.533840  [304000/508153]\n",
      "loss: 0.485797  [307200/508153]\n",
      "loss: 0.518924  [310400/508153]\n",
      "loss: 0.494441  [313600/508153]\n",
      "loss: 0.486089  [316800/508153]\n",
      "loss: 0.498266  [320000/508153]\n",
      "loss: 0.484421  [323200/508153]\n",
      "loss: 0.495612  [326400/508153]\n",
      "loss: 0.497922  [329600/508153]\n",
      "loss: 0.574939  [332800/508153]\n",
      "loss: 0.505085  [336000/508153]\n",
      "loss: 0.506380  [339200/508153]\n",
      "loss: 0.496824  [342400/508153]\n",
      "loss: 0.507019  [345600/508153]\n",
      "loss: 0.514033  [348800/508153]\n",
      "loss: 0.503528  [352000/508153]\n",
      "loss: 0.501468  [355200/508153]\n",
      "loss: 0.498095  [358400/508153]\n",
      "loss: 0.519690  [361600/508153]\n",
      "loss: 0.487037  [364800/508153]\n",
      "loss: 0.504735  [368000/508153]\n",
      "loss: 0.522356  [371200/508153]\n",
      "loss: 0.493735  [374400/508153]\n",
      "loss: 0.526209  [377600/508153]\n",
      "loss: 0.457847  [380800/508153]\n",
      "loss: 0.480923  [384000/508153]\n",
      "loss: 0.521302  [387200/508153]\n",
      "loss: 0.523524  [390400/508153]\n",
      "loss: 0.484582  [393600/508153]\n",
      "loss: 0.506930  [396800/508153]\n",
      "loss: 0.492703  [400000/508153]\n",
      "loss: 0.499121  [403200/508153]\n",
      "loss: 0.495536  [406400/508153]\n",
      "loss: 0.465259  [409600/508153]\n",
      "loss: 0.502014  [412800/508153]\n",
      "loss: 0.629791  [416000/508153]\n",
      "loss: 0.485700  [419200/508153]\n",
      "loss: 0.512552  [422400/508153]\n",
      "loss: 0.514561  [425600/508153]\n",
      "loss: 0.502999  [428800/508153]\n",
      "loss: 0.494439  [432000/508153]\n",
      "loss: 0.510088  [435200/508153]\n",
      "loss: 0.515165  [438400/508153]\n",
      "loss: 0.517417  [441600/508153]\n",
      "loss: 0.531078  [444800/508153]\n",
      "loss: 0.480016  [448000/508153]\n",
      "loss: 0.519198  [451200/508153]\n",
      "loss: 0.595976  [454400/508153]\n",
      "loss: 0.541732  [457600/508153]\n",
      "loss: 0.477671  [460800/508153]\n",
      "loss: 0.541884  [464000/508153]\n",
      "loss: 0.532776  [467200/508153]\n",
      "loss: 0.487852  [470400/508153]\n",
      "loss: 0.469714  [473600/508153]\n",
      "loss: 0.443184  [476800/508153]\n",
      "loss: 0.498052  [480000/508153]\n",
      "loss: 0.509162  [483200/508153]\n",
      "loss: 0.505892  [486400/508153]\n",
      "loss: 0.506471  [489600/508153]\n",
      "loss: 0.620160  [492800/508153]\n",
      "loss: 0.490621  [496000/508153]\n",
      "loss: 0.508626  [499200/508153]\n",
      "loss: 0.463600  [502400/508153]\n",
      "loss: 0.545420  [505600/508153]\n",
      "Avg loss: 0.016051 \n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "loss: 0.484666  [    0/508153]\n",
      "loss: 0.485942  [ 3200/508153]\n",
      "loss: 0.509128  [ 6400/508153]\n",
      "loss: 0.677865  [ 9600/508153]\n",
      "loss: 0.483206  [12800/508153]\n",
      "loss: 0.506637  [16000/508153]\n",
      "loss: 0.505225  [19200/508153]\n",
      "loss: 0.520134  [22400/508153]\n",
      "loss: 0.504337  [25600/508153]\n",
      "loss: 0.617687  [28800/508153]\n",
      "loss: 0.506840  [32000/508153]\n",
      "loss: 0.505730  [35200/508153]\n",
      "loss: 0.512530  [38400/508153]\n",
      "loss: 0.514360  [41600/508153]\n",
      "loss: 0.490914  [44800/508153]\n",
      "loss: 0.496114  [48000/508153]\n",
      "loss: 0.511173  [51200/508153]\n",
      "loss: 0.501508  [54400/508153]\n",
      "loss: 0.514811  [57600/508153]\n",
      "loss: 0.511924  [60800/508153]\n",
      "loss: 0.506922  [64000/508153]\n",
      "loss: 0.504463  [67200/508153]\n",
      "loss: 0.530700  [70400/508153]\n",
      "loss: 0.568881  [73600/508153]\n",
      "loss: 0.541909  [76800/508153]\n",
      "loss: 0.504088  [80000/508153]\n",
      "loss: 0.483892  [83200/508153]\n",
      "loss: 0.502108  [86400/508153]\n",
      "loss: 0.484359  [89600/508153]\n",
      "loss: 0.542861  [92800/508153]\n",
      "loss: 0.542664  [96000/508153]\n",
      "loss: 0.507661  [99200/508153]\n",
      "loss: 0.498519  [102400/508153]\n",
      "loss: 0.511506  [105600/508153]\n",
      "loss: 0.494780  [108800/508153]\n",
      "loss: 0.588496  [112000/508153]\n",
      "loss: 0.529207  [115200/508153]\n",
      "loss: 0.511170  [118400/508153]\n",
      "loss: 0.521093  [121600/508153]\n",
      "loss: 0.502752  [124800/508153]\n",
      "loss: 0.501899  [128000/508153]\n",
      "loss: 0.496676  [131200/508153]\n",
      "loss: 0.513487  [134400/508153]\n",
      "loss: 0.488746  [137600/508153]\n",
      "loss: 0.480360  [140800/508153]\n",
      "loss: 0.494233  [144000/508153]\n",
      "loss: 0.504076  [147200/508153]\n",
      "loss: 0.489850  [150400/508153]\n",
      "loss: 0.507135  [153600/508153]\n",
      "loss: 0.505910  [156800/508153]\n",
      "loss: 0.469635  [160000/508153]\n",
      "loss: 0.493886  [163200/508153]\n",
      "loss: 0.511742  [166400/508153]\n",
      "loss: 0.485547  [169600/508153]\n",
      "loss: 0.523227  [172800/508153]\n",
      "loss: 0.492123  [176000/508153]\n",
      "loss: 0.534778  [179200/508153]\n",
      "loss: 0.482314  [182400/508153]\n",
      "loss: 0.486992  [185600/508153]\n",
      "loss: 0.524974  [188800/508153]\n",
      "loss: 0.534479  [192000/508153]\n",
      "loss: 0.514560  [195200/508153]\n",
      "loss: 0.517488  [198400/508153]\n",
      "loss: 0.504809  [201600/508153]\n",
      "loss: 0.498271  [204800/508153]\n",
      "loss: 0.503277  [208000/508153]\n",
      "loss: 0.518300  [211200/508153]\n",
      "loss: 0.457108  [214400/508153]\n",
      "loss: 0.493046  [217600/508153]\n",
      "loss: 0.489686  [220800/508153]\n",
      "loss: 0.505474  [224000/508153]\n",
      "loss: 0.514572  [227200/508153]\n",
      "loss: 0.496624  [230400/508153]\n",
      "loss: 0.518614  [233600/508153]\n",
      "loss: 0.505053  [236800/508153]\n",
      "loss: 0.528015  [240000/508153]\n",
      "loss: 0.493423  [243200/508153]\n",
      "loss: 0.607526  [246400/508153]\n",
      "loss: 0.547552  [249600/508153]\n",
      "loss: 0.500935  [252800/508153]\n",
      "loss: 0.495596  [256000/508153]\n",
      "loss: 0.460922  [259200/508153]\n",
      "loss: 0.501912  [262400/508153]\n",
      "loss: 0.490704  [265600/508153]\n",
      "loss: 0.501074  [268800/508153]\n",
      "loss: 0.529116  [272000/508153]\n",
      "loss: 0.478617  [275200/508153]\n",
      "loss: 0.527150  [278400/508153]\n",
      "loss: 0.483981  [281600/508153]\n",
      "loss: 0.508352  [284800/508153]\n",
      "loss: 0.477596  [288000/508153]\n",
      "loss: 0.494193  [291200/508153]\n",
      "loss: 0.493087  [294400/508153]\n",
      "loss: 0.506049  [297600/508153]\n",
      "loss: 0.489201  [300800/508153]\n",
      "loss: 0.509860  [304000/508153]\n",
      "loss: 0.487189  [307200/508153]\n",
      "loss: 0.485818  [310400/508153]\n",
      "loss: 0.476518  [313600/508153]\n",
      "loss: 0.517872  [316800/508153]\n",
      "loss: 0.487861  [320000/508153]\n",
      "loss: 0.495224  [323200/508153]\n",
      "loss: 0.528929  [326400/508153]\n",
      "loss: 0.492088  [329600/508153]\n",
      "loss: 0.501131  [332800/508153]\n",
      "loss: 0.499267  [336000/508153]\n",
      "loss: 0.518077  [339200/508153]\n",
      "loss: 0.504760  [342400/508153]\n",
      "loss: 0.519355  [345600/508153]\n",
      "loss: 0.519364  [348800/508153]\n",
      "loss: 0.504677  [352000/508153]\n",
      "loss: 0.499355  [355200/508153]\n",
      "loss: 0.506206  [358400/508153]\n",
      "loss: 0.515757  [361600/508153]\n",
      "loss: 0.492671  [364800/508153]\n",
      "loss: 0.489219  [368000/508153]\n",
      "loss: 0.520405  [371200/508153]\n",
      "loss: 0.518169  [374400/508153]\n",
      "loss: 0.471860  [377600/508153]\n",
      "loss: 0.484362  [380800/508153]\n",
      "loss: 0.505052  [384000/508153]\n",
      "loss: 0.513115  [387200/508153]\n",
      "loss: 0.492083  [390400/508153]\n",
      "loss: 0.502197  [393600/508153]\n",
      "loss: 0.502037  [396800/508153]\n",
      "loss: 0.490985  [400000/508153]\n",
      "loss: 0.525276  [403200/508153]\n",
      "loss: 0.540619  [406400/508153]\n",
      "loss: 0.491611  [409600/508153]\n",
      "loss: 0.548154  [412800/508153]\n",
      "loss: 0.509909  [416000/508153]\n",
      "loss: 0.487908  [419200/508153]\n",
      "loss: 0.499622  [422400/508153]\n",
      "loss: 0.518714  [425600/508153]\n",
      "loss: 0.541793  [428800/508153]\n",
      "loss: 0.524821  [432000/508153]\n",
      "loss: 0.513363  [435200/508153]\n",
      "loss: 0.496784  [438400/508153]\n",
      "loss: 0.483948  [441600/508153]\n",
      "loss: 0.506172  [444800/508153]\n",
      "loss: 0.520961  [448000/508153]\n",
      "loss: 0.583060  [451200/508153]\n",
      "loss: 0.511976  [454400/508153]\n",
      "loss: 0.512708  [457600/508153]\n",
      "loss: 0.517326  [460800/508153]\n",
      "loss: 0.511232  [464000/508153]\n",
      "loss: 0.525766  [467200/508153]\n",
      "loss: 0.490336  [470400/508153]\n",
      "loss: 0.527493  [473600/508153]\n",
      "loss: 0.513196  [476800/508153]\n",
      "loss: 0.484660  [480000/508153]\n",
      "loss: 0.484168  [483200/508153]\n",
      "loss: 0.508773  [486400/508153]\n",
      "loss: 0.511158  [489600/508153]\n",
      "loss: 0.490208  [492800/508153]\n",
      "loss: 0.581355  [496000/508153]\n",
      "loss: 0.470284  [499200/508153]\n",
      "loss: 0.509709  [502400/508153]\n",
      "loss: 0.500024  [505600/508153]\n",
      "Avg loss: 0.016115 \n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "loss: 0.494353  [    0/508153]\n",
      "loss: 0.499198  [ 3200/508153]\n",
      "loss: 0.489025  [ 6400/508153]\n",
      "loss: 0.482578  [ 9600/508153]\n",
      "loss: 0.518084  [12800/508153]\n",
      "loss: 0.503201  [16000/508153]\n",
      "loss: 0.494308  [19200/508153]\n",
      "loss: 0.524850  [22400/508153]\n",
      "loss: 0.507035  [25600/508153]\n",
      "loss: 0.515591  [28800/508153]\n",
      "loss: 0.524242  [32000/508153]\n",
      "loss: 0.513150  [35200/508153]\n",
      "loss: 0.556415  [38400/508153]\n",
      "loss: 0.495445  [41600/508153]\n",
      "loss: 0.501601  [44800/508153]\n",
      "loss: 0.517377  [48000/508153]\n",
      "loss: 0.511289  [51200/508153]\n",
      "loss: 0.476344  [54400/508153]\n",
      "loss: 0.501261  [57600/508153]\n",
      "loss: 0.511827  [60800/508153]\n",
      "loss: 0.494798  [64000/508153]\n",
      "loss: 0.575840  [67200/508153]\n",
      "loss: 0.479745  [70400/508153]\n",
      "loss: 0.497582  [73600/508153]\n",
      "loss: 0.503297  [76800/508153]\n",
      "loss: 0.491431  [80000/508153]\n",
      "loss: 0.504205  [83200/508153]\n",
      "loss: 0.496900  [86400/508153]\n",
      "loss: 0.490549  [89600/508153]\n",
      "loss: 0.509571  [92800/508153]\n",
      "loss: 0.497003  [96000/508153]\n",
      "loss: 0.521089  [99200/508153]\n",
      "loss: 0.509834  [102400/508153]\n",
      "loss: 0.598853  [105600/508153]\n",
      "loss: 0.497580  [108800/508153]\n",
      "loss: 0.505180  [112000/508153]\n",
      "loss: 0.478690  [115200/508153]\n",
      "loss: 0.498194  [118400/508153]\n",
      "loss: 0.678233  [121600/508153]\n",
      "loss: 0.515056  [124800/508153]\n",
      "loss: 0.492149  [128000/508153]\n",
      "loss: 0.521934  [131200/508153]\n",
      "loss: 0.515670  [134400/508153]\n",
      "loss: 0.530112  [137600/508153]\n",
      "loss: 0.499792  [140800/508153]\n",
      "loss: 0.507622  [144000/508153]\n",
      "loss: 0.533774  [147200/508153]\n",
      "loss: 0.497159  [150400/508153]\n",
      "loss: 0.519355  [153600/508153]\n",
      "loss: 0.505856  [156800/508153]\n",
      "loss: 0.682492  [160000/508153]\n",
      "loss: 0.501990  [163200/508153]\n",
      "loss: 0.504089  [166400/508153]\n",
      "loss: 0.505543  [169600/508153]\n",
      "loss: 0.497645  [172800/508153]\n",
      "loss: 0.503106  [176000/508153]\n",
      "loss: 0.511901  [179200/508153]\n",
      "loss: 0.503616  [182400/508153]\n",
      "loss: 0.494156  [185600/508153]\n",
      "loss: 0.510444  [188800/508153]\n",
      "loss: 0.503910  [192000/508153]\n",
      "loss: 0.535852  [195200/508153]\n",
      "loss: 0.509969  [198400/508153]\n",
      "loss: 0.505002  [201600/508153]\n",
      "loss: 0.526091  [204800/508153]\n",
      "loss: 0.500741  [208000/508153]\n",
      "loss: 0.522690  [211200/508153]\n",
      "loss: 0.478188  [214400/508153]\n",
      "loss: 0.505749  [217600/508153]\n",
      "loss: 0.514957  [220800/508153]\n",
      "loss: 0.528015  [224000/508153]\n",
      "loss: 0.496615  [227200/508153]\n",
      "loss: 0.479523  [230400/508153]\n",
      "loss: 0.641907  [233600/508153]\n",
      "loss: 0.533888  [236800/508153]\n",
      "loss: 0.517678  [240000/508153]\n",
      "loss: 0.492422  [243200/508153]\n",
      "loss: 0.509297  [246400/508153]\n",
      "loss: 0.483679  [249600/508153]\n",
      "loss: 0.502671  [252800/508153]\n",
      "loss: 0.491356  [256000/508153]\n",
      "loss: 0.494888  [259200/508153]\n",
      "loss: 0.523301  [262400/508153]\n",
      "loss: 0.520673  [265600/508153]\n",
      "loss: 0.484972  [268800/508153]\n",
      "loss: 0.491327  [272000/508153]\n",
      "loss: 0.493334  [275200/508153]\n",
      "loss: 0.482097  [278400/508153]\n",
      "loss: 0.484130  [281600/508153]\n",
      "loss: 0.581236  [284800/508153]\n",
      "loss: 0.494085  [288000/508153]\n",
      "loss: 0.520152  [291200/508153]\n",
      "loss: 0.496068  [294400/508153]\n",
      "loss: 0.501564  [297600/508153]\n",
      "loss: 0.492137  [300800/508153]\n",
      "loss: 0.507139  [304000/508153]\n",
      "loss: 0.492092  [307200/508153]\n",
      "loss: 0.497249  [310400/508153]\n",
      "loss: 0.506984  [313600/508153]\n",
      "loss: 0.489286  [316800/508153]\n",
      "loss: 0.493599  [320000/508153]\n",
      "loss: 0.468079  [323200/508153]\n",
      "loss: 0.505282  [326400/508153]\n",
      "loss: 0.520744  [329600/508153]\n",
      "loss: 0.496707  [332800/508153]\n",
      "loss: 0.488819  [336000/508153]\n",
      "loss: 0.508533  [339200/508153]\n",
      "loss: 0.492342  [342400/508153]\n",
      "loss: 0.485560  [345600/508153]\n",
      "loss: 0.549619  [348800/508153]\n",
      "loss: 0.499749  [352000/508153]\n",
      "loss: 0.489865  [355200/508153]\n",
      "loss: 0.617358  [358400/508153]\n",
      "loss: 0.496729  [361600/508153]\n",
      "loss: 0.478377  [364800/508153]\n",
      "loss: 0.517609  [368000/508153]\n",
      "loss: 0.503499  [371200/508153]\n",
      "loss: 0.491693  [374400/508153]\n",
      "loss: 0.491768  [377600/508153]\n",
      "loss: 0.488697  [380800/508153]\n",
      "loss: 0.498514  [384000/508153]\n",
      "loss: 0.494181  [387200/508153]\n",
      "loss: 0.476389  [390400/508153]\n",
      "loss: 0.495564  [393600/508153]\n",
      "loss: 0.452009  [396800/508153]\n",
      "loss: 0.519607  [400000/508153]\n",
      "loss: 0.508236  [403200/508153]\n",
      "loss: 0.500650  [406400/508153]\n",
      "loss: 0.499318  [409600/508153]\n",
      "loss: 0.510284  [412800/508153]\n",
      "loss: 0.506683  [416000/508153]\n",
      "loss: 0.513454  [419200/508153]\n",
      "loss: 0.494093  [422400/508153]\n",
      "loss: 0.502366  [425600/508153]\n",
      "loss: 0.522345  [428800/508153]\n",
      "loss: 0.541584  [432000/508153]\n",
      "loss: 0.494949  [435200/508153]\n",
      "loss: 0.589973  [438400/508153]\n",
      "loss: 0.525778  [441600/508153]\n",
      "loss: 0.494685  [444800/508153]\n",
      "loss: 0.509405  [448000/508153]\n",
      "loss: 0.482759  [451200/508153]\n",
      "loss: 0.490609  [454400/508153]\n",
      "loss: 0.487304  [457600/508153]\n",
      "loss: 0.501586  [460800/508153]\n",
      "loss: 0.496402  [464000/508153]\n",
      "loss: 0.486685  [467200/508153]\n",
      "loss: 0.505225  [470400/508153]\n",
      "loss: 0.504267  [473600/508153]\n",
      "loss: 0.490590  [476800/508153]\n",
      "loss: 0.502630  [480000/508153]\n",
      "loss: 0.480866  [483200/508153]\n",
      "loss: 0.511953  [486400/508153]\n",
      "loss: 0.506024  [489600/508153]\n",
      "loss: 0.471373  [492800/508153]\n",
      "loss: 0.553041  [496000/508153]\n",
      "loss: 0.549596  [499200/508153]\n",
      "loss: 0.478933  [502400/508153]\n",
      "loss: 0.478113  [505600/508153]\n",
      "Avg loss: 0.015937 \n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "loss: 0.517780  [    0/508153]\n",
      "loss: 0.515137  [ 3200/508153]\n",
      "loss: 0.476755  [ 6400/508153]\n",
      "loss: 0.559298  [ 9600/508153]\n",
      "loss: 0.494282  [12800/508153]\n",
      "loss: 0.502927  [16000/508153]\n",
      "loss: 0.519507  [19200/508153]\n",
      "loss: 0.503138  [22400/508153]\n",
      "loss: 0.495268  [25600/508153]\n",
      "loss: 0.505910  [28800/508153]\n",
      "loss: 0.495544  [32000/508153]\n",
      "loss: 0.481103  [35200/508153]\n",
      "loss: 0.479328  [38400/508153]\n",
      "loss: 0.537436  [41600/508153]\n",
      "loss: 0.496060  [44800/508153]\n",
      "loss: 0.523516  [48000/508153]\n",
      "loss: 0.492462  [51200/508153]\n",
      "loss: 0.516556  [54400/508153]\n",
      "loss: 0.518122  [57600/508153]\n",
      "loss: 0.503876  [60800/508153]\n",
      "loss: 0.503821  [64000/508153]\n",
      "loss: 0.532741  [67200/508153]\n",
      "loss: 0.496347  [70400/508153]\n",
      "loss: 0.489513  [73600/508153]\n",
      "loss: 0.495563  [76800/508153]\n",
      "loss: 0.470883  [80000/508153]\n",
      "loss: 0.482603  [83200/508153]\n",
      "loss: 0.506421  [86400/508153]\n",
      "loss: 0.514001  [89600/508153]\n",
      "loss: 0.497645  [92800/508153]\n",
      "loss: 0.511013  [96000/508153]\n",
      "loss: 0.517056  [99200/508153]\n",
      "loss: 0.526685  [102400/508153]\n",
      "loss: 0.483035  [105600/508153]\n",
      "loss: 0.475958  [108800/508153]\n",
      "loss: 0.545776  [112000/508153]\n",
      "loss: 0.520689  [115200/508153]\n",
      "loss: 0.509414  [118400/508153]\n",
      "loss: 0.517160  [121600/508153]\n",
      "loss: 0.516250  [124800/508153]\n",
      "loss: 0.500408  [128000/508153]\n",
      "loss: 0.486330  [131200/508153]\n",
      "loss: 0.499647  [134400/508153]\n",
      "loss: 0.487569  [137600/508153]\n",
      "loss: 0.539114  [140800/508153]\n",
      "loss: 0.488186  [144000/508153]\n",
      "loss: 0.510823  [147200/508153]\n",
      "loss: 0.508038  [150400/508153]\n",
      "loss: 0.504478  [153600/508153]\n",
      "loss: 0.510211  [156800/508153]\n",
      "loss: 0.510182  [160000/508153]\n",
      "loss: 0.482852  [163200/508153]\n",
      "loss: 0.507741  [166400/508153]\n",
      "loss: 0.514443  [169600/508153]\n",
      "loss: 0.491193  [172800/508153]\n",
      "loss: 0.464336  [176000/508153]\n",
      "loss: 0.492472  [179200/508153]\n",
      "loss: 0.591378  [182400/508153]\n",
      "loss: 0.469508  [185600/508153]\n",
      "loss: 0.506536  [188800/508153]\n",
      "loss: 0.552082  [192000/508153]\n",
      "loss: 0.510930  [195200/508153]\n",
      "loss: 0.496441  [198400/508153]\n",
      "loss: 0.564117  [201600/508153]\n",
      "loss: 0.509560  [204800/508153]\n",
      "loss: 0.628528  [208000/508153]\n",
      "loss: 0.500676  [211200/508153]\n",
      "loss: 0.488909  [214400/508153]\n",
      "loss: 0.491336  [217600/508153]\n",
      "loss: 0.526133  [220800/508153]\n",
      "loss: 0.486491  [224000/508153]\n",
      "loss: 0.507882  [227200/508153]\n",
      "loss: 0.508035  [230400/508153]\n",
      "loss: 0.501894  [233600/508153]\n",
      "loss: 0.498990  [236800/508153]\n",
      "loss: 0.504843  [240000/508153]\n",
      "loss: 0.523656  [243200/508153]\n",
      "loss: 0.499424  [246400/508153]\n",
      "loss: 0.527959  [249600/508153]\n",
      "loss: 0.500894  [252800/508153]\n",
      "loss: 0.504977  [256000/508153]\n",
      "loss: 0.462979  [259200/508153]\n",
      "loss: 0.502330  [262400/508153]\n",
      "loss: 0.493502  [265600/508153]\n",
      "loss: 0.521921  [268800/508153]\n",
      "loss: 0.491300  [272000/508153]\n",
      "loss: 0.537808  [275200/508153]\n",
      "loss: 0.502930  [278400/508153]\n",
      "loss: 0.534842  [281600/508153]\n",
      "loss: 0.464152  [284800/508153]\n",
      "loss: 0.518631  [288000/508153]\n",
      "loss: 0.489118  [291200/508153]\n",
      "loss: 0.497918  [294400/508153]\n",
      "loss: 0.514325  [297600/508153]\n",
      "loss: 0.480023  [300800/508153]\n",
      "loss: 0.509463  [304000/508153]\n",
      "loss: 0.486213  [307200/508153]\n",
      "loss: 0.503815  [310400/508153]\n",
      "loss: 0.467769  [313600/508153]\n",
      "loss: 0.516889  [316800/508153]\n",
      "loss: 0.509743  [320000/508153]\n",
      "loss: 0.538806  [323200/508153]\n",
      "loss: 0.488372  [326400/508153]\n",
      "loss: 0.518175  [329600/508153]\n",
      "loss: 0.507529  [332800/508153]\n",
      "loss: 0.494484  [336000/508153]\n",
      "loss: 0.496904  [339200/508153]\n",
      "loss: 0.501580  [342400/508153]\n",
      "loss: 0.498209  [345600/508153]\n",
      "loss: 0.504009  [348800/508153]\n",
      "loss: 0.511130  [352000/508153]\n",
      "loss: 0.498842  [355200/508153]\n",
      "loss: 0.510863  [358400/508153]\n",
      "loss: 0.502608  [361600/508153]\n",
      "loss: 0.484349  [364800/508153]\n",
      "loss: 0.503092  [368000/508153]\n",
      "loss: 0.501098  [371200/508153]\n",
      "loss: 0.500194  [374400/508153]\n",
      "loss: 0.508171  [377600/508153]\n",
      "loss: 0.501728  [380800/508153]\n",
      "loss: 0.477605  [384000/508153]\n",
      "loss: 0.508883  [387200/508153]\n",
      "loss: 0.504507  [390400/508153]\n",
      "loss: 0.601106  [393600/508153]\n",
      "loss: 0.516658  [396800/508153]\n",
      "loss: 0.511497  [400000/508153]\n",
      "loss: 0.523794  [403200/508153]\n",
      "loss: 0.483103  [406400/508153]\n",
      "loss: 0.518957  [409600/508153]\n",
      "loss: 0.538980  [412800/508153]\n",
      "loss: 0.523537  [416000/508153]\n",
      "loss: 0.500716  [419200/508153]\n",
      "loss: 0.476420  [422400/508153]\n",
      "loss: 0.511622  [425600/508153]\n",
      "loss: 0.483571  [428800/508153]\n",
      "loss: 0.493681  [432000/508153]\n",
      "loss: 0.506730  [435200/508153]\n",
      "loss: 0.484769  [438400/508153]\n",
      "loss: 0.496250  [441600/508153]\n",
      "loss: 0.475987  [444800/508153]\n",
      "loss: 0.475443  [448000/508153]\n",
      "loss: 0.512902  [451200/508153]\n",
      "loss: 0.491131  [454400/508153]\n",
      "loss: 0.509788  [457600/508153]\n",
      "loss: 0.522685  [460800/508153]\n",
      "loss: 0.512414  [464000/508153]\n",
      "loss: 0.507377  [467200/508153]\n",
      "loss: 0.515665  [470400/508153]\n",
      "loss: 0.498261  [473600/508153]\n",
      "loss: 0.486311  [476800/508153]\n",
      "loss: 0.516857  [480000/508153]\n",
      "loss: 0.513351  [483200/508153]\n",
      "loss: 0.527694  [486400/508153]\n",
      "loss: 0.495184  [489600/508153]\n",
      "loss: 0.494486  [492800/508153]\n",
      "loss: 0.497704  [496000/508153]\n",
      "loss: 0.583701  [499200/508153]\n",
      "loss: 0.495184  [502400/508153]\n",
      "loss: 0.512571  [505600/508153]\n",
      "Avg loss: 0.015949 \n",
      "\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "loss: 0.479227  [    0/508153]\n",
      "loss: 0.505114  [ 3200/508153]\n",
      "loss: 0.491794  [ 6400/508153]\n",
      "loss: 0.505313  [ 9600/508153]\n",
      "loss: 0.509585  [12800/508153]\n",
      "loss: 0.488664  [16000/508153]\n",
      "loss: 0.480542  [19200/508153]\n",
      "loss: 0.514185  [22400/508153]\n",
      "loss: 0.503627  [25600/508153]\n",
      "loss: 0.510081  [28800/508153]\n",
      "loss: 0.496093  [32000/508153]\n",
      "loss: 0.491102  [35200/508153]\n",
      "loss: 0.522162  [38400/508153]\n",
      "loss: 0.496814  [41600/508153]\n",
      "loss: 0.510958  [44800/508153]\n",
      "loss: 0.521753  [48000/508153]\n",
      "loss: 0.511691  [51200/508153]\n",
      "loss: 0.535912  [54400/508153]\n",
      "loss: 0.502765  [57600/508153]\n",
      "loss: 0.496231  [60800/508153]\n",
      "loss: 0.491603  [64000/508153]\n",
      "loss: 0.510131  [67200/508153]\n",
      "loss: 0.489153  [70400/508153]\n",
      "loss: 0.495483  [73600/508153]\n",
      "loss: 0.502520  [76800/508153]\n",
      "loss: 0.569188  [80000/508153]\n",
      "loss: 0.499753  [83200/508153]\n",
      "loss: 0.515615  [86400/508153]\n",
      "loss: 0.503836  [89600/508153]\n",
      "loss: 0.528751  [92800/508153]\n",
      "loss: 0.505076  [96000/508153]\n",
      "loss: 0.499779  [99200/508153]\n",
      "loss: 0.515245  [102400/508153]\n",
      "loss: 0.513804  [105600/508153]\n",
      "loss: 0.478838  [108800/508153]\n",
      "loss: 0.493426  [112000/508153]\n",
      "loss: 0.502658  [115200/508153]\n",
      "loss: 0.500556  [118400/508153]\n",
      "loss: 0.510943  [121600/508153]\n",
      "loss: 0.498133  [124800/508153]\n",
      "loss: 0.518642  [128000/508153]\n",
      "loss: 0.501777  [131200/508153]\n",
      "loss: 0.517356  [134400/508153]\n",
      "loss: 0.483806  [137600/508153]\n",
      "loss: 0.477056  [140800/508153]\n",
      "loss: 0.488884  [144000/508153]\n",
      "loss: 0.527019  [147200/508153]\n",
      "loss: 0.501662  [150400/508153]\n",
      "loss: 0.492219  [153600/508153]\n",
      "loss: 0.519070  [156800/508153]\n",
      "loss: 0.486026  [160000/508153]\n",
      "loss: 0.499888  [163200/508153]\n",
      "loss: 0.494238  [166400/508153]\n",
      "loss: 0.489880  [169600/508153]\n",
      "loss: 0.486064  [172800/508153]\n",
      "loss: 0.489792  [176000/508153]\n",
      "loss: 0.513083  [179200/508153]\n",
      "loss: 0.517404  [182400/508153]\n",
      "loss: 0.502066  [185600/508153]\n",
      "loss: 0.481235  [188800/508153]\n",
      "loss: 0.513499  [192000/508153]\n",
      "loss: 0.530859  [195200/508153]\n",
      "loss: 0.500533  [198400/508153]\n",
      "loss: 0.514189  [201600/508153]\n",
      "loss: 0.513270  [204800/508153]\n",
      "loss: 0.496154  [208000/508153]\n",
      "loss: 0.521961  [211200/508153]\n",
      "loss: 0.494454  [214400/508153]\n",
      "loss: 0.490679  [217600/508153]\n",
      "loss: 0.539811  [220800/508153]\n",
      "loss: 0.508616  [224000/508153]\n",
      "loss: 0.509554  [227200/508153]\n",
      "loss: 0.486172  [230400/508153]\n",
      "loss: 0.493025  [233600/508153]\n",
      "loss: 0.514439  [236800/508153]\n",
      "loss: 0.506218  [240000/508153]\n",
      "loss: 0.540235  [243200/508153]\n",
      "loss: 0.475133  [246400/508153]\n",
      "loss: 0.500700  [249600/508153]\n",
      "loss: 0.492611  [252800/508153]\n",
      "loss: 0.538991  [256000/508153]\n",
      "loss: 0.504205  [259200/508153]\n",
      "loss: 0.513794  [262400/508153]\n",
      "loss: 0.503575  [265600/508153]\n",
      "loss: 0.515998  [268800/508153]\n",
      "loss: 0.495988  [272000/508153]\n",
      "loss: 0.489406  [275200/508153]\n",
      "loss: 0.494985  [278400/508153]\n",
      "loss: 0.496464  [281600/508153]\n",
      "loss: 0.487542  [284800/508153]\n",
      "loss: 0.517511  [288000/508153]\n",
      "loss: 0.509967  [291200/508153]\n",
      "loss: 0.512415  [294400/508153]\n",
      "loss: 0.504615  [297600/508153]\n",
      "loss: 0.496125  [300800/508153]\n",
      "loss: 0.497860  [304000/508153]\n",
      "loss: 0.495105  [307200/508153]\n",
      "loss: 0.509462  [310400/508153]\n",
      "loss: 0.507732  [313600/508153]\n",
      "loss: 0.504673  [316800/508153]\n",
      "loss: 0.513709  [320000/508153]\n",
      "loss: 0.506788  [323200/508153]\n",
      "loss: 0.485947  [326400/508153]\n",
      "loss: 0.494499  [329600/508153]\n",
      "loss: 0.504732  [332800/508153]\n",
      "loss: 0.512140  [336000/508153]\n",
      "loss: 0.508694  [339200/508153]\n",
      "loss: 0.485741  [342400/508153]\n",
      "loss: 0.501746  [345600/508153]\n",
      "loss: 0.471786  [348800/508153]\n",
      "loss: 0.500631  [352000/508153]\n",
      "loss: 0.507233  [355200/508153]\n",
      "loss: 0.488850  [358400/508153]\n",
      "loss: 0.479633  [361600/508153]\n",
      "loss: 0.500081  [364800/508153]\n",
      "loss: 0.492813  [368000/508153]\n",
      "loss: 0.547605  [371200/508153]\n",
      "loss: 0.500137  [374400/508153]\n",
      "loss: 0.483216  [377600/508153]\n",
      "loss: 0.491862  [380800/508153]\n",
      "loss: 0.492490  [384000/508153]\n",
      "loss: 0.505126  [387200/508153]\n",
      "loss: 0.499711  [390400/508153]\n",
      "loss: 0.490419  [393600/508153]\n",
      "loss: 0.503004  [396800/508153]\n",
      "loss: 0.546932  [400000/508153]\n",
      "loss: 0.501389  [403200/508153]\n",
      "loss: 0.486815  [406400/508153]\n",
      "loss: 0.505758  [409600/508153]\n",
      "loss: 0.509577  [412800/508153]\n",
      "loss: 0.472375  [416000/508153]\n",
      "loss: 0.509205  [419200/508153]\n",
      "loss: 0.463416  [422400/508153]\n",
      "loss: 0.498287  [425600/508153]\n",
      "loss: 0.517524  [428800/508153]\n",
      "loss: 0.491073  [432000/508153]\n",
      "loss: 0.496567  [435200/508153]\n",
      "loss: 0.509156  [438400/508153]\n",
      "loss: 0.508432  [441600/508153]\n",
      "loss: 0.523337  [444800/508153]\n",
      "loss: 0.512228  [448000/508153]\n",
      "loss: 0.494054  [451200/508153]\n",
      "loss: 0.518495  [454400/508153]\n",
      "loss: 0.495820  [457600/508153]\n",
      "loss: 0.544187  [460800/508153]\n",
      "loss: 0.483936  [464000/508153]\n",
      "loss: 0.492129  [467200/508153]\n",
      "loss: 0.490261  [470400/508153]\n",
      "loss: 0.495184  [473600/508153]\n",
      "loss: 0.513716  [476800/508153]\n",
      "loss: 0.510713  [480000/508153]\n",
      "loss: 0.489886  [483200/508153]\n",
      "loss: 0.470906  [486400/508153]\n",
      "loss: 0.517489  [489600/508153]\n",
      "loss: 0.484271  [492800/508153]\n",
      "loss: 0.493791  [496000/508153]\n",
      "loss: 0.510537  [499200/508153]\n",
      "loss: 0.487288  [502400/508153]\n",
      "loss: 0.499287  [505600/508153]\n",
      "Avg loss: 0.015848 \n",
      "\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "loss: 0.526442  [    0/508153]\n",
      "loss: 0.475502  [ 3200/508153]\n",
      "loss: 0.503761  [ 6400/508153]\n",
      "loss: 0.523800  [ 9600/508153]\n",
      "loss: 0.495615  [12800/508153]\n",
      "loss: 0.507313  [16000/508153]\n",
      "loss: 0.481569  [19200/508153]\n",
      "loss: 0.501372  [22400/508153]\n",
      "loss: 0.474949  [25600/508153]\n",
      "loss: 0.503271  [28800/508153]\n",
      "loss: 0.510715  [32000/508153]\n",
      "loss: 0.505647  [35200/508153]\n",
      "loss: 0.510875  [38400/508153]\n",
      "loss: 0.506076  [41600/508153]\n",
      "loss: 0.499015  [44800/508153]\n",
      "loss: 0.515016  [48000/508153]\n",
      "loss: 0.477800  [51200/508153]\n",
      "loss: 0.500747  [54400/508153]\n",
      "loss: 0.497107  [57600/508153]\n",
      "loss: 0.503721  [60800/508153]\n",
      "loss: 0.485395  [64000/508153]\n",
      "loss: 0.491496  [67200/508153]\n",
      "loss: 0.490685  [70400/508153]\n",
      "loss: 0.497727  [73600/508153]\n",
      "loss: 0.514350  [76800/508153]\n",
      "loss: 0.497458  [80000/508153]\n",
      "loss: 0.501371  [83200/508153]\n",
      "loss: 0.508865  [86400/508153]\n",
      "loss: 0.490628  [89600/508153]\n",
      "loss: 0.510668  [92800/508153]\n",
      "loss: 0.504016  [96000/508153]\n",
      "loss: 0.487288  [99200/508153]\n",
      "loss: 0.484773  [102400/508153]\n",
      "loss: 0.489519  [105600/508153]\n",
      "loss: 0.506415  [108800/508153]\n",
      "loss: 0.495222  [112000/508153]\n",
      "loss: 0.506739  [115200/508153]\n",
      "loss: 0.489927  [118400/508153]\n",
      "loss: 0.493133  [121600/508153]\n",
      "loss: 0.510516  [124800/508153]\n",
      "loss: 0.506423  [128000/508153]\n",
      "loss: 0.514089  [131200/508153]\n",
      "loss: 0.524850  [134400/508153]\n",
      "loss: 0.495890  [137600/508153]\n",
      "loss: 0.483217  [140800/508153]\n",
      "loss: 0.510711  [144000/508153]\n",
      "loss: 0.484346  [147200/508153]\n",
      "loss: 0.491043  [150400/508153]\n",
      "loss: 0.499068  [153600/508153]\n",
      "loss: 0.515502  [156800/508153]\n",
      "loss: 0.492697  [160000/508153]\n",
      "loss: 0.479131  [163200/508153]\n",
      "loss: 0.487487  [166400/508153]\n",
      "loss: 0.489942  [169600/508153]\n",
      "loss: 0.501175  [172800/508153]\n",
      "loss: 0.514264  [176000/508153]\n",
      "loss: 0.508747  [179200/508153]\n",
      "loss: 0.490857  [182400/508153]\n",
      "loss: 0.514485  [185600/508153]\n",
      "loss: 0.508262  [188800/508153]\n",
      "loss: 0.507565  [192000/508153]\n",
      "loss: 0.511684  [195200/508153]\n",
      "loss: 0.629583  [198400/508153]\n",
      "loss: 0.509051  [201600/508153]\n",
      "loss: 0.483749  [204800/508153]\n",
      "loss: 0.511823  [208000/508153]\n",
      "loss: 0.502696  [211200/508153]\n",
      "loss: 0.488274  [214400/508153]\n",
      "loss: 0.520620  [217600/508153]\n",
      "loss: 0.479968  [220800/508153]\n",
      "loss: 0.499809  [224000/508153]\n",
      "loss: 0.484530  [227200/508153]\n",
      "loss: 0.507382  [230400/508153]\n",
      "loss: 0.477608  [233600/508153]\n",
      "loss: 0.521812  [236800/508153]\n",
      "loss: 0.503085  [240000/508153]\n",
      "loss: 0.508743  [243200/508153]\n",
      "loss: 0.502918  [246400/508153]\n",
      "loss: 0.501661  [249600/508153]\n",
      "loss: 0.501691  [252800/508153]\n",
      "loss: 0.506264  [256000/508153]\n",
      "loss: 0.481415  [259200/508153]\n",
      "loss: 0.467392  [262400/508153]\n",
      "loss: 0.502592  [265600/508153]\n",
      "loss: 0.510661  [268800/508153]\n",
      "loss: 0.515404  [272000/508153]\n",
      "loss: 0.492599  [275200/508153]\n",
      "loss: 0.482469  [278400/508153]\n",
      "loss: 0.500596  [281600/508153]\n",
      "loss: 0.594860  [284800/508153]\n",
      "loss: 0.469224  [288000/508153]\n",
      "loss: 0.497726  [291200/508153]\n",
      "loss: 0.524986  [294400/508153]\n",
      "loss: 0.509494  [297600/508153]\n",
      "loss: 0.483838  [300800/508153]\n",
      "loss: 0.466585  [304000/508153]\n",
      "loss: 0.490120  [307200/508153]\n",
      "loss: 0.526921  [310400/508153]\n",
      "loss: 0.488021  [313600/508153]\n",
      "loss: 0.481953  [316800/508153]\n",
      "loss: 0.511759  [320000/508153]\n",
      "loss: 0.491236  [323200/508153]\n",
      "loss: 0.524065  [326400/508153]\n",
      "loss: 0.505038  [329600/508153]\n",
      "loss: 0.570734  [332800/508153]\n",
      "loss: 0.468306  [336000/508153]\n",
      "loss: 0.515421  [339200/508153]\n",
      "loss: 0.477319  [342400/508153]\n",
      "loss: 0.517202  [345600/508153]\n",
      "loss: 0.524108  [348800/508153]\n",
      "loss: 0.513873  [352000/508153]\n",
      "loss: 0.507181  [355200/508153]\n",
      "loss: 0.509205  [358400/508153]\n",
      "loss: 0.501730  [361600/508153]\n",
      "loss: 0.504688  [364800/508153]\n",
      "loss: 0.501102  [368000/508153]\n",
      "loss: 0.475917  [371200/508153]\n",
      "loss: 0.509398  [374400/508153]\n",
      "loss: 0.491505  [377600/508153]\n",
      "loss: 0.494851  [380800/508153]\n",
      "loss: 0.525963  [384000/508153]\n",
      "loss: 0.504417  [387200/508153]\n",
      "loss: 0.507647  [390400/508153]\n",
      "loss: 0.468645  [393600/508153]\n",
      "loss: 0.526582  [396800/508153]\n",
      "loss: 0.495327  [400000/508153]\n",
      "loss: 0.499782  [403200/508153]\n",
      "loss: 0.485865  [406400/508153]\n",
      "loss: 0.501519  [409600/508153]\n",
      "loss: 0.496048  [412800/508153]\n",
      "loss: 0.482021  [416000/508153]\n",
      "loss: 0.485158  [419200/508153]\n",
      "loss: 0.494615  [422400/508153]\n",
      "loss: 0.504738  [425600/508153]\n",
      "loss: 0.516161  [428800/508153]\n",
      "loss: 0.507284  [432000/508153]\n",
      "loss: 0.535902  [435200/508153]\n",
      "loss: 0.498372  [438400/508153]\n",
      "loss: 0.477143  [441600/508153]\n",
      "loss: 0.503245  [444800/508153]\n",
      "loss: 0.486519  [448000/508153]\n",
      "loss: 0.515151  [451200/508153]\n",
      "loss: 0.499214  [454400/508153]\n",
      "loss: 0.490220  [457600/508153]\n",
      "loss: 0.512315  [460800/508153]\n",
      "loss: 0.503518  [464000/508153]\n",
      "loss: 0.501645  [467200/508153]\n",
      "loss: 0.509203  [470400/508153]\n",
      "loss: 0.492941  [473600/508153]\n",
      "loss: 0.504810  [476800/508153]\n",
      "loss: 0.513687  [480000/508153]\n",
      "loss: 0.507339  [483200/508153]\n",
      "loss: 0.476933  [486400/508153]\n",
      "loss: 0.502801  [489600/508153]\n",
      "loss: 0.503006  [492800/508153]\n",
      "loss: 0.503672  [496000/508153]\n",
      "loss: 0.519017  [499200/508153]\n",
      "loss: 0.505456  [502400/508153]\n",
      "loss: 0.500123  [505600/508153]\n",
      "Avg loss: 0.015966 \n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "print(\"start\")\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train_loop(train_dataloader, image_model, caption_model, loss_fn, img_optimizer, cpt_optimizer)\n",
    "    test_loop(test_dataloader, image_model, caption_model, loss_fn)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "# 現在の日付を取得します\n",
    "now = datetime.now()\n",
    "\n",
    "# YYYY-MM-DD形式で日付を出力します\n",
    "formatted_date = now.strftime(\"%Y-%m-%d\")\n",
    "\n",
    "torch.save(caption_model.state_dict(), f'model_caption_{formatted_date}.pth')\n",
    "torch.save(image_model.state_dict(), f'model_image_{formatted_date}.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[[[2.2489, 2.2489, 2.2489,  ..., 2.2489, 2.2489, 2.2489],\n",
      "          [2.2489, 2.2489, 2.2489,  ..., 2.2489, 2.2489, 2.2489],\n",
      "          [2.2489, 2.2489, 2.2489,  ..., 2.2489, 2.2489, 2.2489],\n",
      "          ...,\n",
      "          [2.2318, 2.2318, 2.2318,  ..., 2.2489, 2.2489, 2.2489],\n",
      "          [2.2318, 2.2318, 2.2318,  ..., 2.2489, 2.2489, 2.2489],\n",
      "          [2.2318, 2.2318, 2.2318,  ..., 2.2489, 2.2489, 2.2489]],\n",
      "\n",
      "         [[2.4286, 2.4286, 2.4286,  ..., 2.4286, 2.4286, 2.4286],\n",
      "          [2.4286, 2.4286, 2.4286,  ..., 2.4286, 2.4286, 2.4286],\n",
      "          [2.4286, 2.4286, 2.4286,  ..., 2.4286, 2.4286, 2.4286],\n",
      "          ...,\n",
      "          [2.4286, 2.4286, 2.4286,  ..., 2.4286, 2.4286, 2.4286],\n",
      "          [2.4286, 2.4286, 2.4286,  ..., 2.4286, 2.4286, 2.4286],\n",
      "          [2.4286, 2.4286, 2.4286,  ..., 2.4286, 2.4286, 2.4286]],\n",
      "\n",
      "         [[2.6400, 2.6400, 2.6400,  ..., 2.6400, 2.6400, 2.6400],\n",
      "          [2.6400, 2.6400, 2.6400,  ..., 2.6400, 2.6400, 2.6400],\n",
      "          [2.6400, 2.6400, 2.6400,  ..., 2.6400, 2.6400, 2.6400],\n",
      "          ...,\n",
      "          [2.6400, 2.6400, 2.6400,  ..., 2.6400, 2.6400, 2.6400],\n",
      "          [2.6400, 2.6400, 2.6400,  ..., 2.6400, 2.6400, 2.6400],\n",
      "          [2.6400, 2.6400, 2.6400,  ..., 2.6400, 2.6400, 2.6400]]],\n",
      "\n",
      "\n",
      "        [[[2.2489, 2.2489, 2.2489,  ..., 2.2489, 2.2489, 2.2489],\n",
      "          [2.2489, 2.2489, 2.2489,  ..., 2.2489, 2.2489, 2.2489],\n",
      "          [2.2489, 2.2489, 2.2489,  ..., 2.2489, 2.2489, 2.2489],\n",
      "          ...,\n",
      "          [2.2489, 2.2489, 2.2489,  ..., 2.2489, 2.2489, 2.2489],\n",
      "          [2.2489, 2.2489, 2.2489,  ..., 2.2489, 2.2489, 2.2489],\n",
      "          [2.2489, 2.2489, 2.2489,  ..., 2.2489, 2.2489, 2.2489]],\n",
      "\n",
      "         [[2.4286, 2.4286, 2.4286,  ..., 2.4286, 2.4286, 2.4286],\n",
      "          [2.4286, 2.4286, 2.4286,  ..., 2.4286, 2.4286, 2.4286],\n",
      "          [2.4286, 2.4286, 2.4286,  ..., 2.4286, 2.4286, 2.4286],\n",
      "          ...,\n",
      "          [2.4286, 2.4286, 2.4286,  ..., 2.4286, 2.4286, 2.4286],\n",
      "          [2.4286, 2.4286, 2.4286,  ..., 2.4286, 2.4286, 2.4286],\n",
      "          [2.4286, 2.4286, 2.4286,  ..., 2.4286, 2.4286, 2.4286]],\n",
      "\n",
      "         [[2.6400, 2.6400, 2.6400,  ..., 2.6400, 2.6400, 2.6400],\n",
      "          [2.6400, 2.6400, 2.6400,  ..., 2.6400, 2.6400, 2.6400],\n",
      "          [2.6400, 2.6400, 2.6400,  ..., 2.6400, 2.6400, 2.6400],\n",
      "          ...,\n",
      "          [2.6400, 2.6400, 2.6400,  ..., 2.6400, 2.6400, 2.6400],\n",
      "          [2.6400, 2.6400, 2.6400,  ..., 2.6400, 2.6400, 2.6400],\n",
      "          [2.6400, 2.6400, 2.6400,  ..., 2.6400, 2.6400, 2.6400]]],\n",
      "\n",
      "\n",
      "        [[[2.2489, 2.2489, 2.2489,  ..., 2.2489, 2.2489, 2.2489],\n",
      "          [2.2489, 2.2489, 2.2489,  ..., 2.2489, 2.2489, 2.2489],\n",
      "          [2.2489, 2.2489, 2.2489,  ..., 2.2489, 2.2489, 2.2489],\n",
      "          ...,\n",
      "          [2.2489, 2.2489, 2.2489,  ..., 2.2489, 2.2489, 2.2489],\n",
      "          [2.2489, 2.2489, 2.2489,  ..., 2.2489, 2.2489, 2.2489],\n",
      "          [2.2489, 2.2489, 2.2489,  ..., 2.2489, 2.2489, 2.2489]],\n",
      "\n",
      "         [[2.4286, 2.4286, 2.4286,  ..., 2.4286, 2.4286, 2.4286],\n",
      "          [2.4286, 2.4286, 2.4286,  ..., 2.4286, 2.4286, 2.4286],\n",
      "          [2.4286, 2.4286, 2.4286,  ..., 2.4286, 2.4286, 2.4286],\n",
      "          ...,\n",
      "          [2.4286, 2.4286, 2.4286,  ..., 2.4286, 2.4286, 2.4286],\n",
      "          [2.4286, 2.4286, 2.4286,  ..., 2.4286, 2.4286, 2.4286],\n",
      "          [2.4286, 2.4286, 2.4286,  ..., 2.4286, 2.4286, 2.4286]],\n",
      "\n",
      "         [[2.6400, 2.6400, 2.6400,  ..., 2.6400, 2.6400, 2.6400],\n",
      "          [2.6400, 2.6400, 2.6400,  ..., 2.6400, 2.6400, 2.6400],\n",
      "          [2.6400, 2.6400, 2.6400,  ..., 2.6400, 2.6400, 2.6400],\n",
      "          ...,\n",
      "          [2.6400, 2.6400, 2.6400,  ..., 2.6400, 2.6400, 2.6400],\n",
      "          [2.6400, 2.6400, 2.6400,  ..., 2.6400, 2.6400, 2.6400],\n",
      "          [2.6400, 2.6400, 2.6400,  ..., 2.6400, 2.6400, 2.6400]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[2.2489, 2.2489, 2.2489,  ..., 2.2489, 2.2489, 2.2489],\n",
      "          [2.2489, 2.2489, 2.2489,  ..., 2.2489, 2.2489, 2.2489],\n",
      "          [2.2489, 2.2489, 2.2489,  ..., 2.2489, 2.2489, 2.2489],\n",
      "          ...,\n",
      "          [2.2489, 2.2489, 2.2489,  ..., 2.2489, 2.2489, 2.2489],\n",
      "          [2.2489, 2.2489, 2.2489,  ..., 2.2489, 2.2489, 2.2489],\n",
      "          [2.2489, 2.2489, 2.2489,  ..., 2.2489, 2.2489, 2.2489]],\n",
      "\n",
      "         [[2.4286, 2.4286, 2.4286,  ..., 2.4286, 2.4286, 2.4286],\n",
      "          [2.4286, 2.4286, 2.4286,  ..., 2.4286, 2.4286, 2.4286],\n",
      "          [2.4286, 2.4286, 2.4286,  ..., 2.4286, 2.4286, 2.4286],\n",
      "          ...,\n",
      "          [2.4286, 2.4286, 2.4286,  ..., 2.4286, 2.4286, 2.4286],\n",
      "          [2.4286, 2.4286, 2.4286,  ..., 2.4286, 2.4286, 2.4286],\n",
      "          [2.4286, 2.4286, 2.4286,  ..., 2.4286, 2.4286, 2.4286]],\n",
      "\n",
      "         [[2.6400, 2.6400, 2.6400,  ..., 2.6400, 2.6400, 2.6400],\n",
      "          [2.6400, 2.6400, 2.6400,  ..., 2.6400, 2.6400, 2.6400],\n",
      "          [2.6400, 2.6400, 2.6400,  ..., 2.6400, 2.6400, 2.6400],\n",
      "          ...,\n",
      "          [2.6400, 2.6400, 2.6400,  ..., 2.6400, 2.6400, 2.6400],\n",
      "          [2.6400, 2.6400, 2.6400,  ..., 2.6400, 2.6400, 2.6400],\n",
      "          [2.6400, 2.6400, 2.6400,  ..., 2.6400, 2.6400, 2.6400]]],\n",
      "\n",
      "\n",
      "        [[[2.2489, 2.2489, 2.2489,  ..., 2.2489, 2.2489, 2.2489],\n",
      "          [2.2489, 2.2489, 2.2489,  ..., 2.2489, 2.2489, 2.2489],\n",
      "          [2.2489, 2.2489, 2.2489,  ..., 2.2489, 2.2489, 2.2489],\n",
      "          ...,\n",
      "          [2.2489, 2.2489, 2.2489,  ..., 2.2489, 2.2489, 2.2489],\n",
      "          [2.2489, 2.2489, 2.2489,  ..., 2.2489, 2.2489, 2.2489],\n",
      "          [2.2489, 2.2489, 2.2489,  ..., 2.2489, 2.2489, 2.2489]],\n",
      "\n",
      "         [[2.4286, 2.4286, 2.4286,  ..., 2.4286, 2.4286, 2.4286],\n",
      "          [2.4286, 2.4286, 2.4286,  ..., 2.4286, 2.4286, 2.4286],\n",
      "          [2.4286, 2.4286, 2.4286,  ..., 2.4286, 2.4286, 2.4286],\n",
      "          ...,\n",
      "          [2.4286, 2.4286, 2.4286,  ..., 2.4286, 2.4286, 2.4286],\n",
      "          [2.4286, 2.4286, 2.4286,  ..., 2.4286, 2.4286, 2.4286],\n",
      "          [2.4286, 2.4286, 2.4286,  ..., 2.4286, 2.4286, 2.4286]],\n",
      "\n",
      "         [[2.6400, 2.6400, 2.6400,  ..., 2.6400, 2.6400, 2.6400],\n",
      "          [2.6400, 2.6400, 2.6400,  ..., 2.6400, 2.6400, 2.6400],\n",
      "          [2.6400, 2.6400, 2.6400,  ..., 2.6400, 2.6400, 2.6400],\n",
      "          ...,\n",
      "          [2.6400, 2.6400, 2.6400,  ..., 2.6400, 2.6400, 2.6400],\n",
      "          [2.6400, 2.6400, 2.6400,  ..., 2.6400, 2.6400, 2.6400],\n",
      "          [2.6400, 2.6400, 2.6400,  ..., 2.6400, 2.6400, 2.6400]]],\n",
      "\n",
      "\n",
      "        [[[2.2489, 2.2489, 2.2489,  ..., 2.2489, 2.2489, 2.2489],\n",
      "          [2.2489, 2.2489, 2.2489,  ..., 2.2489, 2.2489, 2.2489],\n",
      "          [2.2489, 2.2489, 2.2489,  ..., 2.2489, 2.2489, 2.2489],\n",
      "          ...,\n",
      "          [2.2147, 2.2318, 2.2318,  ..., 1.6667, 1.8037, 2.0263],\n",
      "          [2.2147, 2.2318, 2.2318,  ..., 1.7009, 1.8550, 2.0605],\n",
      "          [2.2318, 2.2318, 2.2318,  ..., 1.7352, 1.8893, 2.0777]],\n",
      "\n",
      "         [[2.4286, 2.4286, 2.4286,  ..., 2.4286, 2.4286, 2.4286],\n",
      "          [2.4286, 2.4286, 2.4286,  ..., 2.4286, 2.4286, 2.4286],\n",
      "          [2.4286, 2.4286, 2.4286,  ..., 2.4286, 2.4286, 2.4286],\n",
      "          ...,\n",
      "          [2.3936, 2.4111, 2.4111,  ..., 1.5007, 1.7283, 1.9909],\n",
      "          [2.3936, 2.4111, 2.4111,  ..., 1.5707, 1.7983, 2.0609],\n",
      "          [2.4111, 2.4111, 2.4111,  ..., 1.6583, 1.8683, 2.1310]],\n",
      "\n",
      "         [[2.6400, 2.6400, 2.6400,  ..., 2.6400, 2.6400, 2.6400],\n",
      "          [2.6400, 2.6400, 2.6400,  ..., 2.6400, 2.6400, 2.6400],\n",
      "          [2.6400, 2.6400, 2.6400,  ..., 2.6400, 2.6400, 2.6400],\n",
      "          ...,\n",
      "          [2.6400, 2.6226, 2.6226,  ..., 1.7337, 1.9428, 2.2217],\n",
      "          [2.6400, 2.6400, 2.6400,  ..., 1.8034, 2.0125, 2.2740],\n",
      "          [2.6400, 2.6400, 2.6400,  ..., 1.8731, 2.0823, 2.3437]]]]), [' ◇ティティベイト titivate フェイクファーノーカラーブルゾン。昨年大人気だったフェイクファーブルゾンが今年も登場。ふんわりと柔らかい素材で着心地抜群なフェイクファーブルゾンは、1着は持っておきたいマストアイテム。昨年はなかったキルティング加工を裏地に施し、より暖かく着用していただけます。程よい丈感がパンツにもスカートにも合わせやすく、コーディネートの幅が広がります。昨年の4色にグレージュとキャメルが追加され、カラーバリエーション豊富な計6色で揃えました。', ' レザーの質感と相性の良いゴールドの金具の風合いが、こなれ感ある印象を与えてくれるサッチェルバック。・裏あり ・ショルダーストラップ取り外し可 ・ショルダーストラップ調節可 《MARCO BIANCHINI/マルコビアンキーニ》イタリア・トスカーナの、ファミリーで営むレザーバッグブランド。イタリアンレザーの持つカジュアルさにフェミニンなアクセントのあるデザインです。', ' ジャパンメイドの型を使用したパンプス。今季のテーマに合わせてリネン混素材を使用したデザイン。ラインが美しく履き心地も良くUngridの定番パンプス。吸湿、速乾性に富むこの素材は、シワになりやすく、表面上、部分的にムラ、ネップ、飛び込みキズなどが見受けられる場合がございますが、天然素材リネンの特徴ですのでご了承ください。', ' 立体的なフラワーモチーフに艶めく1つパールビジューをあしらって、リッチキュートに仕上げたピアス。シックなカラーリングが、大人っぽい印象を与えてくれます。ちょっとしたパーティシーンなどにもぴったり。', ' ★OUTDOOR別注リバティトートバッグ★使い心地の良さと耐久性を併せ持ち、バリュープライスを実現した全世界で愛用されている『OUTDOORPRODUCTS（アウトドアプロダクツ）』。2013FWリバティプリントがアクセントになったトートバッグをご紹介します!持ち手と底・内側パイピングにリバティプリントを使用した、華やかな雰囲気がステキ。予めご了承下さい。', ' しっとりと温かく贅沢な着心地を叶えるカシミア100%のタートルネックニットがお目見え。幅の広いワイドなリブ編みで、フィット感がありつつも動きやすくソフトな着やすさがうれしい。シンプルなデザインに仕上がっているから、メイン使いはもちろんインナーとしての使用もおすすめな万能アイテム。トルソー着用はホワイト 38サイズ。', ' merry jenny (メリージェニー） 秋冬に着たくなる ベロア生地のtopsには プリーツサテンの袖を 乗せて。光沢感のあるベロア生地と つややかなサテン生地の相性は◎ ぽよん、としたお袖は それだけでかわいい シルエットです。', ' 旬のチェックと千鳥格子、万能カラーブラックの3色展開です。トレンドのハイウエストとセンタープレスで、洗練された女性らしい印象に。すっきりとした腰回りで、トップスインもスタイリッシュに決まる主役級の一本です。着用', ' 毎年デザインを変えて登場する「エムエスジーエム」の人気アイテムでもあるロゴTシャツ。ロゴはワッペン風のスポーティーな雰囲気で、シンプルながらもしっかりと主張するデザイン！\\u3000ボーイズライクなビックサイズに、コンパクトな着丈とシルエットのバランス感も絶妙。ハイウエストボトムなどとコーディネートするのも◎。トルソー着用はピンクSサイズ、ブラックXSサイズ。', ' 丸みのあるフォルムに、クラシックムードが漂うボストンタイプ風のダテメガネ。シンプルながらも、華奢なフレームデザインで女性らしさと高級感をプラス。シンプルな着こなしに投入するだけで、トレンド感のある表情にしてくれます。クールな印象になれるブラックと、やわらかな表情に見せてくれるべっ甲の2色でのラインナップ。', ' ハイクオリティの素材を用い、ディテールまでこだわりのもの作りを行う“東洋エンタープライズ社”に別注した、アロハシャツ。シャリ感のあるレーヨン素材は肌触りも良く、軽快です。ルーズパンツやロングスカートと合わせるストレスフリーなコーディネートがおすすめ。夏の羽織りとしても大活躍する便利な1枚です。', ' カカトからヒールにかけてランダムにビジューを散りばめました。バックスタイルを華やかに演出してくれます。', ' ウールダブルクロスで仕立てた上質なコート。後ろの衿までの構築的なボリュームに拘り、さらにコクーン型シルエットで360度立体的な美しいラインを実現しました。ドロップショルダーとビッグシルエットでトレンドライクなディテールを取り入れ、オーバーなサイジングは大人のこなれ感のある雰囲気を演出してくれるおすすめの逸品。■MATERIAL■しっかりとした高密度の2重織の素材を使用しており、しなやかでハリ感が特徴の品のある素材感。■COORDINATE■ひざ下のロング丈で全体的に丸みをもたせたオーバーシルエットは羽織るだけで雰囲気抜群。ハリのある厚みをもたせた生地感が着ぶくれることなく、スッキリとした着こなしに仕上がります。どんなコーディネートにもハマる大人の抜け感を演出できるいち押しコート。', ' 冬の定番素材コーデュロイを使用したパンツです。裏側には柔らかなフリース素材をボンディングしているので、ふんわり暖かな着心地に仕上がっています。股上は自然なウエストラインでフィット。レッグは膝から裾に向かって少し細くなるゆるやかなテーパードスタイル。シンプルなデザインなのでさまざまなコーディネートで活躍します。●サイズ＆フィット・股上は自然なウエストラインでフィット・膝から裾に向かって少し細くなるゆるやかなテーパードスタイル●素材＆機能・裏側には柔らかなフリース素材をボンディング・ストレッチ素材', ' ◇トプカピ TOPKAPI ラメ混バスクベレー帽。さりげなくラメがきらめくバスクベレー帽。ウールにラメをミックスし華やかな雰囲気に仕上げました。丸く型入れされ、かぶる時に形が決まりやすいのでベレー帽ビギナーの方にもおすすめ。ころんと可愛らしいシルエットなので、スタイリングが女性らしく決まります。今シーズンは新色のブラウンが登場。', ' ウィゴー（レディース）のWEGO ThankYouロゴソックス幅広いスタイリングに合わせて頂けるロゴソックス。クウォーター丈の程よい長さのソックスはパンプスやフラットシューズと合わせたり、ロルアップしてチラリと覗かせる様な着こなしもオススメ。お値段も手頃でコストパフォーマンスにも優れたアイテムです。ヌードサイズとは衣服を身につけない身体のサイズです。お客様ご自身の身体サイズと比較してお選びください。■素材： ポリエステル 綿 その他   ■お手入：   >>商品のお取り扱い方法   ■原産国：', ' トレンドのカモフラージュ柄を全面に使用したアイテムで、ストラップとの配色が大人っぽい印象です。', ' スタイル良く魅せるベーシックなタイトスカート。しっかりとした生地で肌にフィットし、スリムな美シルエットを映し出します。深めのダブルスリットが脚長効果も◎。上品な大人の女性のスタイリングを実現させるイチオシアイテムです。', ' 異なる表情を見せる2つのラウンドモチーフと、ボリューム感たっぷりのタッセルを組み合わせたネックレス。どこか民族的なエッセンスを感じさせるデザインがコーディネートに遊び心をプラスしてくれます。', ' 秋冬らしい起毛素材を使用したストールです。やわらかな肌触りで軽やかな着け心地。上品なチェック柄で、顔まわりを優しい雰囲気に演出します。', ' 秋も引き続き取り入れたいガウチョパンツの提案です。さらっとした素材感と適度なハリ感のある生地で上品に仕上げました。トレンドのストライプはピッチの太さを変えて表情をつけたこだわりぶり。縦長効果を演出してくれる狙いも。', ' 表面を起毛させたニットのチェックガウンカーディンガン。今季注目のチェック柄をジャガードで表現しました。さっと軽く羽織れて、保温性もあるので秋から冬にかけて活躍。', ' GISELe12月号掲載甲の低い、スクエアトゥが今季らしいショートブーツ。艶を抑えた、合皮を使用しているので、上品なイメージに。', ' サークル持ち手、メタリック持ち手、ショルダーチェーンが付いた、3WAYの使い方が出来るコンパクトなBAGです。', ' キュッとウエストをリボンでマークするサッシュベルト。バックルがないので、女性らしくスタイルにアクセントを添えます。ゆるっとしたワンピースやロングニットを、くしゅっとブラウジングするだけで、メリハリのあるシルエットをメイク出来ます', ' 存在感抜群のサーキュラースカート。高めのウエスト位置から広がる美しいシルエットは主役級の迫力。クラシカルにもカジュアルにもお使いいただけるアイテムです。製品寸法をご確認くださいませ。この場合はご注文をキャンセルさせて頂いておりますので予めご了承くださいませ。', ' ざっくりとしたケーブル編みの袖がおしゃれなニットプルオーバー。ゆったり感のあるシルエットがトレンドライクで、着るだけで旬の着こなしが叶います。女性らしいきれい色からシックなカラーまで豊富なバリエーションなので、お気に入りの1枚がきっと見つかるはずです。', ' スタイリングのアクセント使いにぴったりな、スクエアスウィッチングピアス。', ' 「Lattice」四角のシンプルなシェイプの中にクラシカルな柄を閉じ込めたコレクション。透かし柄にセットしたメレダイヤが動くたびに繊細なきらめきを放ちます。', ' 内側・・・ボア。内側はふかふかで肌触りがよく暖かいです。親指と人差し指先にスマートフォンなどに反応する素材を使用。手袋をしたままスマートフォンの操作が可能です', ' パンツスタイルでクールに持ちたい大人顔クラッチ。】ショルダータイプとしても持てるクラッチバッグ。スクエアのフォルムにマッチするシンプルなデザインが、クリーンな大人のスタイルを演出してくれます。ドレススタイルやカジュアルに合わせてもデキル女をアピールできるクールフェイスが魅力。', ' ◇Samantha Vega(サマンサベガ)クローシェ 大ラウンドフリルのカッティングと大きなリボンでガーリーに仕上げたクローシェから新色が登場。花冠のチャームがで華やかさをプラスしています。'], tensor([1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1,\n",
      "        0, 1, 1, 1, 0, 0, 0, 0])]\n"
     ]
    }
   ],
   "source": [
    "# データローダーから一つのデータを取り出します\n",
    "data_iter = iter(train_dataloader)\n",
    "data_one = next(data_iter)\n",
    "\n",
    "print(data_one)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_492/2761686107.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdata_one\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: 'list' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "data_one[1].shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
